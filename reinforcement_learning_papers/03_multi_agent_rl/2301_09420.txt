arXiv:2301.09420v1 [cs.LG] 20 Jan 2023

On Multi-Agent Deep Deterministic Policy
Gradients and their Explainability for SMARTS
Environment
Ansh Mittal

Aditya Malte

USC Viterbi School of Engineering)
University of Southern California
Los Angeles, CA, USA
anshm@usc.edu

USC Viterbi School of Engineering)
University of Southern California
Los Angeles, CA, USA
malte@usc.edu

Abstract—Multi-Agent RL or MARL is one of the complex
problems in Autonomous Driving literature that hampers the
release of fully-autonomous vehicles today. Several simulators
have been in iteration after their inception to mitigate the problem of complex scenarios with multiple agents in Autonomous
Driving. One such simulator–SMARTS, discusses the importance
of cooperative multi-agent learning. For this problem, we discuss
two approaches–MAPPO and MADDPG, which are based onpolicy and off-policy RL approaches. We compare our results
with the state-of-the-art results for this challenge and discuss
the potential areas of improvement while discussing the explainability of these approaches in conjunction with waypoints in the
SMARTS environment.
Index Terms—Multi-Agent RL, SMARTS, MAPPO, MADDPG, Explainability, Post-Hoc Explainability

I. I NTRODUCTION

Fig. 1: Motivation
We are interested in solving a multi-agent learning (MAL)
problem related to the SMARTS environment [1]. The main
motivation for choosing this project was that despite the as the ones given in fig 2. It also defines different levels for
immense amount of deep learning architecture, the Autonomous different types of RL agents in Autonomous Vehicle literature
Vehicle industry has yet to bring more Autonomous Vehicles as given below in table I.
on road. This majorly stems from the problems that arise when
The contributions of this project are three-fold:
different agents interact with each other. This can further have
1) We implement a Multi-Agent algorithm for Proximal Pola corresponding effect of the agents being non-cooperative
icy Optimization and Deep Deterministic Policy Gradients
with each other. Further, interpretability issues arise due to
(using Prioritized Replay Buffers)
both of these which leads market stakeholders to invest less in
2) We compare their performance on the 4 evaluation metrics
the Autonomous Vehicle industry. This has been represented
given in the SMARTS environment
in fig. 1. Hence, till today there has been no fully automated
3) Further, we discuss the explainability of the better algovehicle on road. Further, only 54.2 million semi-Automated
rithm for MARL
Vehicles 1 are projected to be there in 2024 which makes The rest of the report is structured as follows. Section II
this a bleak future for the AV industry. Hence, we explore discusses the main problem statement of this research project
two different types of policies in the field of Multi-Agent effort. Section III discusses the problems and challenges faced
Reinforcement Learning (MARL) and propose a way to work when working on this project. We discuss our solutions to
their explainability for future use-cases. We plan to open-source this problem formulation in IV. We further list our results in
the code after extending and getting this research published. section V and compare them with the results obtained in the
The simulations will be made available at that time as well.
official challenge that NeurIPS organized. Then, we briefly
The SMARTS (abb. Scalable Multi-Agent RL Training discuss the explainability aspect that this problem can take
School) environment actually creates several scenarios such from here on in section VI. Conclusively, section VII discusses
the conclusions and reasons why our method falls short of the
1 Statistics taken from the website https://www.statista.com/statistics/
1230664/projected-number-autonomous-cars-worldwide/
state-of-the-art results.

Fig. 2: Scenarios
Level

Description

M0
Rule-based Planning
M1 Single-Agent Learning w/o coordinated learning
M2 Multi-Agent Modelling w Opponent Modelling
M3 Coordinated MAL w/ independent execution
M4
Local Equilibrium oriented MAL
M5
Social Welfare-oriented MAL

Possible RL Approaches
Not Applicable
Agent implicitly learns to anticipate other agent’s actions according to our car and then takes the next steps
MARL to model learning of our agent using other agents, e.g., “how likely to our car is to yield if their lane starts to change”
Coordinated Learning of what to expect of each other even if no explicit coordination, e.g., some cars will leave some gap
Learn as a local group towards a certain equilibrium for double merge scenarios, etc.
Learning broader repercussions of the car’s actions

TABLE I: Different levels of Multi-Agent Learning in Autonomous Driving discussed in the SMARTS environment [1]

II. P ROBLEM D EFINITION
This project aimed to develop methods using different
simulation environment data from SMARTS [1] to innovate
autonomous driving multi-agent that drive to their destination
as safely and quickly as possible while following all the rules
of the traffic system. For this formulation, the algorithms were
evaluated on 4 different metrics, namely Completion, Time,
Humanness, and Rules. All these metrics are discussed in
detail in Section V. This project leverages the SMARTS docker
containers for simulations2 . Further, a lot of different methods
of environment setup (Docker, Singularity, and WSL direct
setup) were tried for different reasons that have been mentioned
in the next section.
III. T ECHNICAL C HALLENGES
During working on this project, we faced several challenges
which have been mentioned below.
1) We were faced with the most striking requirement of GPU
compute as the computation involved in any RL algorithm
can take days to train on a normal CPU. For this, we had
to further divide our approach into three steps.
2 https://github.com/huawei-noah/SMARTS/tree/comp-1 (accessed: October
20, 2022)

We first trained for 1000 episodes on the general system
and ran a simulation while the model was training using
Envision Sumo.
• Then, we took the model used for our approach to the
Singularity on HPC.
• The model weights were then transferred to our system
to check the model performance on the envision server
since a GUI wasn’t available on the HPC client.
2) Since there are multiple setups that can help us run the
Envision Server, we were able to successfully run the
competition environment but there were some files missing
and version inconsistencies in different branches in the
GitHub repository (till November 3, 2022; the competition
end date ended up being postponed by a week due to
this) and hence we weren’t able to efficiently work at that
time.
3) With the baseline (Single Agent) that SMARTS proposes,
there were issues with finding which approach works well.
Further, the Ray package and TensorFlow packages can
have conflicting dependencies for some versions. Hence,
Version Control was vital for this research project.
•

IV. P ROPOSED S OLUTION AND M ETHODOLOGY
This section discusses the solution that the authors of this
research project proposed for training the policies for the
SMARTS environment. The authors used 2 different types

Fig. 3: Schematic Diagram for MAPPO using Ray and TensorFlow environments (taken from the blog in the footnotes)

of algorithms, namely, MAPPO (Multi-Agent Proximal Policy
Optimization) and MADDPG (Multi-Agent Deep Deterministic
Policy Gradient) for MAL and learning the policies. These have
been discussed briefly below for the two algorithms mentioned
in table II.
A. Multi-Agent Proximal Policy Optimization

based Learning, and Motivation-based Reinforcement Learning.
Finally, there have been approaches that use the replay buffers
approach from Deep Deterministic Policy Gradients (off-policy
method) [11] for the task of cooperative MARL3 [12]. For
implementing MAPPO, we referred to various sources to
implement an architecture as given in the fig 3 4 . Further,
OpenAI’s Gym was used during the experiments for testing
and tutorial purposes for understanding the various aspects
of Multi-Agent Training and Self-Training for Open World
Models [13]5 .
Apart from the approaches discussed above, there has been
recent work on integrating the Actor-Critic method with the
PPO RL paradigm that are beyond the scope of this project but
should be able to guide this project in several future directions.

Proximal Policy Optimization is a class of on-policy Reinforcement Learning algorithms that uses some of the advantages
of Trust Region Policy Optimization (TRPO) [2]–[4]. This
set of algorithms alternates between sampling data from the
environment through interactions, and optimizing a “surrogate”
objective function that enables multiple epochs of minibatch
updates. For more information about the algorithm, it is
advised to read the original paper by OpenAI [4]. Further,
an adaptive clipping approach for PPO [5] was developed B. Multi-Agent Deep Deterministic Policy Gradients (with
later by building on prior works. After the inception of Priority-based Replay buffers)
PPO, there have been various cooperative and Multi-Agent
MADDPG is composed of various agents interacting cooperProximal Policy Optimization implementations for various use
atively for MARL scenarios. It is composed of DDPG [11] for
cases such as targeted localization [6], Online scheduling for
Production [7], Health-care [8]. Moreover, there has been
3 Github source code can be found here https://github.com/marlbenchmark/
a combination of Convolutions with RL approaches such on-policy
4 Blog
read:
https://medium.com/analytics-vidhya/
as CMAPPO (Convolutional Multi-Agent PPO) [9] which
deep-multi-agent-reinforcement-learning-with-tensorflow-agents-1d4a91734d1f
learn an objective based on learning and exploring a new
and GitHub repository: https://github.com/rmsander/marl_ppo.git
environment most effectively by combining various domains
5 Github Page for the World Model which leverages MDN, CNN-VAE, and
such as Convolutions (for RGBD+ information), Curriculum- RL techniques for various RL-based simulations: https://worldmodels.github.io/

Fig. 4: Schematic Diagram for MADDPG taken from the research here [14]

multiple agents which interact with each other in a cooperative single episode. This leads to higher values in our evaluation
environment. It is an off-policy RL approach as opposed to and similar is the case with all other participating teams in the
MAPPO as it learns the value of the optimal policy independent competition. We list our results with both the algorithms in
of the agent’s actions6 . Before discussing the term MAPPO, it table II. It can be seen that MADDPG considerably outperforms
should be interesting to get some background for Q-learning MAPPO. This is because the priority experience buffers we
since DDPG is based on Q-learning 7 . Since it’s a model-free explore for this problem statement take the gradients of speed,
off-policy actor-critic algorithm that integrates the benefits of acceleration, and completion rate to define the priority queues.
Deterministic Policy Gradients [15] with the Deep Q-Networks We further add a few screenshots for simulations for MADDPG
(DQNs). Being an off-policy algorithm, it separately learns in fig 5.
stochastic behavior policy for exploration and deterministic
VI. O N EXPLAINABILITY OF MADDPG
policy for target updates [16]. Further, according to the original
article, it maintains a replay buffer based on the temporal
We go through several research [23]–[25] to comment on the
difference. But in our use case, we keep a priority list for this explainability of these deep reinforcement learning approaches
temporal replay buffer based on the number of accidents, rule to understand the decisions made by MARL policy networks.
violations, completion time, and jerks much like what had been In our approach, we have used various metric parameters to
done for MAPPO [17].
design the priority experience replay buffer which can help the
There has been a considerable extension to the MADDPG MADDPG learn the prioritized experience for better results.
paradigm where recurrent DPGs were used for complex envi- This is visible in the table II, where we see that our MADDPG
ronments like Cognitive Electronic Warfare [18] and Partially approach is able to outperform the top 3 winners of the
Observable Environments for Communication systems [19]. A SMARTS challenge. Further, we also observe better values for
mixed environment approach was taken for complex environ- Humanness when compared to Top-2 winners. This happens
ments using MADDPG [20], whereas Decomposed Approach as the priority experience replay buffer encodes the information
was introduced for learning multi-agent policies for UAV for implicit explainability (transparent explainability) of the
clusters to build a connected communication network [21]. algorithm and post-hoc explainability of the model can be
Further, these set of algorithms were also discussed in the depicted by the blue waypoints as depicted in fig 5. This can
context of working with smart grids for edge technology [22] help appeal to various stakeholders about how the car is taking
and have shown to perform considerably well when compared action in the evaluation scenarios over different periods.
to the other state-of-the-art.
VII. C ONCLUSION AND D ISCUSSION
V. R ESULTS AND C OMPARATIVE A NALYSIS
As noted above the results depict that our model falls short
For the evaluation of the algorithms introduced in section IV,
of
the state-of-the-art approaches. This might be due to several
we discuss the evaluation metrics we calculated using the
reasons
as discussed below.
evaluation subdirectory on the branch comp-1 under track-1
1) The training for our model didn’t converge when we
of the NeurIPS competition. Please note that these approaches
stopped the training. We had to stop the training as the
are discussed in the context of only online learning (which is
amount of time that we could query an HPC node was
listed in task 1 of the NeurIPS competition). So, the evaluation
about to be exhausted and we had to save the results and
metrics are as given below.
get the evaluation scores for the same.
1) Completion This represents the completion of the total
2)
The top-2 approaches efficiently leverage Birdeyes view
scenarios and hence can be calculated as the total number
images from the simulation as feedback to change the
of crashes for each episode
directions of the car, which we haven’t integrated into our
2) Time This metric discusses the total number of steps used
model. This can lead to suboptimal performances.
by each agent (red cars) for each episode
3)
Finally, suboptimal hyperparameters of the approaches
3) Humannness It refers to how close are our agents to
that we leverage might lead to local minima rather than
imitating human-level driving scenarios. It is the average
getting global minima.
of the total distance to obstacles, angular jerk, linear jerk,
We plan to work on this project further and extend its scope
and lane center offset for each episode
4) Rules It refers to the total number of rules violated such twofold. First, we plan to implement a CV algorithm to
as lane changing, Wrong Way, Speed Overlimit for each reinforce the RL strategies with additional heuristics (left,
right, keep) as is done in the best approach. Secondly, after
episode.
It is of importance to note that since we are taking the average getting feedback from the professor, we’d like to work on
for each episode, there can be multiple agents (red cars) in a evolutionary computing for this approach. For example, these
approaches can include Parameter-Exploring Policy Gradients
6 Difference given here: https://stats.stackexchange.com/questions/184657/
with or without CV reinforcement [26], [27]. We can also tune
what-is-the-difference-between-off-policy-and-on-policy-learning
the hyperparameter search using evolutionary techniques like
7 Blog
Post
for
introduction
to
Q-Learning:
Genetic Algorithms, Particle Swarm Optimization, CMAES,
https://medium.com/intro-to-artificial-intelligence/
q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf
Gray Wolf Optimization, etc (given enough GPU compute).

Algorithms

Completion

Time

Humanness

Rules

Speed-Direction Meta Controller Policies (TJUDRL lab)
Waypoints-Trajectory Planning (CNN for Birdview) w/ Safety Score Learning
Interaction-Aware Transformers
MAPPO (w Replay Buffer and fine-tuning)
MADDPG (with Priority-based Replay Buffers & fine-tuning)

0.24 (1)
0.31 (2)
0.32 (3)
0.72 (16*)
0.64 (10*)

385.43 (1)
467.30 (4)
386.43 (2)
723.1 (13*)
746.29 (15*)

8161.24 (16)
4292.02 (12)
641.86 (4)
4865.07 (13*)
1362.7 (9*)

1.03 (12)
16.96 (17)
0.43 (10)
0.72 (12*)
0.22 (9*)

TABLE II: Algorithms used and their comparison with the state-of-the-art winner solutions. The numbers in brackets are the
ranks of the algorithm. The (*) represents projected ranks as we weren’t able to submit our models to the official competition.

(a) Highway Merge scenario

(b) Zoo Intersection Scenario

Fig. 5: Simulation results for MADDPG (w Priority Experience Replay Buffers)

ACKNOWLEDGMENT
We thank Prof Jyotirmoy V. Deshmukh for his continuous
support during the duration of the course CSCI513. We also
thank the course TAs–Xin Qin and Aniruddh Puranic, for their
continuous support and accommodation during the duration
of the course CSCI513. We thank USC for providing the
services for High-Performance Computing for the purpose of
this project.
R EFERENCES
[1] Zhou, Ming, et al. "Smarts: Scalable multi-agent reinforcement learning
training school for autonomous driving." arXiv preprint arXiv:2010.09776
(2020).
[2] Schulman, John, et al. "Trust region policy optimization." International
conference on machine learning. PMLR, 2015.
[3] Kurutach, Thanard, et al. "Model-ensemble trust-region policy optimization." arXiv preprint arXiv:1802.10592 (2018).
[4] Schulman, John, et al. "Proximal policy optimization algorithms." arXiv
preprint arXiv:1707.06347 (2017).
[5] Chen, Gang, Yiming Peng, and Mengjie Zhang. "An adaptive clipping approach for proximal policy optimization." arXiv preprint
arXiv:1804.06461 (2018).
[6] Alagha, Ahmed, et al. "Target localization using multi-agent deep
reinforcement learning with proximal policy optimization." Future
Generation Computer Systems 136 (2022): 342-357.
[7] Lohse, Oliver, Noah Pütz, and Korbinian Hörmann. "Implementing
an Online Scheduling Approach for Production with Multi Agent
Proximal Policy Optimization (MAPPO)." IFIP International Conference
on Advances in Production Management Systems. Springer, Cham, 2021.
[8] Allen, Ross E., et al. "Health-Informed Policy Gradients for Multi-Agent
Reinforcement Learning." arXiv preprint arXiv:1908.01022 (2019).
[9] Sander, Ryan. "EMERGENT AUTONOMOUS RACING VIA MULTIAGENT PROXIMAL POLICY OPTIMIZATION."

[10] Chen, Zichen, Budhitama Subagdja, and Ah-Hwee Tan. "End-to-end
deep reinforcement learning for multi-agent collaborative exploration."
2019 IEEE International Conference on Agents (ICA). IEEE, 2019.
[11] Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement
learning." arXiv preprint arXiv:1509.02971 (2015).
[12] Yu, Chao, et al. "The surprising effectiveness of ppo in cooperative,
multi-agent games." arXiv preprint arXiv:2103.01955 (2021).
[13] Ha, David, and Jürgen Schmidhuber. "World models." arXiv preprint
arXiv:1803.10122 (2018).
[14] Qie, Han, et al. "Joint optimization of multi-UAV target assignment and
path planning based on multi-agent reinforcement learning." IEEE access
7 (2019): 146264-146272.
[15] Silver, David, et al. "Deterministic policy gradient algorithms." International conference on machine learning. PMLR, 2014.
[16] Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement
learning." arXiv preprint arXiv:1509.02971 (2015).
[17] Hou, Yuenan, and Yi Zhang. "Improving DDPG via prioritized experience
replay." no. May (2019).
[18] Wei, Xiaolong, et al. "Recurrent MADDPG for object detection and
assignment in combat tasks." IEEE Access 8 (2020): 163334-163343.
[19] Wang, Rose E., Michael Everett, and Jonathan P. How. "R-MADDPG
for partially observable environments and limited communication." arXiv
preprint arXiv:2002.06684 (2020).
[20] Wan, Kaifang, et al. "ME-MADDPG: An efficient learning-based
motion planning method for multiple agents in complex environments."
International Journal of Intelligent Systems 37.3 (2022): 2393-2427.
[21] Zhu, Zixiong, et al. "Building a Connected Communication Network for
UAV Clusters Using DE-MADDPG." Symmetry 13.8 (2021): 1537.
[22] Lei, Wenxin, et al. "MADDPG-based security situational awareness for
smart grid with intelligent edge." Applied Sciences 11.7 (2021): 3101.
[23] Krajna, Agneza, et al. "Explainability in reinforcement learning: perspective and position." arXiv preprint arXiv:2203.11547 (2022).
[24] Wells, Lindsay, and Tomasz Bednarz. "Explainable ai and reinforcement
learning—a systematic review of current approaches and trends."
Frontiers in artificial intelligence 4 (2021): 550030.
[25] Heuillet, Alexandre, Fabien Couthouis, and Natalia Díaz-Rodríguez.
"Explainability in deep reinforcement learning." Knowledge-Based
Systems 214 (2021): 106685.

[26] Sehnke, Frank, et al. "Parameter-exploring policy gradients." Neural
Networks 23.4 (2010): 551-559.
[27] Sehnke, Frank, et al. "Multimodal parameter-exploring policy gradients." 2010 Ninth International Conference on Machine Learning and
Applications. IEEE, 2010.

