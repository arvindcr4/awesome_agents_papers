MODEL-FREE MEAN-FIELD REINFORCEMENT LEARNING:
MEAN-FIELD MDP AND MEAN-FIELD Q-LEARNING

arXiv:1910.12802v2 [math.OC] 13 Oct 2021

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

Abstract. We study infinite horizon discounted Mean Field Control (MFC) problems with common
noise through the lens of Mean Field Markov Decision Processes (MFMDP). We allow the agents to
use actions that are randomized not only at the individual level but also at the level of the population.
This common randomization allows us to establish connections between both closed-loop and open-loop
policies for MFC and Markov policies for the MFMDP. In particular, we show that there exists an
optimal closed-loop policy for the original MFC. Building on this framework and the notion of stateaction value function, we then propose reinforcement learning (RL) methods for such problems, by
adapting existing tabular and deep RL methods to the mean-field setting. The main difficulty is the
treatment of the population state, which is an input of the policy and the value function. We provide
convergence guarantees for tabular algorithms based on discretizations of the simplex. Neural network
based algorithms are more suitable for continuous spaces and allow us to avoid discretizing the mean
field state space. Numerical examples are provided.

Key words. Mean field reinforcement learning, Mean field Markov Decision Processes, McKeanVlasov control
AMS subject classification. 65M12, 65M99, 93E20, 93E25
Contents
1. Introduction
2. Model Description and Notations
2.1. Probabilistic set-up of the MFC model
2.2. Probabilistic framework and classes of policies
2.3. Optimization and value functions
3. Mean-Field MDP
3.1. Mean-field MDP framework
3.2. Assumptions and optimization problem for MFMDP
3.3. Dynamic Programming principle for MFMDP
4. Relations between the models
4.1. Relations between MFC closed-loop policies and MFMDP policies
4.2. Relations between MFC closed-loop and open-loop policies
5. Mean-Field Q-Learning
5.1. State-action value function
5.2. Controls for finite state and action spaces
5.3. Simplex discretization and tabular MFQ-learning
5.4. Deep reinforcement learning for MFMDP
6. Numerical Examples
This work has been supported by NSF grant DMS-1716673 and ARO grant W911NF-17-1-0578.
1

2
3
3
6
9
9
10
11
12
13
14
16
18
18
20
21
27
28

2

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

6.1. Example 1: Cyber security model
6.2. Example 2: Discrete distribution planning
6.3. Example 3: Swarm motion
References
Appendix A. Auxiliary results for Section 2
Appendix B. Proofs for Section 4.1
Appendix C. Proofs for Section 4.2
Appendix D. Disintegration of kernels
Appendix E. Details on Q-learning, Section 5
E.1. Proof of Theorem 36
E.2. DDPG algorithm

28
31
33
34
40
42
43
43
43
43
45

1. Introduction
In todayâ€™s highly connected world, important applications often involve very large number of interacting rational agents. Understanding how individual decisions aggregate to create global outcome and
how, in turn, the agents react to those outcomes to adjust their behavior is a major challenge for numerous applications. Theoretical analyses distinguish between competitive and cooperative scenarios using
game-theoretic notions. From a computational viewpoint, solving games with multiple players becomes
infeasible when their number grows, in no small part because the number of pairwise interactions increases exponentially. To cope with this issue, mean field games (MFG) and mean field control (MFC)
problems, also called McKean-Vlasov (MKV) control have been introduced (see e.g. [31, 28, 9, 12, 13]).
Assuming that the population is homogeneous (the agents have the same transition and cost functions),
and that the interactions are symmetric (these functions depend only on the empirical distribution
of the other agents), the main idea is to use a mean-field approximation of the populationâ€™s state in
order to simplify the model. It is then sufficient to study the interactions between one representative
player and the population distribution rather than the interactions between every pair of players. In
the past decade, theoretical results and potential applications have received a growing level of interest. Numerical methods, which are a crucial tool for applications, have also been developed, mostly
based on deterministic methods for partial differential equations (see e.g. [2, 1, 3]). Computational
methods based on deep learning have recently been introduced and seem particularly suitable to tackle
high-dimensional MFG and MFC problems (see e.g. [16, 15, 5, 24, 21, 35, 4]).
In the past few years, the question of learning solutions of mean field problems in a model-free way has
gained momentum (see e.g., [36, 27, 22, 18, 25, 26, 7, 34, 33, 14]). Roughly speaking, the main point is to
develop computational methods that can compute MFG or MFC solutions only by sampling realizations
of trajectories and without having access to the model (i.e., the transition and cost functions). Modelfree methods for a single agent control problem have been developed in the framework of reinforcement
learning (RL), building on the formalism of Markov Decision Processes (MDP). One of the most wellknown methods is the so-called Q-learning which exploits the dynamic programming principle satisfied
by the state-action value function (also called Q-function). Multi-agent reinforcement learning (MARL)
extends RL methods to situations in which several agents are simultaneously learning. Breakthrough
results have been obtained in games with a small number of players (such as chess or go).

MODEL FREE MEAN FIELD RL

3

In this work, we focus on a setting with an infinite cooperative population modeled by an MFC
problem in discrete time with an infinite horizon discounted cost. Not only is the dynamics is subject
to idiosyncratic and common noise, but the actions too are subject to both idiosyncratic and common
randomness. The problem can be interpreted as one posed to a central planner who helps a very large
population of agents to minimize its social cost and, to this end, can first sample a random policy for
the whole population before letting each agent sample their action based on this common policy. This
is a distinctive feature compared with the existing literature, in particular [25, 26] which has studied RL
for MFC without common noise and [33] which has studied MFMDP in the presence of common noise
but without common randomization. We use this extra source of randomness to connect open-loop and
closed-loop policies for the MFC problem to a mean field MDP (MFMDP) whose state is the population
distribution. Defining properly this MFMDP and dealing rigorously with common noise and common
policy randomization leads us to carry out a careful probabilistic analysis of this type of problems.
Besides the development of the theoretical framework, we also investigate how RL methods can be
adapted to the MFC setting, using tabular methods or neural network based methods. Since policies
are common to the whole population in the MFMDP, randomization used in RL methods translates
into common randomization from the point of view of the MFC problem. We illustrate the performance
of two methods on several numerical examples.
The main contributions are threefold. (1) We introduce a MFMDP and study its connection with
the original MFC problem: we prove a DPP for the MFMDP value function (Theorem 19), on which we
build to enables to prove equality of the open-loop and closed-loop value functions for the MFC problem
(Theorem 27). Furthermore, we show existence of a stationary closed-loop policy (Proposition 25). (2)
We study the state-action value function (or Q-function) of the MFMDP, for which we prove a DPP
(Theorem 30). (3) We propose several RL methods: a tabular Q-learning relying on a discretization
of the mean-field state simplex (Theorem 35), and a deep RL method to deal with continuous state or
action spaces, which allows us to avoid simplex discretization or to deal with randomized actions.
The rest of the paper is organized as follows. Section 2 introduces the main concepts for the MFC
problem, including the probabilistic framework and the notions of open-loop and closed-loop policies.
Then, the corresponding MFMDP and its DPP are given in Section 3. The connections between
the MFC and the MFMDP are developed in Section 4. We then turn our attention to the numerical
aspects. In Section 5, we introduce the state-action value function, prove it satisfies a DPP, and propose
computational methods based on RL. Several numerical examples are provided in Section 6 to illustrate
original features of the MFMDP with common noise and randomized actions.
2. Model Description and Notations
Throughout the paper we work with Borel spaces, namely spaces homeomorphic to a non-empty
Borel subset of some Polish space. If C is such a space, we denote by BC its Borel Ïƒ-field and PpCq the
space of probability measures on pC, BC q implicitly assumed to be equipped with the topology of the
weak convergence and its corresponding Borel Ïƒ-field BPpCq . For all the measurability issues we refer
the reader to any of the textbooks [10] or [29].
2.1. Probabilistic set-up of the MFC model.
In this section, we specify what we mean by mean-field models with common noise. We first introduce
the major building blocks, leaving the description of the dynamics for later on.
Definition 1 (MFC model). An infinite horizon discounted mean-field control (MFC) model with
common noise is based on the following elements pS, A, E, E 0 , F, f, Î³q:

4

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

â€š A Borel space pS, BS q for the state space.
â€š A Borel space pA, BA q for the action space.
â€š Two Borel spaces pE, BE q and pE 0 , BE 0 q for the values of the idiosyncratic and common noise.
â€š A Borel measurable function F : S Ë† A Ë† PpS Ë† Aq Ë† E Ë† E 0 Ã‘ S called the system function.
â€š A bounded Borel measurable function f : S Ë† A Ë† PpS Ë† Aq Ã‘ R called the one-stage cost
function.
â€š A discount factor Î³ P p0, 1q.
The system function F is used to describe the evolution of the state process based on a state, an
action, a mean-field interaction term1 and two noise terms.
Even if we are willing to postpone the regularity conditions on F and f , the above definition is still
incomplete. It introduces the building blocks of the model, but does not explain how the actions are
taken and how the system evolves over time. In order to motivate the nature of the assumptions we are
about to introduce, we take an informal excursion in the world of finitely many actors. Our goal is to
motivate the following important features of our model:
(i) The mean-field interactions are conditioned on all shared information.
(ii) The actions are randomized by additional sources of randomness.
2.1.1. Motivation from Finitely Many Player Models. Let us imagine that N robots with states Xn1 , Â¨ Â¨ Â¨ , XnN ,
take actions Î±n1 , Â¨ Â¨ Â¨ , Î±nN at time n and that their next state is given by:
N
Â¯
Â´
1 Ã¿
i
Î´pXni ,Î±in q , in`1 , n Ä› 0, i â€œ 1, Â¨ Â¨ Â¨ , N
Xn`1
â€œ F Xni , Î±ni ,
N jâ€œ1
1n , Â¨ Â¨ Â¨ , N
n , Â¨ Â¨ Â¨ being independent and identically distributed (i.i.d. from now on) random shocks with
distribution Î½ P PpEq. Let us also assume that at each time n, a central unit collects the information
about the states, the actions and the costs (or rewards) incurred by the individual robots, and minimizes
the overall average cost to the system as given by:
N
8
n
Ë˜Ä±
1 Ã¿ â€Ã¿ n ` j j 1 Ã¿
E
Î³ f Xn , Î±n ,
Î´pXni ,Î±in q .
N jâ€œ1 nâ€œ0
N iâ€œ1
In the limit N Ã‘ 8, using standard propagation of chaos arguments, we expect that the coupled
evolutions of the states of the individual robots will become independent, and that each state evolves
according to the dynamics:
Xn`1 â€œ F pXn , Î±n , PpXn ,Î±n q , n`1 q,

n Ä› 0,

and one can then imagine that the optimization of the central unit reduces to the minimization:
8
â€Ã¿
`
Ë˜Ä±
inf
E
Î³ n f Xn , Î±n , PpXn ,Î±n q ,
Î±â€œpÎ±n qnÄ›0

nâ€œ0

where PpXn ,Î±n q denotes the joint law of the state-action couple at time n. This is the formulation of
(discrete time) MFC problem, which we will not call and MDP because of the lack of Markov property
due to the presence of PpXn ,Î±n q in the equation, responsible for the McKean-Vlasov nature of the
dynamics.
1The interactions are through the joint state-action distribution, which is sometimes referred to as â€œextendedâ€ MFC or

MFC of controls.

MODEL FREE MEAN FIELD RL

5

Still, such a model does not account for the fact that the robots evolve in an environment which is
most likely random. Since all the robots face the same random shocks due to the randomness of the
common environment, we introduce a sequence p0n qnÄ›1 of i.i.d. random elements in E 0 , with common
law Î½ 0 P PpE 0 q, independent of the idiosyncratic shocks pin qnÄ›1,iâ€œ1,Â¨Â¨Â¨ ,N of the individual robots, and
with this addition to the model, the individual robot state dynamics become:
N
Â¯
Â´
1 Ã¿
i
Î´pXni ,Î±in q , in`1 , 0n`1 ,
Xn`1
â€œ F Xni , Î±ni ,
N jâ€œ1

n Ä› 0, i â€œ 1, Â¨ Â¨ Â¨ , N.

Even though we added significant sources of coupling between the robots experiences, exchangeability
among the robots persists, and in the limit N Ã‘ 8, a conditional form of the propagation of chaos
theory should lead to conditional independence of individual dynamics which should take the generic
form:
Xn`1 â€œ F pXn , Î±n , P0pXn ,Î±n q , n`1 , 0n`1 q, n Ä› 0,
P0pXn ,Î±n q being the conditional law of the state-action couple pXn , Î±n q given the common noise p0n qnÄ›1 .
Accordingly, the optimization of the central unit should be:
8
â€Ã¿
`
Ë˜Ä±
inf
E
Î³ n f Xn , Î±n , P0pXn ,Î±n q .
Î±â€œpÎ±n qnÄ›0

nâ€œ0

This form of Mean Field discrete time Control problem with common noise is now very close to the
model we investigate in this paper. The last question we would like to address before turning to the
theoretical analysis of the model is: Could the conditional distributions P0pXn ,Î±n q depend upon some
extra sources of randomness? Indeed, if individual robots and the central unit are allowed to use mixed
strategies, they need independent sources of randomness to randomize their actions and decisions at
each time. So since the cost optimization and the choice of an action strategy are performed by the
central unit,
â€š the central unit needs a source of randomness to randomize the choice of a mixed policy to be
dispatched to the individual robots, and
â€š the individual robots need to sample their actions from the policy sampled for them by the
central unit.
So in the limit N Ã‘ 8, the idiosyncratic randomizations of the individual robots average out, while
the central unit randomization remains.
As a result, the conditioning in P0pXn ,Î±n q should be with respect to the common noise 01 , Â¨ Â¨ Â¨ , 0n as
well as the sources of randomness used by the central unit to randomize the policy handed out to the
individual robots for implementation.
2.1.2. Back to the MFC problem formulation. Motivated by the previous N -agent model discussion
when the size of the population N tends to infinity, we propose the following set-up to disentangle
clearly the various sources of randomness.
We assume that all the sources of randomness are from a probability space pâ„¦, F, Pq supporting
(i) an i.i.d. sequence pn qnÄ›0 with distribution Î½ P PpEq modeling the idiosyncratic random shocks;
(ii) an i.i.d. sequence p0n qnÄ›0 with distribution Î½ 0 P PpE 0 q modeling the common noise;
(iii) a random variable U with distribution PU in a Borel space pÎ¥, BÎ¥ q providing the randomization
for the initial state;

6

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

(iv) an i.i.d. sequence pÏ‘n qnÄ›0 of random variables in a Borel space pÎ˜, BÎ˜ q with distribution PÏ‘
providing the generic robot with a source of randomization for their action choices;
(v) an i.i.d. sequence pÏ‘0n qnÄ›0 of random variables in a Borel space pÎ˜0 , BÎ˜0 q with distribution PÏ‘0
providing the central unit with a source of randomization for the choices of policies.
We assume that all these random sequences are independent of each other. We also assume that PÏ‘ and
PÏ‘0 are both atomless. This guarantees the existence of Borel measurable functions h : Î˜ ÃÃ‘ r0, 1s and
h0 : Î˜0 ÃÃ‘ r0, 1s which are uniformly distributed when viewed as random variables on the probability
spaces pÎ˜, BÎ˜ , PÏ‘ q and pÎ˜0 , BÎ˜0 , PÏ‘0 q respectively. The uniform random variables constructed with the
functions h and h0 will be used repeatedly with the following classical result from measure theory which
we state for the sake of later reference.
Lemma 2 (Blackwell-Dubins Lemma). For any Polish space B, there exists a measurable function
ÏB : PpBq Ë† r0, 1s ÃÃ‘ B, which we shall call the Blackwell-Dubins function of the space B, satisfying
i) for each Î½ P PpBq and each uniform random variable U â€ U p0, 1q, the B - valued random variable
ÏB pÎ½, U q has distribution Î½;
ii) for almost every u P r0, 1s, the function Î½ ÃÃ‘ ÏB pÎ½, uq is continuous for the weak topology of PpBq.
For the sake of illustration, some of the computations will be done in the canonical probability space
pâ„¦c , F c , Pc q defined by:
â„¦c â€œ Î¥ Ë† Î˜ Ë† Î˜0 Ë† pE Ë† E 0 Ë† Î˜ Ë† Î˜0 q8 ,

F c â€œ BÎ¥ Ë† BÎ˜ Ë† BÎ˜0 Ë† pBE Ë† BE 0 Ë† BÎ˜ Ë† BÎ˜0 q8 ,

and the product probability Pc given by:
Pc â€œ PU b PÏ‘ b PÏ‘0 b pÎ½ b Î½ 0 b PÏ‘ b PÏ‘0 q8 .
A generic element Ï‰ P â„¦ reads
Ï‰ â€œ pu, Î¸0 , Î¸00 , e1 , e01 , Î¸1 , Î¸10 , e2 , e02 . . . , en , e0n , Î¸n , Î¸n0 , . . .q,
and we realize the random variables as coordinate mappings, U pÏ‰q â€œ u, and for n Ä› 0,
Ï‘n pÏ‰q â€œ Î¸n ,

Ï‘0n pÏ‰q â€œ Î¸n0 ,

Îµn`1 pÏ‰q â€œ en`1 ,

Îµ0n`1 pÏ‰q â€œ e0n`1 .

We will use the short hand notations Ï‘n â€œ pÏ‘0 , . . . , Ï‘n q, Ï‘0n â€œ pÏ‘00 , . . . , Ï‘0n q for every n Ä› 0, and
Îµn â€œ pÎµ1 , . . . , Îµn q, Îµ0n â€œ pÎµ01 , . . . , Îµ0n q for every n Ä› 1.
2.2. Probabilistic framework and classes of policies.
2.2.1. Filtrations, action processes, and control processes. We now introduce the framework that will
be used to rigorously study the MFC problem. As explained intuitively in the introduction, we will
distinguish several types of randomness. We will also distinguish between actions (elements of A), and
controls (probability measures on A). These are the building blocks to define later the notion of policy
(see Â§ 2.2.3).
The ÏƒÂ´field for the initial state is denoted by Fx0 â€œ ÏƒtU u. We introduce four filtrations to define
the action and control processes. The filtrations of the idiosyncratic and common noises are:
0

F0Îµ â€œ F0Îµ â€œ tH, â„¦u,

FnÎµ â€œ ÏƒtÎµn u,

0

FnÎµ â€œ ÏƒtÎµ0n u,

The filtration of the (idiosyncratic) action randomization is:
FnÎ˜ â€œ ÏƒtÎ˜0 , . . . , Î˜n u,

n Ä› 0.

n Ä› 1.

MODEL FREE MEAN FIELD RL

7

The filtration of the (common) policy randomization is:
0

FnÎ˜ â€œ ÏƒtÎ˜00 , . . . , Î˜0n u,

n Ä› 0.

We also introduce three new filtrations F0 â€œ pFn0 qnÄ›0 , Gc â€œ pGnc qnÄ›0 and Ga â€œ pGna qnÄ›0 defined by:
F00 â€œ ÏƒtÏ‘00 u
G0c â€œ ÏƒtU , Ï‘00 u,
G0a â€œ ÏƒtU , Ï‘00 , Ï‘0 u,

0

0

Fn0 â€œ FnÎµ _ FnÎ˜ , â€œ ÏƒtÏ‘0n , Îµ0n u,
Îµ0

Î˜0

Îµ0

Î˜0

n Ä› 1,

0
0
Î˜ , â€œ ÏƒtU , Ï‘
Gnc â€œ Fx0 _ FnÎµ _ Fn _ Fn _ FnÂ´1
nÂ´1 , Ï‘n , Îµn , Îµn u,

Gna â€œ Fx0 _ FnÎµ _ Fn _ Fn _ FnÎ˜ â€œ ÏƒtU , Ï‘nÂ´1 , Ï‘0n , Îµn , Îµ0n , Ï‘n u,

n Ä› 1,
n Ä› 1.

Next, we introduce the terminology which is going to help us characterize the information available
to a generic robot and the central unit to make their choices of actions and policies. Incidentally, we
start referring to the robots as agents and everything related to agents (e.g. actions, controls, policies,
costs, etc) will be referred as level-0. This is in contrast with the lifted stochastic optimization model
which we introduce in Section 3 whose elements will be referred to as level-1. For reasons which will
become clear later, the central unit controlling the robots will be called the level-1 controller. This
new terminology frees our presentation of the theoretical results from the gory details of the robotic
application we used to motivate the set-up.
Definition 3. A level-0 action is an element of A. A level-0 (mixed) control is a random probability measure on pA, BA q, that is, any random variable with values in the Borel space pPpAq, BPpAq q.
Typically, a level-0 action is denoted by a and a level-0 control is denoted by a. Unless specified
otherwise, all the controls we consider are mixed. So for the sake of brevity, we will omit this term.
We now consider the notion of action and control processes for a representative agent. Intuitively, an
action process is the realization of a control process, where the sampling is done using pÏ‘n qnÄ›0 .
Definition 4. A level-0 action process is a sequence of random variables Î± â€œ pÎ±n qnÄ›0 with values
in A which is adapted to the filtration Ga . The set of such action processes is denoted by A. A level0 control process is a sequence a â€œ pan qnÄ›0 of level-0 controls which is adapted to the filtration
Gc . Finally, an `action process Î± â€œ pÎ±n qnÄ›0
Ë˜ is said to be a realization of a level-0 control process
a â€œ pan qnÄ›0 if L Î±n | ÏƒtU , Ï‘nÂ´1 , Ï‘0n , Îµn , Îµ0n u â€œ an , P Â´ a.s., for every n Ä› 0.
Here and in the following, we use the notations PÎ¾ and LpÎ¾q interchangeably for the distribution of a
random element Î¾, and we use natural extensions to denote conditional distributions.
It can be shown (see Lemma 39 in the appendix) that, for any level-0 control process
ÅŸ a and any two
realizations Î±, Î±1 of a, every bounded Borel measurable function h, E rhpÎ±n1 q | Gnc s â€œ A hpÎ±qan pdÎ±q â€œ
E rhpÎ±n q | Gnc s, P-a.s., n Ä› 0.
2.2.2. Conditional distribution and state process. We are now in a position to describe precisely the
mean-field interactions in the system function, and provide a clear definition of the state process driven
by a mixed control process in the mean-field model with common noise.
Definition 5. For any initial distribution Âµ0 P PpSq and level-0 action process Î± â€œ pÎ±n qnÄ›0 , we say
that a process XÎ±,Âµ0 â€œ pXnÎ±,Âµ0 qnÄ›0 is a state process associated to pÎ±, Âµ0 q for the MFC model if:
X0Î±,Âµ0 is an S-valued Fx0 - random variable with distribution Âµ0 , and for every n Ä› 0,
`
Ë˜
Î±,Âµ0
(1)
Xn`1
â€œ F XnÎ±,Âµ0 , Î±n , P0pX Î±,Âµ0 ,Î±n q , Îµn`1 , Îµ0n`1 ,
n

8

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

`
Ë˜
where P0pX Î±,Âµ0 ,Î± q is a regular version of L pXnÎ±,Âµ0 , Î±n q | Fn0 , the conditional joint distribution of staten
n
action at time n with respect to common noise and common randomization up to current time.
Such a state process XÎ±,Âµ0 is adapted to the filtration Gx â€œ pGnx qnÄ›0 , defined by
(2)

G0x â€œ ÏƒtU u,

Gnx â€œ ÏƒtU , pÏ‘k , Ï‘0k , Îµk`1 , Îµ0k`1 qkâ€œ0,...,nÂ´1 u,

n Ä› 1.

For each level-0 action process Î± and each n Ä› 0, we denote by P0X Î±,Âµ0 a regular version of the
n
conditional distribution LpXnÎ±,Âµ0 | Fn0 q. It holds:
P0X Î±,Âµ0 â€œ LpXnÎ±,Âµ0 | Ïƒt0n , Ï‘0nÂ´1 uq,

(3)

n

P Â´ a.s..

this is due to the fact that XnÎ±,Âµ0 is Gnx -measurable, hence XnÎ±,Âµ0 KF Îµ0 _F Î˜0 Ï‘0n .
n

nÂ´1

2.2.3. Open-loop and closed-loop policies. We now introduce two concepts of policies: open-loop and
closed-loop ones.
We first consider open-loop policies. For each n Ä› 0, let În â€œ Î¥ Ë† pÎ˜ Ë† Î˜0 Ë† E Ë† E 0 qn Ë† Î˜0 , with
the convention that Î0 â€œ Î¥ Ë† Î˜0 , and let us define Î¾ â€œ pÎ¾n qnÄ›0 by:
Î¾0 â€œ pU , Ï‘0n q,

and

Î¾n â€œ pU , pÏ‘k , Ï‘0k , Îµk`1 , Îµ0k`1 qkâ€œ0,...,nÂ´1 , Ï‘0n q,

n Ä› 1.

Definition 6. An level-0 open-loop policy is a sequence Ï€ â€œ pÏ€n qnÄ›0 of deterministic measurable
functions Ï€n : În Ã‘ PpAq, called the open-loop strategy functions at time n of policy Ï€. The set of
all open-loop policies is denoted by Î OL . A level-0 control process a â€œ pan qnÄ›0 is said to be generated
by the open-loop policy Ï€ if:
(4)

an â€œ Ï€n pÎ¾n q,

P Â´ a.s.,

n Ä› 0.

Notice that because of measurability restrictions (an must be adapted to the filtration Gc ), there is
a one-to-one correspondence between level-0 control processes a and open-loop policies Ï€, and both
objects can be identified through equation (4). We shall use one object or the other depending on
whether we want to emphasize the stochastic process or the sequence of deterministic functions defining
the process. To be consistent with Definition 4, we shall often short-circuit the control process a and
say that an action process Î± â€œ pÎ±n qnÄ›0 is (a realization of the control process) generated by Ï€ if
LpÎ±n | Gnc q â€œ Ï€n pÎ¾n q, P Â´ a.s., for all n Ä› 0.
We now consider closed-loop policies.
Definition 7. A closed-loop Markov strategy function is a measurable function from SË†PpSqË†Î˜0
into PpAq. A closed-loop Markov policy Ï€ â€œ pÏ€n qnÄ›0 is a sequence of such functions. The set of
all closed-loop Markov policies is denoted by Î CL .
The choice of this form of closed-loop Markov strategy function is suggested by the mean-field nature
of the dynamics (1) and the form of the costs (6). Taking values in PpAq instead of A indicates that
the strategy functions are mixed. Typically, they suggest that at each time n Ä› 0, the action Î±n P A
taken according to such a policy should be sampled from a probability measure depending directly on
the values of XnÎ±,Âµ0 and P0X Î±,Âµ0 , and the random variable Ï‘0n used by the level-1 controller to randomize
n
their choice. We formalize this procedure in Definition 8 below.
Definition 8. For a closed-loop Markov policy Ï€ P Î CL and an initial distribution Âµ0 P PpSq, a pair
of state and action processes pX, Î±q â€œ pXn , Î±n qnÄ›0 is said to be generated by pÏ€, Âµ0 q if
i) X is a state process associated to pÎ±, Âµ0 q in the sense of Definition 5.

MODEL FREE MEAN FIELD RL

ii) The action process Î± is adapted to Ga and satisfies
`
Ë˜
`
Ë˜
(5)
L Î±n | Gnc â€œ Ï€n Xn , P0Xn , Ï‘0n ,
P Â´ a.s.,

9

n Ä› 0.

In Definition 8, we can view the state and action processes as constructed simultaneously by alternatively invoking the system dynamics (1) and the sampling procedure consistent with (5). A convenient
way to construct an action process Î± satisfying (5) is to use the Blackwell-Dubinâ€™s Lemma 2. Indeed, if
ÏA is the Blackwell-Dubinâ€™s function of`A `and the uniformly
Ë˜
Ë˜distributed random variables Un is given
by Un â€œ hpÏ‘n q, we can choose Î±n â€œ ÏA Ï€n Xn , P0Xn , Ï‘0n , Un , P-a.s., n Ä› 0.
Remark 9. Even though we call a policy Ï€ P Î CL a â€œMarkovâ€ policy, it does not imply any Markov
property for the state process X associated to such a policy. This abuse of terminology can be explained
by our intention to work with level-1 Markov policies which will imply the Markov property for a lifted
measure-valued state process constructed in the next section. Also, since we only use the term â€œMarkov
policyâ€ in the closed-loop setting, we shall most often drop the term closed-loop hereafter and only call
them simply Markov policies.
2.3. Optimization and value functions.
We now move to the definition of the optimization problems for an MFC model with open-loop and
closed-loop Markov policies.
We now introduce the value function associated to an action process.
Definition 10. For every level-0 action process Î±, the value function J Î± : PpSq Ã‘ R is defined for
every Âµ0 P PpSq by:
Â«
ff
Â´
Â¯
Ã¿
Î±
n
Î±,Âµ0
0
(6)
J pÂµ0 q :â€œ E
Î³ f Xn , Î±n , PpX Î±,Âµ0 ,Î±n q ,
n

nÄ›0

where the state process XÎ±,Âµ0 is associated to pÎ±, Âµ0 ) according to the dynamics (1).
The value of J Î± pÂµq is well-defined for every Âµ P PpSq because f is measurable and bounded. Furthermore, this value depends
Â´ only upon
Â¯ the sequence of one-dimensional marginal distributions of the
0
PpS Ë† Aq-valued process PpX Î±,Âµ0 ,Î± q
.
n

n

nÄ›0

For any open-loop or closed-loop policy Ï€, we can show that P0pXn ,Î±n q depends on the action process
only through the policy Ï€ provided pX, Î±q is generated by Ï€. See Lemma 40 in the appendix. As a
consequence, we can define, for any level-0 action process Î± generated by Ï€,
J Ï€ pÂµq â€œ J Î± pÂµq,

Âµ P PpSq.

Accordingly, we define the optimal open-loop value function and the optimal closed-loop value function as:
J OL,Ëš pÂµq â€œ inf J Ï€ pÂµq,
J CL,Ëš pÂµq â€œ inf J Ï€ pÂµq,
Âµ P PpSq,
Ï€PÎ OL

Ï€PÎ CL

which are finite because we assume that the one-stage cost function f is bounded and Î³ P p0, 1q.
3. Mean-Field MDP
In this section, we introduce a Markov Decision Process (MDP) which we use to identify an optimal
closed-loop Markov policy for our original MFC model.

10

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

3.1. Mean-field MDP framework.
The key observation is that, for a mixed Markov closed-loop policy Ï€ P Î CL , the associated value
function J Ï€ can be viewed `as the value
Ë˜ function of an MDP with state space PpSq, state process
0
0
pPXn qnÄ›0 and action process PpXn ,Î±n q nÄ›0 with values in the new action space PpS Ë† Aq. Actions need
to be consistent with the state in the sense that the first marginal of any action which can be taken
while in a given state, has to be equal to the state itself. We provide a rigorous definition of this MDP,
and we show its connection to the original MFC model.
A mean-field MDP (MFMDP) consists of a six-tuple pSÌ„, AÌ„, Î“Ì„, P, fÂ¯, Î³q as described below:
â€š The state space is the Borel space SÌ„ :â€œ PpSq; a generic element in SÌ„ is denoted by Âµ.
â€š The action space is the Borel space AÌ„ :â€œ PpS Ë† Aq; a generic element in AÌ„ is denoted by aÌ„.
â€š The control constraint is a set-valued function UÌ„ from SÌ„ into the set of non-empty subsets of AÌ„
defined by:
(7)

UÌ„ pÂµq :â€œ taÌ„ P AÌ„; pr1 paÌ„q â€œ Âµu,

@ Âµ P SÌ„;

where pr1 : AÌ„ Ã‘ SÌ„ is the projection function that maps aÌ„ P AÌ„ onto its first marginal distribution
on S. We shall also use the notation
(8)

Î“Ì„ :â€œ tpÂµ, aÌ„q P SÌ„ Ë† AÌ„; aÌ„ P UÌ„ pÂµqu.
â€š The transition probability kernel P : Î“Ì„ Ã‘ PpSÌ„q, which is a Borel measurable function.
â€š The one-stage cost function fÂ¯ : Î“Ì„ Ã‘ R, which is a bounded measurable function.
â€š The discount coefficient Î³ P p0, 1q.

Remark 11. The projection map pr1 is continuous, so the constraint set UÌ„ pÂµq is closed in AÌ„ for
every Âµ P SÌ„. The graph Grppr1 q :â€œ tpaÌ„, Âµq : pr1 paÌ„q â€œ Âµu Ä‚ AÌ„ Ë† SÌ„ is closed, so Î“Ì„ is also closed in SÌ„ Ë† AÌ„.
Hence Î“Ì„ is an analytic subset of SÌ„ Ë† AÌ„, and a Polish space on its own. We assume that Î“Ì„ is endowed
with the induced topology as well as the trace Ïƒ-field inherited from SÌ„ Ë† AÌ„.
Definition 12. The six-tuple pSÌ„, AÌ„, Î“Ì„, P, fÂ¯, Î³q is said to be the MFMDP lifted from the MFC
model pS, A, E, E 0 , F, f, Î³q of Definition 1 if it satisfies:
â€š The transition kernel P is given by
`
(9)
P pÂµ, aÌ„qpdÂµ1 q â€œ Î½ 0 Ë FÌ„ pÂµ, aÌ„, Â¨qÂ´1 qpdÂµ1 q,
pÂµ, aÌ„q P Î“Ì„,
where FÌ„ : Î“Ì„ Ë† E 0 Ã‘ SÌ„ is the system function defined in terms of F by:
FÌ„ pÂµ, aÌ„, e0 q â€œ paÌ„ b Î½q Ë F pÂ¨, Â¨, aÌ„, Â¨, e0 qÂ´1 ,
pÂµ, aÌ„, e0 q P Î“Ì„ Ë† E 0 .
â€š The one-stage cost function fÂ¯ : Î“Ì„ Ã‘ R of the MFMDP satisfies:
Å¼
Â¯
(11)
f pÂµ, aÌ„q â€œ
f px, Î±, aÌ„qaÌ„pdx, dÎ±q,
pÂµ, aÌ„q P Î“Ì„.
(10)

SË†A

Here and in the following we denote by Î½ Ë g Â´1 the push-forward of a measure Î½ by a measurable
function g. We defined the dynamics using a transition kernel P for two reasons: 1) to conform with
the standard literature on MDPs which seems to prefer transition kernels to system functions; 2) we
shall restrict ourselves to Markovian policies and control processes given by feedback functions of the
state for the analysis of the lifted MDP.
We can check that FÌ„ is Borel measurable; see e.g. [10, Proposition 7.29] and Remark 41 in the
appendix.

MODEL FREE MEAN FIELD RL

11

Remark 13. In anticipation for what is going to come next, we want to emphasize that the above
MFMDP satisfies the assumptions of [10, Chapter 8-9]. We will use the results therein to derive a form
of Dynamic Programming Principle (DPP) for the MFMDP.
To highlight the tight connections between the lifted MFMDP and the original MFC, we define the
notion of mixed strategy and mixed Markov policy for MFMDP.
Definition 14. We call level-1 pure strategy function any Borel measurable function from SÌ„ into
AÌ„ whose graph is contained in Î“Ì„. We call level-1 mixed strategy function any Borel measurable
p
function Ï€Ì„ from SÌ„ into PpAÌ„q satisfying Ï€Ì„pÂµqpUÌ„ pÂµqq â€œ 1, Âµ P SÌ„. We denote by Î  (resp. Î ) the set of
pure (resp. mixed) strategy functions.
p

We shall identify every Î² P Î  with the corresponding level-1 mixed strategy Ï€Ì„ defined by Ï€Ì„pÂµq â€œ Î´Î²pÂµq
for Âµ P SÌ„. For the sake of brevity, we sometimes omit the term â€œmixedâ€ or â€œrandomizedâ€ when Ï€Ì„ P Î ,
p
but we always keep the term â€œpureâ€ or â€œnon-randomizedâ€ when Ï€Ì„ P Î  . We now define policies as
sequences of strategy functions.
Definition 15. A mixed Markov policy for MFMDP is an element of Î Ì„ :â€œ pÎ Ì„qN . Similarly, a pure
policy is an element of Î Ì„p :â€œ pÎ Ì„p qN . We say that a policy Ï€Ì„ â€œ pÏ€Ì„n qnÄ›0 is stationary if the strategy
functions Ï€Ì„n are equal for all n.
Keeping with the spirit of the previous section, these policies should be called â€œMarkovâ€ policies.
We restrict ourselves to these policies and refrain from using history dependent policies because we are
mostly interested in optimizing value functions, and we know that for MDPs like our lifted MFMDP,
for each history dependent policy, there exists a Markov policy with the same value function (as defined
below in Definition 17). See for example [10, Proposition 9.1].
Definition 16. A pair of state and action processes pÂµ, aÌ„q â€œ pÂµn , aÌ„n qnÄ›0 is said to be generated by
pÏ€Ì„, Âµq P Î Ì„ Ë† SÌ„ if the following conditions are satisfied: Âµ0 â€œ Âµ,
(12)

Âµn`1 â€œ FÌ„ pÂµn , aÌ„n , Îµ0n`1 q,

P Â´ a.s.

n Ä› 0,

and aÌ„ is an AÌ„-valued process adapted to F0 satisfying:
(13)

LpaÌ„n |Âµn q â€œ Ï€Ì„n pÂµn q,

P Â´ a.s.

n Ä› 0.

3.2. Assumptions and optimization problem for MFMDP.
We will sometimes rely on the following assumptions in the analysis of the model.
Assumption (H1).
â€š System function F : For every pe, e0 q P EË†E 0 , the function F pÂ¨, Â¨, Â¨, e, e0 q
is continuous in its remaining variables.
â€š One-stage cost function f : f : S Ë† A Ë† PpS Ë† Aq Ã‘ R is continuous.
To show existence of an optimal policy, we will make use of the following extra assumption:
Assumption (H2). Compactness: The state space S and the action space A are compact metric
spaces.
In order to obtain a dynamic programming principle (DPP) with Borel measurable mixed Markov
policies, measurability issues lead us to work under Assumption (H1) for the MFC model. It can be
shown that, under this assumption, FÌ„ is Borel measurable and for every e0 P E 0 , FÌ„ pÂ¨, Â¨, e0 q is continuous
in its remaining variables, and fÂ¯ is bounded and lower semi-continuous. See Lemma 42 in the appendix.

12

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

Definition 17. For every Ï€Ì„ P Î Ì„, we define the value function J Ï€Ì„ by:
â€Ã¿
Ä±
(14)
JÂ¯Ï€Ì„ pÂµq :â€œ E
Î³ n fÂ¯pÂµn , aÌ„n q ,
Âµ P SÌ„,
nÄ›0

where pÂµ, aÌ„q â€œ pÂµn , aÌ„n qnÄ›0 is any pair of state and action processes generated by pÏ€Ì„, Âµq. If Ï€Ì„ P Î Ì„ is
stationary with a strategy function Ï€Ì„ P Î Ì„, then we let JÂ¯Ï€ â€œ JÂ¯Ï€Ì„ .
It can be shown that the value function JÂ¯Ï€Ì„ given in (14) is well defined because the expectation in
(14) does not depend upon the particular choice of the pair of state action processes pÂµ, aÌ„q generated
by pÏ€Ì„, Âµq. See Lemma 43 in the appendix.
Remark 18. Using measurability arguments found for example in [10, Chapter 7], one can check that
JÂ¯Ï€Ì„ is Borel measurable when Ï€Ì„ P Î Ì„.
With the value function for MFMDP at hand, we define the optimal value function of the
MFMDP as:
Âµ P SÌ„.
JÂ¯Ëš pÂµq â€œ inf JÂ¯Ï€Ì„ pÂµq,
Ï€Ì„PÎ Ì„

3.3. Dynamic Programming principle for MFMDP. We state and prove the Dynamic Programming principle for the optimal value function with Borel measurable mixed Markov policies.
Theorem 19. Assume that (H1) and (H2) hold. Then, the function JÂ¯Ëš is bounded and lower semicontinuous, and moreover it is the unique bounded and lower semi-continuous function satisfying the
Â¯
following dynamic programming equation with unknown J:
!
â€ `
Ë˜Ä±)
Â¯
(15)
JpÂµq
â€œ inf fÂ¯pÂµ, aÌ„q ` Î³E JÂ¯ FÌ„ pÂµ, aÌ„, Îµ0 q
,
Âµ P SÌ„.
aÌ„PUÌ„ pÂµq
Ëš
Furthermore, there exists a pure stationary Ï€Ì„ Ëš â€œ pÏ€Ì„ Ëš , Ï€Ì„ Ëš , . . . q P Î Ì„p that is optimal, i.e., JÂ¯Ï€Ì„ â€œ JÂ¯Ëš .

The Dynamic Programming Principle (15) is known to hold for universally measurable policies. See
for example [10, Proposition 9.8].2 The gist of the above theorem is to show that it also holds for Borel
measurable policies.
Proof. Step 1: Bellman operator and fixed point with universal measurability.
the Bellman operator TÌ„ by:
!
â€ `
Ë˜Ä±)
Â¯
,
Âµ P SÌ„.
(16)
rTÌ„ JspÂµq
â€œ inf fÂ¯pÂµ, aÌ„q ` Î³E JÂ¯ FÌ„ pÂµ, aÌ„, Îµ0 q

We define

aÌ„PUÌ„ pÂµq

Then, by [10, Proposition 9.8], TÌ„ is a strict contraction on the space of bounded universally measurable
functions on SÌ„ and the fixed point coincides with the optimal value function for universally measurable
policies, namely, JÂ¯Ëš,U niv defined as:
JÂ¯Ëš,U niv pÂµq â€œ inf JÂ¯Ï€Ì„ ,
Âµ P SÌ„
Ï€Ì„PÎ Ì„U niv

where Î Ì„U niv is the set of universally measurable mixed strategy functions, and for every Ï€Ì„ P Î Ì„U niv , JÂ¯Ï€Ì„
is defined as in (14) for the Boreal measurable case.
Step 2: Fixed point with Borel measurability. We will apply the Banach fixed point theorem
for TÌ„ on LscpSÌ„q, which denotes the set of real valued, bounded and lower semi-continuous functions on
2Beware that the MFMDP setting is denoted with overlines in the notation of the present paper, but due to the common

noise it corresponds to a stochastic model in [10], which is denoted without overlines.

MODEL FREE MEAN FIELD RL

13

SÌ„. This set is a closed subset of the Banach space of real valued bounded functions on SÌ„ endowed with
the sup norm.
The key point is to show that TÌ„ leaves LscpSÌ„q invariant. This follows by the measurable selection
theorem for lower semi-continuous functions given in [10, Proposition 7.33], since we can show that
the content of the curly bracket in (16) is lower semi-continuous if JÂ¯ P LscpSÌ„q. Indeed, fÂ¯ is lower
semi-continuous (see Lemma 42 in the Appendix). Moreover, since FÌ„ is continuous for 0 fixed (again
by Lemma 42), the expectation is a continuous function of pÂµ, aÌ„q whenever JÂ¯ is continuous by the
dominated convergence theorem. Now since a function is lower semi-continuous if and only if it is the
pointwise limit of an increasing sequence of continuous functions, we can use the monotone convergence
theorem to show that the expectation is the limit of an increasing sequence of continuous functions,
hence that it is lower semi-continuous.
Last, TÌ„ is a strict contraction for the sup norm. We thus conclude by the Banach fixed point theorem
that TÌ„ has a unique fixed point in LscpSÌ„q, which we denote by JÂ¯Ëš,bd,lsc . In other words, JÂ¯Ëš,bd,lsc is the
unique bounded lower semi-continuous function satisfying the dynamic programming equation (15).
Step 3: JÂ¯Ëš,bd,lsc â€œ JÂ¯Ëš . Being lower semi-continuous, JÂ¯Ëš,bd,lsc is universally measurable, so it is also
a fixed point in the space of universally measurable functions. Hence, by uniqueness, it coincides with
JÂ¯Ëš,U niv . Furthermore Î Ì„ Ä Î Ì„U niv . So we deduce:
(17)

JÂ¯Ëš,bd,lsc pÂµq â€œ JÂ¯Ëš,U niv pÂµq Ä JÂ¯Ëš pÂµq,

Âµ P SÌ„.

Assumption (H2) implies that the lifted MFMDP satisfies the assumptions of [10, Corollary 9.17.2]
so there exists a stationary pure (at the level-1) Borel measurable policy Ï€Ì„ Ëš â€œ pÏ€Ì„ Ëš , Ï€Ì„ Ëš , Â¨ Â¨ Â¨ q which is
optimal in the sense that:
Ëš
JÂ¯Ï€Ì„ pÂµq â€œ JÂ¯Ëš,U niv pÂµq,
Âµ P SÌ„.
Consequently:
Ëš
JÂ¯Ëš pÂµq Ä JÂ¯Ï€Ì„ pÂµq â€œ JÂ¯Ëš,U niv pÂµq â€œ JÂ¯Ëš,bd,lsc pÂµq,
Âµ P SÌ„,
Ëš
Ëš,U
niv
Â¯
Â¯
which together with (17) gives the equality J â€œ J
.
Hence JÂ¯Ëš satisfies the dynamic programming principle (15) and by Step 4, it is the unique bounded
lower semi-continuous function satisfying it.


U niv

is only universally measurable, for each
Remark 20. When the mixed strategy function Ï€Ì„n P Î 
time n, as in the above proof of Theorem 19, the understanding of condition (13) requires a modicum of
care. Let qn â€œ LpÂµn q P PpSÌ„q be the distribution of the random state Âµn with values in SÌ„. We consider
a Borel measurable kernel, Ï€Ì„qn : pSÌ„, BSÌ„ q Ã‘ pPpAÌ„q, BPpAÌ„q q, such that Ï€Ì„qn pÂµq â€œ Ï€Ì„n pÂµq for qn -almost every
Âµ P SÌ„ (see [10, Lemma 7.28 (c)] for existence). Then condition (13) says that Ï€Ì„qn is a regular version
of the conditional probability of aÌ„n given Âµn . Furthermore, the integration of a function Ï†pÂµn , Â¨q with
respect to Ï€Ì„n pÂµn q should be understood in the following sense:
Å¼
Å¼
Ï†pÂµ, aÌ„qÏ€Ì„n pÂµqpdaÌ„q â€œ
Ï†pÂµ, aÌ„qÏ€Ì„qn pÂµqpdaÌ„q,
AÌ„

AÌ„

for qn Â´almost every Âµ P SÌ„, where qn â€œ LpÂµn q.
4. Relations between the models

14

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

4.1. Relations between MFC closed-loop policies and MFMDP policies.
In this section, we discuss some of the connections between the original level-0 MFC model and the
lifted MFMDP model.
We start by highlighting that, intuitively, a closed-loop policy for the MFC can be viewed as sampled
from a policy for the MFMDP by picking the common randomness. The following definition formalizes
this idea.
Definition 21. Let Ï€ P Î CL and Ï€Ì„ P Î Ì„. We say that they correspond to each other if for each
Âµ P SÌ„ and n Ä› 0, Ï€Ì„n pÂµq P PpAÌ„q is equal to the push forward of PÏ‘0 by the map:
Î˜0 Q Î¸0 ÃÃ‘ ÂµpdxqÏ€n px, Âµ, Î¸0 qpdÎ±q P AÌ„.
Note that if Ï€ and Ï€Ì„ correspond to each other, then one is stationary if and only if the other one is.
The main result of this section is the following.
Theorem 22. Assume (H1) holds. Then for every Âµ P PpSq, tJ Ï€ pÂµq : Ï€ P Î CL u â€œ tJÂ¯Ï€Ì„ pÂµq : Ï€Ì„ P Î Ì„u.
Similarly, for stationary policies, we have: for every Âµ P PpSq, tJ Ï€ pÂµq : Ï€ P Î CL stationaryu â€œ
tJÂ¯Ï€Ì„ pÂµq : Ï€Ì„ P Î Ì„ stationaryu.
This result means that for every Ï€ P Î CL , there exists Ï€Ì„ P Î Ì„ such that J Ï€ â€œ JÂ¯Ï€Ì„ and conversely, for
every Ï€Ì„ P Î Ì„, there exists Ï€ P Î CL such that this equality holds, with the same result in the stationary
case.
In preparation for the proof of this result, we give two useful technical lemmas whose proofs are
deferred to the appendix. These lemmas describe the properties of conditional distributions of the
level-0 state and action processes.
Lemma 23. Assume (H1) holds. Let Î± P A, Âµ0 P PpSq, and let X be the associated state process.
Then:
(18)

P0Xn`1 â€œ FÌ„ pP0Xn , P0pXn ,Î±n q , Îµ0n`1 q,

P Â´ a.s.

n Ä› 0.

So LpP0Xn`1 q â€œ P pP0Xn , P0pXn ,Î±n q q, n Ä› 0, where the transition kernel P was defined in (9).
Lemma 24. Assume (H1) holds. Let Î± P A, Âµ0 P PpSq, and let X be the associated state process.
For every n Ä› 0, let Îºn : SÌ„ Ã‘ PpAÌ„q be the Borel measurable disintegration kernel of LpP0Xn , P0pXn ,Î±n q q
along its first marginal. Then, if pÎ¶, Î·Ì„q is an pSÌ„ Ë† AÌ„q - valued pair of stochastic processes which are F0 adapted, and satisfy: Î¶0 â€œ Âµ0 , PÂ´a.s., Î¶n`1 â€œ FÌ„ pÎ¶n , Î·Ì„n , Îµ0n`1 q, PÂ´a.s., n Ä› 0, and if LpÎ·Ì„n |Î¶n q â€œ Îºn pÎ¶n q,
P Â´ a.s. n Ä› 0, we have:
`
Ë˜
(19)
LpÎ¶n , Î·Ì„n q â€œ L P0Xn , P0pXn ,Î±n q ,
n Ä› 0.
Intuitively, pÎºn qnÄ›0 plays the role of the conditional law of the action process. We will come back to
this interpretation in the proof of Lemma 29. We are now ready to prove Theorem 22.
Proof of Theorem 22. Step 1: Let Ï€ P Î CL . Let Ï€Ì„ corresponding to Ï€ in the sense of Definition 21.
We now check the equality of the value functions J Ï€ and JÂ¯Ï€Ì„ . Note that if Ï€Ì„ is stationary, then so is Ï€.
Let pX, Î±q be a pair of state and action processes generated by pÏ€, Âµ0 q. Then
â€Ã¿
â€Ã¿
`
Ë˜Ä±
`
Ë˜Ä±
J Ï€ pÂµ0 q â€œ E
Î³ n f Xn , Î±n , P0pXn ,Î±n q â€œ E
Î³ n fÂ¯ P0Xn , P0pXn ,Î±n q .
nÄ›0

nÄ›0

MODEL FREE MEAN FIELD RL

15

We now check that Âµn â€œ P0Xn and aÌ„n â€œ P0pXn ,Î±n q form a pair of state and action processes generated
by pÏ€Ì„, Âµ0 q. This is indeed the case because (12) is implied by Lemma 23 and (13) is implied by the
definition of Ï€Ì„n . Consequently,
â€Ã¿
`
Ë˜Ä±
JÂ¯Ï€Ì„ pÂµ0 q â€œ E
Î³ n fÂ¯ P0Xn , P0pXn ,Î±n q â€œ J Ï€ pÂµ0 q.
nÄ›0

Step 2: Conversely, let Ï€Ì„ â€œ pÏ€Ì„n qnÄ›0 in Î Ì„. For every n Ä› 0, Ï€Ì„n : SÌ„ Ã‘ PpAÌ„q is a Borel measurable map
such that for every Âµ P SÌ„ we have pr1 pÏ€Ì„n pÂµqq â€œ Âµ. According to the universal disintegration theorem [29,
Corollary 1.26], there exists a Borel measurable probability kernel K : S Ë† PpS Ë† Aq Ë† PpSq Ã‘ PpAq
such that for every Ï P PpS Ë† Aq and Âµ P PpSq such that pr1 pÏq â€œ Âµ, we have Ï â€œ Âµ bÌ‚ KpÂ¨, Ï, Âµq, where
bÌ‚ denotes the product of a measure and a kernel. So for every integer n Ä› 0, x P S, Âµ P SÌ„ and Î¸0 P Î˜0 ,
we define:
Â´
`
Ë˜ Â¯
(20)
Ï€n px, Âµ, Î¸0 q :â€œ K x, ÏAÌ„ Ï€Ì„n pÂµq, h0 pÎ¸0 q , Âµ ,
where ÏAÌ„ is the Blackwell-Dubins function of AÌ„. Note that if Ï€Ì„ is stationary, then so is Ï€. Because the
functions K, h0 and ÏAÌ„ are Borel measurable, so is the strategy function Ï€n for every n Ä› 0. Hence
Ï€ â€œ pÏ€n qnÄ›0 P Î CL . Recall that the function h0 was introduced in Section 2.1.2, and that h0 pÏ‘0 q
is uniformly distributed on r0, 1s by construction. Notice that for every Âµ P SÌ„ and for almost every
Î¸0 P r0, 1s, the definition of the universal disintegration kernel K implies that:
Â´ `
Â´
Ë˜Â¯
`
Ë˜ Â¯
ÏAÌ„ Ï€Ì„n pÂµq, Î¸0 pdx, dÎ±q â€œ ÂµpdxqK x, ÏAÌ„ Ï€Ì„n pÂµq, Î¸0 , Âµ pdÎ±q,

(21)

and as a result, we have:
Â´ `
Ë˜Â¯
ÏAÌ„ Ï€Ì„n pÂµq, h0 pÎ¸0 pdx, dÎ±q â€œ ÂµpdxqÏ€n px, Âµ, Î¸0 qpdÎ±q.

(22)

When Î¸0 is replaced by Ï‘0n , by Blackwell-Dubins lemma (see first point in Lemma 2), the left hand side
of (22) is a random variable with values in AÌ„ â€œ PpS Ë† Aq with distribution Ï€Ì„n pÂµq.
Next, we show that J Ï€ â€œ JÂ¯Ï€Ì„ . Let Âµ0 P SÌ„. Let pÎ¶, Î·Ì„q be state and action processes generated by
pÏ€Ì„, Âµ0 q (see Definition 16). Let pX, Î±q be a pair of state and action processes generated by Ï€ and Âµ0 .
Using the fact that P0pXn ,Î±n q â€œ P0Xn bÌ‚ Ï€n pÂ¨, P0Xn , Ï‘0n q, and the fact that Ï‘0n is independent of P0Xn by
equation (3), we have:
Ï€

J pÂµ0 q â€œ

Ã¿
nÄ›0

Ã¿
â€œ
nÄ›0

Ã¿
â€œ
nÄ›0

â€Å¼
Î³ E
n

SË†A

â€Å¼
n
Î³ E
SË†A

â€Å¼
Î³nE
SË†A

Ä±
f px, Î±, P0pXn ,Î±n q qP0pXn ,Î±n q pdx, dÎ±q
Ä±
`
Ë˜
f x, Î±, P0Xn bÌ‚ Ï€n pÂ¨, P0Xn , Ï‘0n q P0Xn pdxqÏ€n px, P0Xn , Ï‘0n qpdÎ±q
Ä±
`
Ë˜
f px, Î±, aÌ„qaÌ„pdx, dÎ±qÏ€Ì„n P0Xn pdaÌ„q .

16

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

The last equality holds by the fact that both sides of (22) with Î¸0 â€œ Ï‘0n are random variables with
values in AÌ„ â€œ PpS Ë† Aq with distribution Ï€Ì„n pÂµq. On the other hand:
Â«
ff
Ã¿
Ï€Ì„
n
JÂ¯ pÂµ0 q â€œ E
Î³ fÂ¯pÎ¶n , Î·Ì„n q
nÄ›0

Ã¿
â€œ

â€Å¼
Î³ E
AÌ„

nÄ›0

Ã¿
â€œ

ïš¾
fÂ¯pÎ¶n , aÌ„qLpÎ·Ì„n | Î¶n qpdaÌ„q

n

â€Å¼
n

Î³ E

nÄ›0

AÌ„

ïš¾
0
0
Â¯
f pPXn , aÌ„qÏ€Ì„n pPXn qpdaÌ„q ,

where the last equality holds by (13) because pÎ¶, Î·Ì„q are generated by pÏ€Ì„, Âµ0 q. This completes the
proof.

At this stage, we need to emphasize the crucial role played by the common randomization provided
by the sequence pÏ‘0n qnÄ›0 . Its presence is what allowed us to prove that the value of a policy for the
lifted MDP can always be achieved by a closed loop policy of the original MFC.
Comparing to [33], while they prove equality of the optimal value functions without the central
randomization, the latter allows us to prove the identity of the value functions, policy by policy, even
before taking the optimum values.
4.2. Relations between MFC closed-loop and open-loop policies.
We first prove existence of optimal closed-loop Markov policies and then we prove equality of the
open loop and closed loop value functions.
Proposition 25. Assume (H1) and (H2) hold. There exists a stationary closed loop Markov policy
Ëš
for the original MFC that is optimal, i.e., Ï€ Ëš â€œ pÏ€ Ëš , Ï€ Ëš , . . . q P Î CL such that: J Ï€ â€œ J CL,Ëš .
Proof. Let Ï€Ì„ Ëš be an optimal pure stationary Markov policy for MFMDP whose existence is given in
Theorem 19, and let Ï€ Ëš P Î CL be a closed-loop Markov policy whose value function is the same and
whose existence is given in Theorem 22 (for the case of stationary policies). We have:
Ëš
Ëš
J Ï€ pÂµq â€œ JÂ¯Ï€Ì„ pÂµq â€œ inf JÂ¯Ï€Ì„ pÂµq,

Âµ P PpSq,

Ï€Ì„PÎ Ì„

Using Theorem 22 again, for every Ï€ P Î CL for MFC, there exists Ï€Ì„ P Î Ì„ for MFMDP such that
JÂ¯Ï€Ì„ â€œ J Ï€ . So, for every Ï€ P Î CL ,
Ëš
J Ï€ pÂµq Ä› inf JÂ¯Ï€Ì„ pÂµq â€œ J Ï€ pÂµq,

Âµ P PpSq,

Ï€Ì„PÎ Ì„

which concludes the proof.



Remark 26. Notice that because Ï€Ì„ Ëš is pure, since the Blackwell-Dubins function ÏAÌ„ does not depend
upon its second argument when the first is a point mass, we can conclude that Ï€n does not depend upon
the common randomization as given by Ï‘00 . In other words, the above result would still hold even if we
did not have the common randomization.
We now show the equality of the open-loop and closed-loop optimal value functions of the MFC.
Theorem 27. Assume (H1) holds. Then J OL,Ëš â€œ J CL,Ëš .

MODEL FREE MEAN FIELD RL

17

This result is a direct consequence of the following Lemma 28 and Lemma 29, together with Theorem 22. Indeed, by Lemma 28 and Lemma 29,
J CL,Ëš Ä› J OL,Ëš Ä› JÂ¯Ëš .
Moreover, by Theorem 22,
JÂ¯Ëš â€œ J CL,Ëš .
Hence the above inequalities are equalities, which proves the first part of Theorem 27. The existence of
an optimal closed-loop policy stems from Theorem 22, which entails the existence of an optimal openloop policy by Lemma 28. Again, while this type of equality between the optimal value functions could
be expected to hold under different assumptions and without the central randomization, we prove it
here by leveraging the equalities proven in Lemma 28 and Lemma 29 policy by policy, before computing
optima over sets of policies.
First, it is expected that every closed-loop policy for the MFC can be viewed as an open-loop policy
for the MFC, which leads to the following result in terms of value functions.
Lemma 28. Assume (H1) holds. For every Ï€Ìƒ P Î CL , there exists Ï€ P Î OL such that: J Ï€ â€œ J Ï€Ìƒ .
The proof is deferred to the Appendix C. Next, every open-loop policy for the MFC corresponds to
a policy for the MFMDP, as we show in the following result.
Lemma 29. Assume (H1) holds. For every Ï€ P Î OL , there exists Ï€Ì„ P Î Ì„ such that: JÂ¯Ï€Ì„ â€œ J Ï€ .
Proof. Let Ï€ P Î OL . Let us fix an initial distribution Âµ0 P PpSq, let Î± be an action process generated
by Ï€, and X be the state process associated with pÎ±, Âµ0 q (recall Definition 5). For each n Ä› 0, we
consider the probability kernel Îºn : SÌ„ Ã‘ PpAÌ„q defined in the statement of Lemma 24. We construct
by induction `an pSÌ„ Ë† AÌ„q-valued
pair of processes pÎ¶, Î·Ì„q in the following way. For n â€œ 0 we set Î¶0 â€œ Âµ0
Ë˜
0
0
and Î·Ì„0 â€œ ÏAÌ„ Îº0 pÎ¶0 q, h pÏ‘0 q where ÏAÌ„ is the Blackwell-Dubins function of the space AÌ„ introduced in
Lemma 2. Then for any n Ä› 0 we define: Obviously, each time we involve Ï‘00 , we rely on the central
randomization. Still, the conclusion of this lemma should not be considered as obvious because it
proves that we can pack all the dependence on the past carried by the open loop controls at level 0 into
Ë˜ and a probability measure on the space of actions at level 1.
P0Xn , P0Ì€
Xn ,Î±n

Î¶n`1 â€œ FÌ„ pÎ¶n , Î·Ì„n , Îµ0n`1 q,

`
Ë˜
and Î·Ì„n`1 â€œ ÏAÌ„ Îºn`1 pÎ¶n`1 q, h0 pÏ‘0n`1 q .

The process Î·Ì„ is adapted to F0 and by Lemma 2 it satisfies
LpÎ·Ì„n | Î¶n q â€œ Îºn pÎ¶n q,

P Â´ a.s. ,

because Ï‘0n`1 is independent of Î¶n . Thus, by Lemma 24:
Â´
Â¯
Ë˜ ,
(23)
LpÎ¶n , Î·Ì„n q â€œ L P0Xn , P0Ì€
Xn ,Î±n

n Ä› 0,

n Ä› 0.

Let Ï€Ì„ â€œ pÎºn qnÄ›0 . Since Îºn pUÌ„ pÂµqq â€œ 1 for every n Ä› 0, we see that Ï€Ì„ P Î Ì„. We conclude by noting that:
â€ Â´
8
8
Â¯ïš¾
Ã¿
Ã¿
â€œ
â€°
Ï€Ì„
n
n
0
0Ì€
Â¯
Â¯
Â¯
Ë˜
J pÂµ0 q â€œ
Î³ E f pÎ¶n , Î·Ì„n q â€œ
Î³ E f PXn , P
â€œ J Ï€ pÂµ0 q,
nâ€œ0

where the second equality holds by (23).

nâ€œ0

Xn ,Î±n



18

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

5. Mean-Field Q-Learning
5.1. State-action value function.
We now turn our attention to the question of learning the solution of the MFC problem in a modelfree setting, i.e., assuming the model is unknown while still having access to sample realizations of
state trajectories and associated rewards. Before considering algorithms, we first study the so-called
state-action value function.
In order to take advantage of the strongest results proven so far, we now assume both (H1) and
(H2) hold. Under these assumptions, recall that the DPP given by Theorem 19 holds. In this section,
we restrict ourselves to non-randomized stationary policies. When Ï€Ì„ â€œ pÏ€Ì„, Ï€Ì„, . . .q, we use freely the
notation JÂ¯Ï€Ì„ :â€œ JÂ¯Ï€Ì„ . Note however that we will use common randomization in the numerical section for
the purpose of exploration.
In this section, without any loss of generality, we restrict the search for optimal policies to the set:
UÌ„B pAÌ„|SÌ„q â€œ tÏ€Ì„ P Î Ì„p | @Âµ P SÌ„, Ï€Ì„pÂµq P UÌ„ pÂµq u.
For each Ï€Ì„ P UÌ„B pAÌ„|SÌ„q, the mapping SÌ„ Q Âµ ÃÃ‘ Î´Ï€Ì„pÂµq P PpAÌ„q which assigns to each Âµ P SÌ„ the Dirac point
mass at the point Ï€Ì„pÂµq P AÌ„ is a Borel measurable function by definition of the Borel Ïƒ-field of PpAÌ„q.
Now, for each Ï€Ì„ P Î Ì„, we introduce the state-action value function QÌ„Ï€Ì„ : Î“Ì„ Ã‘ R defined by:
Ã¿
(24)
QÌ„Ï€Ì„ pÂµ, aÌ„q :â€œ fÂ¯pÂµ, aÌ„q `
Î³ n ErfÂ¯pÂµn , Ï€Ì„pÂµn qqs,
pÂµ, aÌ„q P Î“Ì„,
nÄ›1

where
the processË˜ pÂµn qnÄ›0 starting at Âµ0 â€œ Âµ satisfies Âµ1 â€œ FÌ„ pÂµ, aÌ„, Îµ01 q, and for every n Ä› 1: Âµn`1 â€œ
`
FÌ„ Âµn , Ï€Ì„pÂµn q, Îµ0n`1 . Next we define the optimal state-action value function by:
(25)

QÌ„Ëš pÂµ, aÌ„q :â€œ

inf

QÌ„Ï€Ì„ pÂµ, aÌ„q,

pÂµ, aÌ„q P Î“Ì„.

Ï€Ì„PUÌ„B pAÌ„|SÌ„q

The main goal of this section is to prove the following dynamic programming principle for QÌ„Ëš .
Theorem 30. Assume (H1) and (H2) hold. The optimal state-action value function QÌ„Ëš satisfies the
so-called Bellman equation for state-action value function:
â€
`
Ë˜Ä±
Ëš
0
1
QÌ„
FÌ„
pÂµ,
aÌ„,
Îµ
q,
aÌ„
,
pÂµ, aÌ„q P Î“Ì„.
(26)
QÌ„Ëš pÂµ, aÌ„q â€œ fÂ¯pÂµ, aÌ„q ` Î³E
inf
Ë˜
`
aÌ„1 PUÌ„ FÌ„ pÂµ,aÌ„,Îµ0 q

We will prove this result by showing that QÌ„Ëš is the unique fixed point of state-action Bellman operator
T defined on the set LscpÎ“Ì„q of bounded lower semi-continuous functions on Î“Ì„, by:
Â«
ff
`
Ë˜
0
1
(27)
rT QÌ„spÂµ, aÌ„q :â€œ fÂ¯pÂµ, aÌ„q ` Î³E
pÂµ, aÌ„q P Î“Ì„.
` inf
Ë˜ QÌ„ FÌ„ pÂµ, aÌ„, Îµ q, aÌ„ ,
aÌ„1 PUÌ„ FÌ„ pÂµ,aÌ„,Îµ0 q

We first justify in Lemma 31 the fact that the operator T is well-defined.
Since Î“Ì„ is a closed subset of the Polish space SÌ„ Ë† AÌ„, it is a Borel space, and the space BDu pÎ“Ì„q
of bounded real-valued universally measurable functions on Î“Ì„ endowed with the sup norm }f }8 â€œ
suppÂµ,aÌ„qPÎ“Ì„ |f pÂµ, aÌ„q| is a Banach space. While the set LscpÎ“Ì„q is not a vector space, it is a closed subset of
BDu pÎ“Ì„q, hence a complete metric space for the metric d8 pf, f 1 q â€œ }f Â´ f 1 }8 .
Lemma 31. Assume (H1) and (H2) hold. The set LscpÎ“Ì„q is invariant under the state-action Bellman
operator T , which is a strict contraction on this metric space.

MODEL FREE MEAN FIELD RL

19

Proof. We first claim that the set LscpÎ“Ì„q is invariant under T . We need to show that T QÌ„ is lower
semi-continuous whenever QÌ„ is. To wit, by the projection property for infima of lower semi-continuous
functions (see for example [10, Proposition 7.33]), the function SÌ„ Q Âµ1 ÃÃ‘ inf aÌ„1 PUÌ„ pÂµ1 q QÌ„pÂµ1 , aÌ„1 q is lower
semi-continuous. Since Î“Ì„ Q pÂµ, aÌ„q ÃÃ‘ Âµ1 â€œ FÌ„ pÂµ, aÌ„, e0 q P SÌ„ is continuous for e0 P E 0 fixed, the infimum in
formula (27) is then a lower semi-continuous function of pÂµ, aÌ„q for fixed e0 P E 0 . Finally, Fatouâ€™s theorem
implies that the expectation in (27) is a lower semi-continuous function of pÂµ, aÌ„q P Î“Ì„. Furthermore fÂ¯ is
continuous. Consequently, T QÌ„ is also lower semi-continuous. Now if QÌ„1 and QÌ„2 are elements of LscpÎ“Ì„q
we have:
ff
Â«
Ë‡
Ë‡
Ë˜
`
Ë˜
`
Ë‡
Ë‡
QÌ„2 FÌ„ pÂµ, aÌ„, Îµ0 q, aÌ„1 Ë‡
inf
QÌ„1 FÌ„ pÂµ, aÌ„, Îµ0 q, aÌ„1 Â´
inf
}T QÌ„1 Â´ T QÌ„2 }8 Ä Î³E sup Ë‡
aÌ„1 PUÌ„ pFÌ„ pÂµ,aÌ„,Îµ0 qq

1
0
pÂµ,aÌ„qPÎ“Ì„ aÌ„ PUÌ„ pFÌ„ pÂµ,aÌ„,Îµ qq

ff

Â«
Ä Î³E

sup

sup

pÂµ,aÌ„qPÎ“Ì„ aÌ„1 PUÌ„ pFÌ„ pÂµ,aÌ„,Îµ0 qq

Ë‡ `
Ë˜Ë‡
Ë˜
`
Ë‡
0
1 Ë‡
0
1
FÌ„
pÂµ,
aÌ„,
Îµ
q,
aÌ„
FÌ„
pÂµ,
aÌ„,
Îµ
q,
aÌ„
Â´
QÌ„
QÌ„
Ë‡
Ë‡ 1
2

Ä Î³}Q1 Â´ Q2 }8 .
Since Î³ Äƒ 1, this proves that T is a strict contraction on LscpÎ“Ì„q. We conclude the proof of the result
using the Banach fixed point theorem.

Using the Markov property, we can rewrite the state-action value function QÌ„Ï€Ì„ in terms of the state
value function JÂ¯Ï€Ì„ :
`
Ë˜
JÂ¯Ï€Ì„ pÂµq â€œ QÌ„Ï€Ì„ Âµ, Ï€Ì„pÂµq ,

(28)

Ï€Ì„ P Î Ì„p , Âµ P SÌ„.

Now, for the optimal value functions, we have the following.
Lemma 32. Assume (H1) and (H2) hold. For all pÂµ, aÌ„q P Î“Ì„: QÌ„Ëš pÂµ, aÌ„q â€œ fÂ¯pÂµ, aÌ„q ` Î³ErJÂ¯Ëš pFÌ„ pÂµ, aÌ„, Îµ0 qqs.
Proof. We show the inequalities in both directions. First, for pÂµ, aÌ„q P Î“Ì„ given, let q â€œ LpÂµ1 q P PpSÌ„q
be the distribution of the random measure Âµ1 â€œ FÌ„ pÂµ, aÌ„, Îµ0 q. By [10, Corollary 9.5.2] and the fact that
tpÏ•, Ï•, . . .q with Ï• P UÌ„B pAÌ„|SÌ„qu Ä Î Ì„, we have:
Å¼
Å¼
Å¼
Ï€Ì„
Ëš
Â¯
Â¯
J pÂµqqpdÂµq Ä
inf
JÂ¯Ï€Ì„ pÂµqqpdÂµq.
J pÂµqqpdÂµq â€œ inf
SÌ„

Ï€Ì„PÎ Ì„ SÌ„

Ï€Ì„â€œpÏ•,Ï•,...q; SÌ„
Ï•PUÌ„B pAÌ„|SÌ„q

â€œ
â€°
Hence f pÂµ, aÌ„q ` Î³E JÂ¯Ëš pÂµ1 q Ä QÌ„Ëš pÂµ, aÌ„q.
Conversely, under the standing assumptions, by Theorem 19 there exists Ï€Ì„ Ëš P UÌ„B pAÌ„|SÌ„q that is
optimal. So we have:
â€ Ëš
Ä±
â€œ
â€°(
â€œ
â€°
QÌ„Ëš pÂµ, aÌ„q â€œ
inf
f pÂµ, aÌ„q ` Î³E JÂ¯Ï€Ì„ pÂµ1 q Ä f pÂµ, aÌ„q ` Î³E JÂ¯Ï€Ì„ pÂµ1 q â€œ f pÂµ, aÌ„q ` Î³E JÂ¯Ëš pÂµ1 q ,
Ï€Ì„PUÌ„B pAÌ„|SÌ„q

for every pÂµ, aÌ„q P Î“Ì„, which concludes the proof.



Lemma 33. Assume (H1) and (H2) hold. QÌ„Ëš is lower semi-continuous and, as a result, there exists
Ï€Ìƒ P UÌ„B pAÌ„|SÌ„q such that for every Âµ P SÌ„, Ï€ÌƒpÂµq P arg inf aÌ„PUÌ„ pÂµq QÌ„Ëš pÂµ, aÌ„q.
`
Ë˜
Proof. For each fixed e0 P E 0 , pÂµ, aÌ„q ÃÃ‘ JÂ¯Ëš FÌ„ pÂµ, aÌ„, e0 q is lower semi-continuous by lower semi-continuity
0 q. As in the proof of Lemma 31, Fatouâ€™s theorem
of JÂ¯Ëš (see Theorem 19) and continuity
â€œ Ëš `of FÌ„ pÂ¨, Â¨, e0 Ë˜â€°
implies that the function pÂµ, aÌ„q ÃÃ‘ E JÂ¯ FÌ„ pÂµ, aÌ„, e q is also lower semi-continuous. Furthermore, fÂ¯ is
continuous. So, by the expression in Lemma 32, QÌ„Ëš is a lower semi-continuous function on Î“Ì„.

20

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

Since Î“Ì„ is a closed subset of SÌ„ Ë† AÌ„ and AÌ„ is compact, by applying a selection theorem for lower
semi-continuous function [10, Proposition 7.33] on QÌ„Ëš : Î“Ì„ Ã‘ R, we obtain
thatË˜ there exists a Borel
`
measurable function Ï€Ìƒ P UÌ„B pAÌ„|SÌ„q whose graph is contained in Î“Ì„ and QÌ„Ëš Âµ, Ï€ÌƒpÂµq â€œ inf aÌ„PUÌ„ pÂµq QÌ„Ëš pÂµ, aÌ„q,
for all Âµ P SÌ„.

Lemma 34. Assume (H1) and (H2) hold. For all Âµ P SÌ„, inf aÌ„PUÌ„ pÂµq QÌ„Ëš pÂµ, aÌ„q â€œ JÂ¯Ëš pÂµq.
Proof. We first show the inequality inf aÌ„PUÌ„ pÂµq QÌ„Ëš pÂµ, aÌ„q Ä› JÂ¯Ëš pÂµq. Let us denote by Ï€Ìƒ P UÌ„B pAÌ„|SÌ„q the
strategy function in Lemma 33. By definition,
inf QÌ„Ëš pÂµ, aÌ„q â€œ QÌ„Ëš pÂµ, Ï€ÌƒpÂµqq â€œ
aÌ„PUÌ„ pÂµq

inf

QÌ„Ï€Ì„ pÂµ, Ï€ÌƒpÂµqq,

Âµ P SÌ„.

Ï€Ì„PUÌ„B pAÌ„|SÌ„q

Then for each Ï€Ì„ P UÌ„B pAÌ„|SÌ„q, we denote Ï€Ì„ Ï€Ìƒ â€œ pÏ€Ìƒ, Ï€Ì„, Ï€Ì„, Ï€Ì„, . . .q P Î Ì„. So,
Â«
ff
8
Ã¿
Ï€Ìƒ
QÌ„Ï€Ì„ pÂµ, Ï€ÌƒpÂµqq â€œ f pÂµ, Ï€ÌƒpÂµqq ` E
Î³ n fÂ¯pÂµn , Ï€Ì„pÂµn qq â€œ JÂ¯Ï€Ì„ pÂµq Ä› JÂ¯Ëš pÂµq,

Âµ P SÌ„,

nâ€œ1

which provides the first inequality. To prove the converse inequality, let Ï€Ì„ Ëš P UÌ„B pAÌ„|SÌ„q be an optimal
non-randomized stationary Markov policy whose existence is given by Theorem 19, and notice that for
every Âµ P SÌ„,
Ëš
Ëš
JÂ¯Ëš pÂµq â€œ JÂ¯Ï€Ì„ pÂµq â€œ QÌ„Ï€Ì„ pÂµ, Ï€Ì„ Ëš pÂµqq Ä› QÌ„Ëš pÂµ, Ï€Ì„ Ëš pÂµqq Ä› inf QÌ„Ëš pÂµ, aÌ„q,
aÌ„PUÌ„ pÂµq

by equation 28, Lemma 32. This concludes the proof.



We can now complete the proof of Theorem 30.
Proof of Theorem 30. The Bellman equation (26) is a direct consequence of Lemma 32 and Lemma 34.
Since T is a strict contraction mapping on LscpÎ“Ì„q by Lemma 31 and since LscpÎ“Ì„q is closed in the Banach
space BDu pÎ“Ì„q, by the Banach fixed point theorem we conclude that QÌ„Ëš is the unique fixed point of T
on LscpÎ“Ì„q.

Next, we build upon the previous results to propose reinforcement learning algorithms for the original
MFC problem. From now on, we assume that the state and action spaces are finite, unless otherwise
specified.
5.2. Controls for finite state and action spaces.
In this rest of this section, we assume that S and A are finite, we denote their numbers of elements
by |S| and |A| respectively, and we denote by xp1q , . . . , xp|S|q and Î±p1q , . . . , Î±p|A|q their elements. We first
revisit the description of the action space and then propose two reinforcement learning methods in this
setting. We shall explain later how to adapt reinforcement learning techniques to the case of continuous
spaces.
Before introducing the mean-field Q-learning algorithm, we first provide a representation of the set
Î“Ì„ Ä SÌ„ Ë† AÌ„ â€œ PpSq Ë† PpS Ë† Aq on which the QÌ„Ëš function is defined.
Since we assume that S finite, its lifted space PpSq can be identified with a simplex S in R|S| . In
other words, we treat a distribution Âµ P PpSq as an |S|-dimensional vector pÂµpiq qiâ€œ1,...,|S| whose nonnegative coordinates sum up to one. Similarly, since A is finite, we identify PpAq to a simplex A in R|A| .
However, representing admissible actions aÌ„ P UÌ„ pÂµq Ä PpS Ë† Aq of the lifted MDP requires a modicum
of care due to the constraint. A first approach is`to identify PpS
Ë† Aq with a simplex in R|S|Ë†|A| and
Ë˜
piq
pjq
to view a lifted action aÌ„ as a |S| Ë† |A| matrix aÌ„px , Î± q 1ÄiÄ|S|,1ÄjÄ|A| of non-negative numbers

MODEL FREE MEAN FIELD RL

21

summing up to 1. Then a pair pÂµ, aÌ„q P SÌ„ Ë† AÌ„ is in Î“Ì„ if and only if the following linear constraint is
Å™|A|
satisfied: jâ€œ1 aÌ„pÂµpiq , Î±pjq q â€œ Âµpiq for all i â€œ 1, . . . , |S|. The above transformation is straightforward but
not sufficient for our purposes because it provides only a representation of the actions and controls of
the central planner, and it does not address the strategy functions of non-randomized stationary mixed
Markovian closed-loop policies for an individual agent in our original optimization problem.
For any pair pÂµ, aÌ„q P Î“Ì„, we can define the mapping kÂµ : S Ã‘ PpAq: for i â€œ 1, . . . , |S|,
kÂµ pxpiq q â€œ aÌ„pÂµpiq , Î±pjq q{Âµpxpiq q,

if Âµpxpiq q Ä… 0,

and any value otherwise. Note that here, there is no common randomization. As proved above (see
Theorem 19), there exists a non-randomized stationary policy for the lifted MDP. So the central planner
can look for strategy functions within the set:
A :â€œ taÌƒ : S Ã‘ PpAq | aÌƒ Borel measurableu.

(29)

Consider the function QÌƒËš : PpSq Ë† A Ã‘ R defined by:
QÌƒËš pÂµ, aÌƒq :â€œ QÌ„Ëš pÂµ, Âµ bÌ‚ aÌƒq.

(30)

Then the Bellman equation (26) becomes:
â€
ïš¾
Å¼
Ëš
Ëš
1
(31)
QÌƒ pÂµ, aÌƒq â€œ
f px, Î±, Âµ bÌ‚ aÌƒqaÌƒpx, dÎ±qÂµpdxq ` Î³E inf
QÌƒ pÂµ1 , aÌƒ q ,
1
aÌƒ PA

SË†A

pÂµ, aÌƒq P PpSq Ë† A,

where Âµ1 â€œ FÌ„ pÂµ, Âµ bÌ‚ aÌƒ, Îµ0 q, keeping in mind that the integral over S Ë† A is in fact a finite sum. Even
though S and A are finite, equation (31) still needs to be understood as a fixed point in the space of
bounded lower semi-continuous functions on a closed subset of a finite dimensional Euclidean space, as
the measurability issues addressed in deriving equation (26) still remain. We also introduce the function
fËœ : PpSq Ë† A Ã‘ R such that:
Å¼
Ëœ
Â¯
f px, Î±, Âµ bÌ‚ aÌƒqaÌƒpx, dÎ±qÂµpdxq,
pÂµ, aÌƒq P PpSq Ë† A.
f pÂµ, aÌƒq :â€œ f pÂµ, Âµ bÌ‚ aÌƒq â€œ
SË†A

In the rest of this section, we propose two model-free algorithms relying on the optimal state-action
value function QÌ„Ëš : Î“Ì„ Ã‘ R or equivalently QÌƒËš : PpSq Ë† A Ã‘ R.
5.3. Simplex discretization and tabular MFQ-learning.
We consider two settings, depending on whether the controls at level-0 are mixed or pure. In both
cases, we prove convergence of a tabular Q-learning algorithm, after suitable discretization of the simplexes. When using pure controls, we can prove not only convergence of the value function but also of
the optimizer.
5.3.1. Q-learning with controls that are mixed at level-0. Since the simplexes S and A are not finite, it is
not possible to directly apply a tabular version of Q-learning algorithm to approximate QÌƒËš . A possible
workaround is to first replace these simplexes by finite subsets SÌŒ Ä‚ S and AÌŒ Ä‚ A. Let AÌŒ â€œ taÌŒ : S Ã‘ AÌŒu.
In particular, |AÌŒ| â€œ |AÌŒ||S| because we identify functions in AÌŒ with |S|-dimensional vectors whose entries
take values in the finite set AÌŒ. To ensure that the mean-field term takes values in the finite set SÌŒ, we
use a projection: at time n, given Âµn P SÌŒ, we compute Âµn`1 â€œ FÌ„ pÂµn , Âµn bÌ‚ aÌŒ, Îµ0n`1 q, and then we project
Âµn`1 back on SÌŒ using a projection operator ProjSÌŒ : PpSq Ã‘ SÌŒ. Precise definitions of the discretization
and the projection are provided below, after introducing a discrete version of the original MFC problem.

22

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

More precisely, we consider the projected MFC problem:
inf JË‡Ï€ÌŒ pÂµ0 q,
Âµ0 P SÌŒ,
Ï€ÌŒPÎ ÌŒ

where Î ÌŒ â€œ tÏ€ÌŒ : S Ë† SÌŒ Ã‘ AÌŒu, and for every strategy function Ï€ÌŒ : S Ë† SÌŒ Ã‘ AÌŒ, JË‡Ï€ÌŒ : SÌŒ Ã‘ R is defined by:
Â«
ff
Â´
Â¯
Ã¿
Ï€ÌŒ
n
Âµ
,Ï€ÌŒ
Âµ
,Ï€ÌŒ
0
0
(32)
JË‡ pÂµ0 q â€œ E
Î³ fËœ Âµn , Ï€ÌŒpÂ¨, Âµn q
nÄ›0

where
(33)

Â´
Â¯
0
0 ,Ï€ÌŒ
ÂµÂµn`1
â€œ ProjSÌŒ Ë FÌ„ ÂµÂµn0 ,Ï€ÌŒ , ÂµÂµn0 ,Ï€ÌŒ bÌ‚ Ï€ÌŒpÂ¨, ÂµÂµn0 ,Ï€ÌŒ q, Îµ0n`1 â€œ: Î¦ÌŒÏ€ÌŒ,Îµn`1 pÂµÂµn0 ,Ï€ÌŒ q.

We will denote by JË‡Ëš and QÌŒËš respectively the optimal state and state-action value functions of this
projected MFC problem. Here QÌŒ : SÌŒ Ë† AÌŒ Ã‘ R can be represented by a matrix (also called a table) in
R|SÌŒ|Ë†|AÌŒ| and is viewed as an approximation of QÌƒËš : PpSq Ë† A Ã‘ R of the original MFC problem.
This problem can be viewed as an MDP with finite state and action spaces. In this case, a straightforward adaptation of the tabular Q-learning algorithm leads to Algorithm 1. Note that, even in the
absence of common noise, this algorithm is possibly stochastic since at each episode, the order in which
the state-action pairs are picked is potentially random. In practice, the order could be fixed in advance
or stem from a sampled trajectory.
Algorithm 1: Mean-Field Q-learning (MFQ) with simplex discretization
Data: A number of episodes Nepi ; a sequence of learning rates pÎ·n qnâ€œ0,...,Nepi Â´1 ; a sequence of
state-action pairs pÂµÌŒn , aÌŒn qnÄ›0 P S Ë† AÌŒ.
Result: QÌŒNepi , an approximation of QÌƒËš on SÌŒ Ë† AÌŒ.
1 begin
2
Initialize table QÌŒ0 P R|SÌŒ|Ë†|AÌŒ| , Âµ0 P S and a0 P A
3
for n â€œ 0, 1, . . . Nepi Â´ 1 do
4
Execute action aÌŒn , observe ÂµÌŒ1n`1 â€œ ProjSÌŒ Ë FÌ„ pÂµÌŒn , ÂµÌŒn bÌ‚ aÌŒn , Îµ0n`1 q and cost fËœpÂµÌŒn , aÌŒn q
5
Initialize QÌŒn`1 â€œ QÌŒn on SÌŒ Ë† AÌŒ
Â¯
Â´
6
Set QÌŒn`1 pÂµÌŒn , aÌŒn q â€œ p1 Â´ Î·n qQÌŒn pÂµÌŒn , aÌŒn q ` Î·n fËœpÂµÌŒn , aÌŒn q ` Î³ min 1 QÌŒn pÂµÌŒ1 , aÌŒ1 q
aÌŒ PAÌŒ

7

n`1

return QÌŒNepi

Algorithm 1 returns the table QÌŒNepi after Nepi episodes. We prove below that this table converges to
the optimal Q-function QÌƒËš in a suitable sense. To keep the paper at a reasonable length, we will make
the following simplifying assumptions.
We endow the simplexes S and A respectively with the Euclidean distances dS and dA of the spaces
R|S| and R|A| . Because S is finite, we can identify A defined in (29) with PpAq|S| and endow it with
the distance dA paÌƒ, aÌƒ1 q â€œ supxPS dA paÌƒpxq, aÌƒ1 pxqq for aÌƒ, aÌƒ1 P A. Furthermore, we consider the following
discretizations of the simplexes. Let ÎµS Ä… 0 satisfying: for all Âµ P S, there exists ÂµÌŒ P SÌŒ s.t. dS pÂµ, ÂµÌŒq Ä
ÎµS . Similarly, let ÎµA Ä… 0 satisfying: for all Î½ P A, there exists Î½ÌŒ P AÌŒ such that dA pÎ½, Î½ÌŒq Ä ÎµA . Because
S is finite and the definition of the distance dA , we have for every aÌƒ P A, there exists aÌŒ P AÌŒ, s.t.
dA paÌƒ, aÌŒq Ä ÎµA .

MODEL FREE MEAN FIELD RL

23

Assumption (H3). Regularity of the data: fËœ is bounded and Lipschitz continuous with respect to
pÂµ, aÌƒq with constant LfËœ, namely for every pÂµ, aÌƒq, pÂµ1 , aÌƒ1 q P S Ë† A, we have
`
Ë˜
|fËœpÂµ, aÌƒq Â´ fËœpÂµ1 , aÌƒ1 q| Ä LfËœ }Âµ Â´ Âµ1 }dS ` dA paÌƒ, aÌƒ1 q
and
fËœpÂµ, aÌƒq Ä LfËœ.
Also, FÌ„ is Lipschitz continuous with respect to Âµ and aÌƒ with constant LFÌ„ in expectation over the randomness of the common noise, namely: for every pÂµ, aÌƒq, pÂµ1 , aÌƒ1 q P S Ë† A,
â€œ
â€°
`
Ë˜
EÎµ0 }FÌ„ pÂµ, Âµ bÌ‚ aÌƒ, Îµ0 q Â´ FÌ„ pÂµ1 , Âµ1 bÌ‚ aÌƒ1 , Îµ0 q}dS Ä LFÌ„ }Âµ Â´ Âµ1 }dS ` dA paÌƒ, aÌƒ1 q
Assumption (H4). Regularity of the value function: JÂ¯Ëš is Lipschitz continuous w.r.t. Âµ with
constant LJÂ¯Ëš .
Assumption (H5). Covering time: There exists a finite Tcov such that with probability 1{2 (over
the randomness of the common noise and of Algorithm 1) the following holds: For every starting point
in SÌŒ Ë† AÌŒ, every element of SÌŒ Ë† AÌŒ has been visited before time Tcov during the execution of Algorithm 1.
The regularity of JÂ¯Ëš in (H4) can typically be ensured through suitable conditions on the data of the
problem, as e.g. in [17, 11, 13]. Assumption (H5) is similar to the covering time assumption in [19].
In practice, exploration can be enhanced by adjusting the greediness level and by using exploring
starts (if the learner can query an oracle which simulates transitions from any pÂµ, aÌƒq). Note that the
boundedness of the one-stage cost fËœ from Assumption (H3) together with the fact that Î³ P p0, 1q ensures
the existence of a finite bound JË‡bound for the state value function of the projected MFC problem. We
denote by Î² â€œ p1 Â´ Î³q{2 the horizon of the MDP corresponding to the projected MFC problem, and
for Î´ P p0, 1q, we let Tcov pÎ´q â€œ rTcov log2 p1{p2Î´qqs. We consider projection operators ProjSÌŒ : S Ã‘ SÌŒ
and ProjAÌŒ : A Ã‘ AÌŒ such that pProjSÌŒ pÂµq, ProjAÌŒ paÌƒqq :â€œ pÂµÌŒ, aÌŒq for every pÂµ, aÌƒq P S Ë† A where pÂµÌŒ, aÌŒq is
the closest point (or one of the closest points, in case of equality) in SÌŒ Ë† AÌŒ with respect to dS and dA .
Based on simplexes discretizations, this point satisfies }Âµ Â´ ÂµÌŒ}dS Ä ÎµS and dA paÌƒ, aÌŒq Ä ÎµA .
Theorem 35. Let Î´ P p0, 1q and Îµ Ä… 0. Assume Assumptions (H3)â€“(H5) hold. Consider learning
rates
There exists Îº P p1{2, 1q such that for every pÂµÌŒ, aÌŒq P SÌŒ Ë† AÌŒ, Î·n :â€œ Î·n pÂµÌŒ, aÌŒq â€œ
` pÎ·n qn satisfying:
Ë˜Îº
1{ 1 ` Cpn, ÂµÌŒ, aÌŒq for each n Ä› 0, where Cpn, ÂµÌŒ, aÌŒq is the number of times up to n that the pair pÂµÌŒ, aÌŒq
has been visited in Algorithm 1. If the number of episodes Nepi is of order
Ë›
Â¨Ëœ
1
`
Ë˜Â¸1 Ë†
Ë†Ë‡
Ë™Ë™ 1Â´Îº
2
ln |SÌŒ| |AÌŒ||S| JË‡bound {p2Î´Î²Îµq Îº
pTcov pÎ´qq1`3Îº JË‡bound
pT
pÎ´qq
J
cov
bound
â€š,
`
ln
(34)
â„¦Ë
Î² 2 Îµ2
Î²
Îµ
then with probability 1 Â´ Î´, for all pÂµ, aÌƒq P S Ë† A,
Ë‡
Ë‡
Â´
Â¯
Ë‡
Ë‡
Ë‡QÌŒNepi ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Â´ QÌ„Ëš pÂµ, Âµ bÌ‚ aÌƒqË‡ Ä Îµ1 ,
Ë†
Ë™
Â¯
Î³
1 Â´
1
where Îµ â€œ Îµ `
LJÂ¯Ëš ` LfËœ ` Î³LJÂ¯Ëš LFÌ„ ÎµS `
LfËœ ` Î³LJÂ¯Ëš LFÌ„ ÎµA .
1Â´Î³
1Â´Î³
Note that Îµ can be chosen as small as desired provided Nepi is large enough. The second and the
third terms in the error Îµ1 are proportional to ÎµS and ÎµA , which is somehow unavoidable in general due
to the projection on the finite sets SÌŒ and AÌŒ. However, this error vanishes as ÎµS Ã‘ 0 and ÎµA Ã‘ 0, i.e.,
as SÌŒ and AÌŒ are better and better approximations of PpSq and PpAq respectively.
We prove this result below. The proof can be summarized in the following three steps: (1) For Nepi
large enough, we have QÌŒNepi Â« QÌŒËš on SÌŒ Ë† AÌŒ; (2) QÌŒËš Â« QÌƒËš on SÌŒ Ë† AÌŒ; (3) For every pÂµ, aÌƒq P S Ë† A,

24

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

QÌƒËš pProjSÌŒ pÂµq, ProjAÌŒ paÌƒqq Â« QÌƒËš pÂµ, aÌƒq. The first step relies on standard Q-learning convergence results [19],
while the two other steps stem from the regularity assumptions and the approximation of pS, Aq by
pSÌŒ, AÌŒq.
Proof of Theorem 35. Recall that we denote by JË‡Ëš and QÌŒËš respectively the state value function and the
state-action value function of the projected MFC problem defined by (32)â€“(33).
We first note that, for every pÂµ, aÌƒq P S Ë† A,
Ë‡
Ë˜
`
Ë˜Ë‡Ë‡
`
Ë‡
Ë‡QÌŒNepi ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Â´ QÌƒËš Âµ, aÌƒ Ë‡
Ë‡
Ë˜Ë‡
Ë˜
`
`
Ä Ë‡QÌŒNepi ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Â´ QÌŒËš ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Ë‡
Ë‡ `
Ë˜Ë‡Ë‡
Ë˜
`
Ë‡
` Ë‡QÌŒËš ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Â´ QÌƒËš ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Ë‡
Ë‡ `
Ë˜
`
Ë˜Ë‡Ë‡
Ë‡
` Ë‡QÌƒËš ProjSÌŒ pÂµq, ProjAÌŒ paÌƒq Â´ QÌƒËš Âµ, aÌƒ Ë‡ .
We then split the proof into three steps, which consist in bounding from above each term in the right
hand side.
Step 1. We first analyze the difference between QÌŒNepi and QÌŒËš . This comes from standard convergence
results on Q-learning for finite state-action spaces. More precisely, under Assumptions (H3) and (H5),
with our choice of learning rates, and given that Nepi is of order (34), we can apply Theorem 4 and
Corollary 34 in [19] for asynchronous Q-learning and polynomial learning rates, and we obtain that,
with probability at least 1 Â´ Î´,
Ë‡
Ë‡
Ë‡
Ë‡
}QÌŒNepi Â´ QÌŒËš }8 â€œ sup Ë‡QÌŒNepi pÂµÌŒ, aÌŒq Â´ QÌŒËš pÂµÌŒ, aÌŒqË‡ Ä Îµ.
pÂµÌŒ,aÌŒqPSÌŒË†AÌŒ

Step 2. We then turn our attention to the difference between QÌŒËš and QÌƒËš . The analysis amounts to
say that the projection on SÌŒ realized at each step does not perturb too much the value function. Recall
0
0
that for some given common noise Îµ0 , the operator Î¦ÌŒÎµ : SÌŒ Ë† AÌŒ Ã‘ SÌŒ is given by Î¦ÌŒÎµ pÂµÌŒ, aÌŒq â€œ ProjSÌŒ Ë
0
FÌ„ pÂµÌŒ, ÂµÌŒ bÌ‚ aÌŒ, Îµ0 q. Likewise, we denote the transition dynamic with FÌ„ by a function Î¦Îµ : S Ë† A Ã‘ S
such that:
0
Î¦Îµ pÂµ, aÌƒq â€œ FÌ„ pÂµ, Âµ bÌ‚ aÌƒ, Îµ0 q,
@pÂµ, aÌƒq P S Ë† A.
Let us start by noting that, for every pÂµÌŒ, aÌŒq P SÌŒ Ë† AÌŒ,
Ë‡
Ë‡
Ë‡ Ëš
Ë‡
Ë‡QÌŒ pÂµÌŒ, aÌŒq Â´ QÌƒËš pÂµÌŒ, aÌŒqË‡
ff
Â«
Ë‡
Ë‡
0
Ë‡ Ë‡Ëš Îµ0
Ë‡
Ä Î³E Ë‡J pÎ¦ÌŒ pÂµÌŒ, aÌŒqq Â´ JÂ¯Ëš pÎ¦Îµ pÂµÌŒ, aÌŒqqË‡
ff
Â«
Ë‡
Ë‡ Ë‡
Ë‡
0
0
0
Ë‡ Ë‡Ëš Îµ0
Ë‡
Ë‡
Ë‡
Ä Î³E Ë‡J pÎ¦ÌŒ pÂµÌŒ, aÌŒqq Â´ JÂ¯Ëš pÎ¦ÌŒÎµ pÂµÌŒ, aÌŒqqË‡ ` Ë‡JÂ¯Ëš pÎ¦ÌŒÎµ pÂµÌŒ, aÌŒqq Â´ JÂ¯Ëš pÎ¦Îµ pÂµÌŒ, aÌŒqqË‡
Â«
ff
â€â€º
Ë‡
â€º
Â´ 0
Â¯
Â´ 0
Â¯Ë‡
Ë‡
â€º Îµ0
â€º
Ëš
Îµ
1
Ëš
Îµ
1 Ë‡
Îµ0
Ä Î³E Ë‡ inf QÌŒ Î¦ÌŒ pÂµÌŒ, aÌŒq, aÌŒ Â´ inf
QÌƒ
Î¦ÌŒ
pÂµÌŒ,
aÌŒq,
aÌƒ
`
Î³L
E
Î¦ÌŒ
pÂµÌŒ,
aÌŒq
Â´
Î¦
pÂµÌŒ,
aÌŒq
â€º
Ë‡
â€º
Ëš
Â¯
J
1
aÌŒ1 PAÌŒ

aÌƒ PA

where the last inequality holds by Lipschitz continuity of JÂ¯Ëš on S, see Assumption (H4).

ïš¾
dS

,

MODEL FREE MEAN FIELD RL

25

The second term in the last inequality can be bounded using the simplex discretization properties
and Assumption (H3):
â€ 0
Ä±
â€°
â€œ
0
E }Î¦ÌŒÎµ pÂµÌŒ, aÌŒq Â´ Î¦Îµ pÂµÌŒ, aÌŒq}dS â€œ EÎµ01 }ProjSÌŒ Ë FÌ„ pÂµÌŒ, aÌŒ, Îµ0 q Â´ FÌ„ pÂµÌŒ, aÌŒ, Îµ0 q}dS Ä ÎµS .
0

For the first term, let ÂµÌŒ1 â€œ Î¦ÌŒÎµ pÂµÌŒ, aÌŒq P SÌŒ to alleviate the notation, and let us consider aÌŒËš1 P AÌŒ and
Ëš
aÌƒ2 P A satisfying: QÌŒËš pÂµÌŒ1 , aÌŒËš1 q â€œ inf aÌŒ1 PAÌŒ QÌŒËš pÂµÌŒ1 , aÌŒ1 q and QÌƒËš pÂµÌŒ1 , aÌƒËš2 q â€œ inf aÌƒ1 PA QÌƒËš pÂµÌŒ1 , aÌƒ1 q . The existence of
aÌŒËš1 and aÌƒËš2 is guaranteed respectively by finiteness of S Ë† AÌŒ and by Lemma 33.
We observe that
QÌŒËš pÂµÌŒ1 , aÌŒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q
Â´
Â¯ Â´
Â¯
â€œ QÌŒËš pÂµÌŒ1 , aÌŒËš1 q Â´ QÌŒËš pÂµÌŒ1 , ProjAÌŒ paÌƒËš2 qq ` QÌŒËš pÂµÌŒ1 , ProjAÌŒ paÌƒËš2 qq Â´ QÌƒËš pÂµÌŒ1 , ProjAÌŒ paÌƒËš2 qq
Â´
Â¯
` QÌƒËš pÂµÌŒ1 , ProjAÌŒ paÌƒËš2 qq Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q
Ë‡
Ë‡ Â´
â€°Â¯
â€œ
Ë‡
Ë‡
Ä 0 ` sup Ë‡pQÌŒËš Â´ QÌƒËš qpÂµÌŒ, aÌŒqË‡ ` fËœpÂµÌŒ1 , ProjAÌŒ paÌƒËš2 qq ` Î³EpÎµ0 q1 JÂ¯Ëš pFÌ„ pÂµÌŒ1 , ProjAÌŒ paÌƒËš2 q, pÎµ0 q1 qq
pÂµÌŒ,aÌŒqPSÌŒË†AÌŒ

Â´
â€œ
â€°Â¯
Â´ fËœpÂµÌŒ1 , aÌƒËš2 q ` Î³EpÎµ0 q1 JÂ¯Ëš pFÌ„ pÂµÌŒ1 , aÌƒËš2 , pÎµ0 q1 qq
Ä }QÌŒËš Â´ QÌƒËš }8 ` pLfËœ ` Î³LJÂ¯Ëš LFÌ„ qÎµA .
On the other hand,
Â´
Â¯ Â´
Â¯
QÌŒËš pÂµÌŒ1 , aÌŒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q â€œ Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q Â´ QÌƒËš pÂµÌŒ1 , aÌŒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌŒËš1 q Â´ QÌŒËš pÂµÌŒ1 , aÌŒËš1 q Ä› Â´}QÌŒËš Â´ QÌƒËš }8 .
Combining the above bounds yields that for every pÂµÌŒ, aÌŒq P SÌŒ Ë† AÌŒ,
Ë‡
Ë‡
Â´
Â¯
Ë‡ Ëš
Ë‡
Ë‡QÌŒ pÂµÌŒ, aÌŒq Â´ QÌƒËš pÂµÌŒ, aÌŒqË‡ Ä Î³ }QÌŒËš Â´ QÌƒËš }8 ` pLfËœ ` Î³LJÂ¯Ëš LFÌ„ qÎµA ` Î³LJÂ¯Ëš ÎµS .
Consequently,
}QÌŒËš Â´ QÌƒËš }8 Ä

Â¯
Î³ Â´
pLfËœ ` Î³LJÂ¯Ëš LFÌ„ qÎµA ` LJÂ¯Ëš ÎµS .
1Â´Î³

Step 3. Last, we look at the difference between QÌƒËš pProjSÌŒ pÂµq, ProjAÌŒ paÌƒqq and QÌƒËš pÂµ, aÌƒq. For every
Âµ P S and aÌƒ P A, letting ÂµÌŒ â€œ ProjSÌŒ pÂµq and aÌŒ â€œ ProjAÌŒ paÌƒq to alleviate the notation, we have }ÂµÌŒ Â´ Âµ}dS Ä
ÎµS and }aÌŒ Â´ aÌƒ}dA Ä ÎµA . We obtain
Ë‡
Ë‡
Â«Ë‡
Ë‡ ff
Ë‡
Ë‡
Ë‡ Ë‡
Ë‡
Ë‡
Ë‡ Ëš
Ë‡ Ë‡Ëœ
Ë‡
Ëš
Ëš
1
Ëš
1
Ë‡
QÌƒ
pÎ¦pÂµÌŒ,
aÌŒq,
aÌƒ
q
Â´
inf
QÌƒ
pÎ¦pÂµ,
aÌƒq,
aÌƒ
q
Ë‡QÌƒ pÂµÌŒ, aÌŒq Â´ QÌƒ pÂµ, aÌƒqË‡ Ä Ë‡f pÂµÌŒ, aÌŒq Â´ fËœpÂµ, aÌƒqË‡ ` Î³E Ë‡Ë‡ inf
Ë‡
Ë‡
Ë‡
aÌƒ1 PA
aÌƒ1 PA
Ë‡â€°
â€œË‡
Ä LfËœ p}ÂµÌŒ Â´ Âµ}dS ` }aÌŒ Â´ aÌƒ}dA q ` Î³E Ë‡JÂ¯Ëš pFÌ„ pÂµÌŒ, aÌŒ, Îµ0 q Â´ JÂ¯Ëš pFÌ„ pÂµÌŒ, aÌŒ, Îµ0 qqË‡
â€œ
â€°
Ä LfËœpÎµS ` ÎµA q ` Î³LJÂ¯Ëš E }FÌ„ pÂµÌŒ, aÌŒ, Îµ0 q Â´ FÌ„ pÂµ, aÌƒ, Îµ0 q}dS
Ä pLfËœ ` Î³LJÂ¯Ëš LFÌ„ qpÎµS ` ÎµA q,
where we used the Lipschitz continuity of fËœ, JÂ¯Ëš , FÌ„ and the assumption on SÌŒ, see Assumptions (H3),
(H4) and the simplex discretization properties.


26

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

5.3.2. Q-learning with controls that are pure at level-0. The above method is designed for the case where
one looks for optimal actions that are potentially randomized at the individual level. Searching in the
space PpAq comes with a computational cost that is reflected in the bounds through the cardinality of
the discrete simplex AÌŒ. In some situations it can be interesting to directly search for actions that are
pure at the individual level.
In this case, instead of (29), the set of strategy functions is (for simplicity we keep the notation A):
A :â€œ taÌƒ : S Ã‘ Au â€œ AS .
In Algorithm 1, we replace aÌŒ P AÌŒ by aÌƒ P A.
Theorem 36. Let Î´ P p0, 1q and Îµ Ä… 0. Assume Assumptions (H3)â€“(H5) hold. Consider learning
rates
There exists Îº P p1{2, 1q such that for every pÂµÌŒ, aÌƒq P SÌŒ Ë† AÌƒ, Î·n :â€œ Î·n pÂµÌŒ, aÌƒq â€œ
` pÎ·n qn satisfying:
Ë˜Îº
1{ 1 ` Cpn, ÂµÌŒ, aÌƒq for each n Ä› 0, where Cpn, ÂµÌŒ, aÌƒq is the number of times up to n that the pair pÂµÌŒ, aÌƒq
has been visited in Algorithm 1. If the number of episodes Nepi is of order
Ë›
Â¨Ëœ
1
`
Ë˜ Â¸ Îº1 Ë†
Ë†Ë‡
Ë™Ë™ 1Â´Îº
1`3Îº
2
|S|
Ë‡
Ë‡
pTcov pÎ´qq
Jbound ln |SÌŒ| |A| Jbound {p2Î´Î²Îµq
pTcov pÎ´qq
Jbound
â€š,
`
ln
(35)
â„¦Ë
2
2
Î² Îµ
Î²
Îµ
then with probability 1 Â´ Î´, for all pÂµ, aÌƒq P S Ë† A,
Ë‡
Ë‡
Â´
Â¯
Ë‡
Ë‡
Ëš
QÌŒ
Proj
pÂµq,
aÌƒ
Â´
QÌ„
pÂµ,
Âµ
bÌ‚
aÌƒq
Ë‡ Nepi
Ë‡ Ä Îµ1 ,
SÌŒ
Ë™
Ë†
Î³
1
L Â¯Ëš ` LfËœ ` Î³LJÂ¯Ëš LFÌ„ ÎµS .
where Îµ â€œ Îµ `
1Â´Î³ J
The above result provides convergence guarantee for the Q-function. Let us now derive a consequence
in terms of the optimizer. To this end, we will use the following additional assumption on the gap
between the values of the best and second-best actions, which is rather standard in approximation
algorithms based on tabular Q-functions [20, 8].
Assumption (H6). Action gap: There exists KA Ä… 0 such that:
QÌƒËš pÂµÌŒ, aÌƒq Â´ inf
QÌƒËš pÂµÌŒ, aÌƒ1 q Ä› KA ,
1
aÌƒ PA

ÂµÌŒ P SÌŒ, aÌƒ P Az arg inf
QÌƒËš pÂµÌŒ, aÌƒ1 q.
1
aÌƒ PA

To recover minimizers or approximate minimizers, it will be convenient to work with the following
operators. In general, they are defined on the vector space Rm . For Ï„ Ä… 0 and x â€œ px1 , . . . , xm q P Rm , we
define softminÏ„ : Rm Ã‘ Rm by
Ã¿
softminÏ„ pxq â€œ peÂ´Ï„ x1 , . . . , eÂ´Ï„ xm q{ eÂ´Ï„ xj .
j

For x P Rm , we define argmine : Rm Ã‘ r0, 1sm by
`
Ë˜m
argminepxq â€œ 1iParg minpxq iâ€œ1 {| arg minpxq|.
where arg minpxq â€œ tj P t1, . . . , mu : xj â€œ mintx1 , . . . , xm uu. In the sequel, we use these operators with
the dimension m â€œ |A| â€œ |A||S| . For any function q : A Ã‘ R, we identify q with the vector pqpaÌƒqqaÌƒPA .
Corollary 37. Assume the same assumptions as in Theorem 36 hold and, in addition, that Assumption (H6) holds. Let QÌŒNepi be the table returned by Algorithm 1, and let Îµ1 be as in Theorem 36. Then
for every ÂµÌŒ P SÌŒ,
a
â€º
`
Ë˜
`
Ë˜â€º
â€ºsoftminÏ„ QÌŒN pÂµÌŒ, Â¨q Â´ argmine QÌƒËš pÂµÌŒ, Â¨q â€º Ä Ï„ Îµ1 |A| ` 2eÂ´Ï„ KA |A|.
epi

2

MODEL FREE MEAN FIELD RL

27

The proof is provided below. The argmine in the second term is here in case there are several optimal
controls. The softmin regularizes the best action predicted by the estimation QÌŒNepi of the function QÌƒËš .
Remark 38. Imagine we want the error bound in Corollary 37, to be smaller than some Î´ Ä… 0. It is
a
` |A| Ë˜
1
sufficient to have: for the second term: Ï„ Ä›
log
; and for the first term: Îµ1 Ä Î´{p2Ï„ |A|q â€œ
KA
Î´{4
`
` |A| Ë˜Ë˜
1{2
Î´KA { 2|A| log
. Then both terms in the error bound will be smaller than Î´{2. Notice that,
Î´{4
contrary to Theorem 35, here we do not need to approximate the probability space of action PpAq by
AÌŒ with an ÎµA -net, hence the error bound in Theorem 36 is independent of any ÎµA . So it is possible to
choose Ï„ and to make Îµ1 as small as we want.
Proof of Corollary 37. We use [23, Proposition 4], which states that softminÏ„ is Ï„ -Lipschitz and [27,
Lemma 7], which states that for pxi qiâ€œ1,...,m ,
}softminÏ„ pxq Â´ argminepxq}2 Ä 2meÂ´Ï„ Î´ ,
where Î´ â€œ inf xj Ä…infpxq xj Â´ infpxq, and Î´ â€œ 8 if all xi are equal. We can apply this latter result to
QÌƒËš pÂµÌŒ, Â¨q thanks to assumption (H6), with m â€œ |AÌŒ| and Î´ â€œ KA . Combining this with Theorem 35, we
have, for every ÂµÌŒ,
â€º
Â¯â€ºâ€º
Â¯
Â´
Â´
â€º
Ëš
Ëš
â€ºsoftminÏ„ QÌŒ pÂµÌŒ, Â¨q Â´ argmine QÌƒ pÂµÌŒ, Â¨q â€º
â€º
â€º
2
â€º
â€º
Â´
Â¯
Â´
Â¯â€ºâ€º
Â´
Â¯
Â´
Â¯â€ºâ€º
â€º
â€º
Ëš
Ëš
Ëš
Ëš
â€º
â€º
â€º
Ä â€ºsoftminÏ„ QÌŒ pÂµÌŒ, Â¨q Â´ softminÏ„ QÌƒ pÂµÌŒ, Â¨q â€º ` â€ºsoftminÏ„ QÌƒ pÂµÌŒ, Â¨q Â´ argmine QÌƒ pÂµÌŒ, Â¨q â€ºâ€º
2
2
â€º
â€º
â€º
â€º Ëš
Ä Ï„ â€ºâ€ºQÌŒ pÂµÌŒ, Â¨q Â´ QÌƒËš pÂµÌŒ, Â¨qâ€ºâ€º ` 2|A|eÂ´Ï„ KA
2
Ë‡
Ë‡
a
Ë‡ Ëš
Ë‡
1
Ä Ï„ |A| sup Ë‡QÌŒ pÂµÌŒ, aÌƒ q Â´ QÌƒËš pÂµÌŒ, aÌƒ1 qË‡ ` 2|A|eÂ´Ï„ KA
aÌƒ1 PA
a
1
Ä Ï„ Îµ |A| ` 2eÂ´Ï„ KA |A|.

5.4. Deep reinforcement learning for MFMDP.
The above method has the advantage to be simple enough to let us carry out a detailed analysis.
However, it cannot be used in practice for large state or actions spaces because of the prohibitive
computational cost due to the discretization of the simplexes. An alternative is to work directly with
continuous spaces, in which case the policies and value functions cannot be represented in a tabular
way. Instead, we can rely on function approximation. To this end, we now propose to use methods
from deep reinforcement learning which are more suitable for continuous spaces. The motivations are
twofold.
First, if S and A are finite but we want to learn an optimal policy that is potentially randomized at
level-0, the discretization approach proposed in Â§ 5.3.1 has a complexity that increases with the number
of points in the discretization of PpAq, which itself increases exponentially quickly with the cardinality
of A. For this reason, it can be interesting to tackle directly AÌ„ â€œ PpAq as a continuous action space
and to use deep RL methods for continuous action space MDPs.
Second, some MFC problems are naturally posed with a continuous state space S. In this case, under
mild conditions, the optimal policy is in fact non-randomized not only at the level-1 but even at the

28

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

level-0. However, the state of the mean field MDP is an element of the infinite dimensional space PpSq.
From a numerical viewpoint, we need two ingredients: (1) a finite-dimensional approximation of the
mean field and (2) a parameterized approximation of the value function or the policy taking this finitedimensional representation of the mean field state as an input. For the second point, we will again use
deep neural networks. For the first point, for the sake of definiteness, we choose to simply replace PpSq
by PpSÌŒq where SÌŒ is a discretization of S with a finite number of points. We assume that, given ÂµÌŒ P PpSÌŒq
and aÌ„ : S Ã‘ A, one can get from the environment a sample of the next state and the associated cost
fËœpÂµÌŒ, aÌ„q. The problem thus boils down to an MDP with finite dimensional (but potentially continuous)
state and action spaces. Such MDPs can be solved with a variety of deep RL algorithms. In the sequel,
we provide numerical illustrations based on the Deep Deterministic Policy Gradient (DDPG) proposed
in [32]. It relies on two neural networks, one for the Q-function (the critic) and one for the policy
(the actor). The heart of the algorithm consists in updating alternatively the critic by minimizing an
empirical square error and the actor by making one step of gradient descent. To improve exploration,
a Gaussian noise an`1 is added to the action prescribed by the actor. Furthermore, for more stability,
target networks are also added. The algorithm is summarized in our setting in Algorithm 2 in the
Appendix E.

6. Numerical Examples
6.1. Example 1: Cyber security model.
For a first testbed, we start with a finite state problem. We revisit the cyber security example
introduced in [30], but here from the point of view of a central planner (such as a large company or a
state) trying to protect its computers against the attacks of a hacker. The situation can thus be phrased
as a MFC problem.
In this model, the population consists of a large group of computers which can be either defended
(D) or undefended (U), and either infected (I) or susceptible (S) of infection. Hence the set S has four
elements corresponding to the four possible combinations: DI, DS, UI, US. The action set is A â€œ t0, 1u,
where 0 is interpreted as the fact that the central planner is satisfied with the current level of protection
(D or U) of the computer under consideration, whereas 1 means that she wants to change this level of
protection. In the latter case, the update occurs at a (fixed) rate Î» Ä… 0. If the controls are pure at
level-0, at each of the four states, the central planner only chooses one action per state and applies it to
all the computers at that state. If the controls are mixed at level-0, then for each state, she chooses a
distribution over actions and then each computer in this state picks independently an action according
D or q U depending
to the chosen distribution. When infected, each computer may recover at rate qrec
rec
on whether it is defended or not. On the other hand, a computer may be infected either directly by a
D (resp. v q U ) if it is defended (resp. undefended), or by undefended infected
hacker, at rate vH qinf
H inf
computers, at rate Î²U U ÂµptU Iuq (resp. Î²U D ÂµptU Iuq) if it is undefended (resp. defended), or by defended
infected computers, at rate Î²DU ÂµptDIuq (resp. Î²DD ÂµptDIuq) if it is undefended (resp. defended). Here
vH can be interpreted as the attack intensity parameter.
In short, the transition matrix is given by:
Ë›
Â¨
Âµ,a
. . . PDSÃ‘DI
Î»a
0
D
Ëšqrec
...
0
Î»a â€¹
â€¹
(36)
P Âµ,a â€œ Ëš
Âµ,a
Ë Î»a
0
. . . PU SÃ‘U I â€š
U
0
Î»a
qrec
...

MODEL FREE MEAN FIELD RL

29

where
Âµ,a
D
PDSÃ‘DI
â€œ vH qinf
` Î²DD ÂµptDIuq ` Î²U D ÂµptU Iuq,
U
PUÂµ,a
SÃ‘U I â€œ vH qinf ` Î²U U ÂµptU Iuq ` Î²DU ÂµptDIuq,

and all the instances of . . . should be replaced by the negative of the sum of the entries of the row in
which . . . appears on the diagonal. At each time step, the central planner pays a protection cost kD Ä… 0
for each defended computer, and a penalty kI Ä… 0 for each infected computer. The instantaneous cost
in the MFMDP is thus defined as:
fÂ¯pÂµ, aÌ„q â€œ kD ÂµptDI, DSuq ` kI ÂµptDI, U Iuq,
pÂµ, aÌ„q P SÌ„ Ë† AÌ„.
The optimal control and optimal flow of distributions can be characterized by a forward-backward ODE
system which can be obtained in way similar to what is done in the MFG setting e.g. in [12, Â§ 7.2.3].
We will use this solution as a benchmark.
Tabular Q-learning. For the sake of illustration, we present results obtained by tabular Q-learning
with simplex discretization as described in Â§ 5.3. The state space for
Å™ the population distribution is SÌ„,
which is identified with the simplex S â€œ tpÂµpiq qiâ€œ1,...,4 P r0, 1s4 : i Âµpiq â€œ 1u. To follow the original
setting considered in [30], we consider pure controls, both at the common and idiosyncratic levels (level-0
and level-1). So we identify AÌ„ with the set of functions AS , which is finite and of cardinality 24 â€œ 16.
We replace S by the finite set:
!
)
Ã¿
SÌŒ â€œ pÂµpiq qiâ€œ1,...,4 P r0, 1{Nm , . . . , 1 Â´ 1{Nm , 1s4 :
Âµpiq â€œ 1 ,
i

where r0, 1{Nm , . . . , 1 Â´ 1{Nm , 1s is a uniform grid over r0, 1s with Nm ` 1 Ä› 2 points. We then aim
at computing the Q-function for the projected MDP with finite state space SÌŒ and action space AS ,
that we still denote by QÌƒËš although we do not consider mixed actions at the level-0. We note that, in
the absence of common noise, the MFMDP is completely deterministic hence it would be enough to
query once each state-action pair from the environment in order to learn the level-1 reward function
and transition function, and hence to be able to compute perfectly the Q-function. However, for the
sake of illustration, we stick to applying Algorithm 1, replacing both A and AÌŒ by AS .
In order to be able to compare with the benchmark solution obtained by the ODE method, we
considerthat the time steps are of size not 1 but âˆ†t â€œ 0.01. Although the problem is set on an infinite
horizon, we truncate the training episodes and the plots at the horizon T â€œ 10.
After Nepi episodes of Q-learning, we obtain an approximation QÌŒNepi of the Q-function, from which we
can recover an approximation aÌ„Nepi of the optimal control by taking the argmax, namely: aÌ„Nepi pÂµ, Â¨q â€œ
arg maxaÌŒPAS QÌŒNepi pProjSÌŒ pÂµq, aÌŒq. We compare the flow of distributions induced by this control aÌ„Nepi with
the optimally controlled flow computed by the ODE method. This method also allows us to compute for
each t the value JÂ¯Ëš pÂµËšt q along the optimal flow pÂµËšt qtPr0,T s , in line with Lemma 34, we compare it with
maxAS QÌŒNepi pProjSÌŒ pÂµËšt q, Â¨q â€œ QÌŒNepi pProjSÌŒ pÂµËšt q, aÌ„Nepi pÂµËšt qq. Figures 1â€“3 show the results for three initial
conditions Âµ0 . We see that the learnt value function approximately matches the JÂ¯Ëš value function, and
the induced flows of distributions approximately match the benchmark ones. For these simulations, we
used Nm â€œ 30, Î³ â€œ 0.5, and the following parameters:
$
Î²
â€œ 0.3, Î²U D â€œ 0.4, Î²DU â€œ 0.3, Î²DD â€œ 0.4,
â€™
& UU
D
U
D
U
qrec â€œ 0.5, qrec
â€œ 0.4, qinf
â€œ 0.4, qinf
â€œ 0.3,
â€™
%
vH â€œ 0.6, Î» â€œ 0.8, kD â€œ 0.3, kI â€œ 0.5.

30

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

0.46
0.5

0.44

0.4

0.42
mODE(x = 1)
mODE(x = 2)
mODE(x = 3)
mODE(x = 4)

0.3
0.2

mQ(x = 1)
mQ(x = 2)
mQ(x = 3)
mQ(x = 4)

value

m

Vopt
VQ

0.40
0.38
0.36

0.1

0.34

0.0
0

2

4

time

6

8

0.32

10

0

2

4

time

6

8

10

Figure 1. Example 1: Cyber security model. Test case 1: m0 â€œ p1{4, 1{4, 1{4, 1{4q.
Left: Evolution of the distribution when using the benchmark optimal control (mODE )
or the control recovered from the learnt Q-function (mQ ). Right: state value function
using the benchmark solution (Vopt ) or the learnt Q-function (VQ ) along the optimal
mean field flow. The benchmark solution is obtained using the ODE method.

1.0

mODE(x = 1)
mODE(x = 2)
mODE(x = 3)
mODE(x = 4)

0.8

mQ(x = 1)
mQ(x = 2)
mQ(x = 3)
mQ(x = 4)

Vopt
VQ

0.70
0.65
0.60

m

value

0.6
0.4

0.55
0.50
0.45
0.40

0.2

0.35
0.0
0

2

4

time

6

8

10

0.30

0

2

4

time

6

8

10

Figure 2. Example 1: Cyber security model. Test case 2: m0 â€œ p1, 0, 0, 0q. Left:
Evolution of the distribution when using the benchmark optimal control (mODE ) or the
control recovered from the learnt Q-function (mQ ). Right: state value function using the
benchmark solution (Vopt ) or the learnt Q-function (VQ ) along the optimal mean field
flow. The benchmark solution is obtained using the ODE method.

Deep Deterministic Policy Gradient. The solution can also be learnt by using the deep RL
algorithm given in Algorithm 2 instead of mean field Q-learning given in Algorithm 1. In the present
case, this approach has the advantage of avoiding the discretization of PpSq since we instead directly
deal with the distribution as a vector in dimension 4. Since, with this method, it is possible to allow
the control to take continuous values, we replace A â€œ t0, 1u by A â€œ r0, 1s.

MODEL FREE MEAN FIELD RL

1.0

mODE(x = 1)
mODE(x = 2)
mODE(x = 3)
mODE(x = 4)

0.8

31

Vopt
VQ

mQ(x = 1)
mQ(x = 2)
mQ(x = 3)
mQ(x = 4)

0.30

0.6
m

value

0.25

0.4

0.20

0.2

0.15

0.0
0

2

4

time

6

8

10

0

2

4

time

6

8

10

Figure 3. Example 1: Cyber security model. Test case 3: m0 â€œ p0, 0, 0, 1q. Left:
Evolution of the distribution when using the benchmark optimal control (mODE ) or the
control recovered from the learnt Q-function (mQ ). Right: state value function using the
benchmark solution (Vopt ) or the learnt Q-function (VQ ) along the optimal mean field
flow. The benchmark solution is obtained using the ODE method.
Furthermore, to make things more interesting, we consider that the attack intensity parameter vH
is stochastic. Since its value affects the evolution of the whole population, we model this using a
common noise. We replace the state distribution by a conditional state distribution, conditioned on the
realization of 0 up to the current time. To wit, let p0n qnÄ›1 be a sequence of i.i.d. random variables
with Gaussian distribution. Let vH,n`1 â€œ vH,n ` 0n`1 , n Ä› 0, vH,0 given. The evolution from time n
to time n ` 1 of the state distribution is by the transition matrix defined in (36) but with the constant
Âµ,a
, PUÂµ,a
PDSÃ‘DI
SÃ‘U I replaced by the following stochastic coefficients that evolve in time due to the fact
that vH is replaced by a stochastic process:
Âµ,a
D
` Î²DD ÂµptDIuq ` Î²U D ÂµptU Iuq,
PDSÃ‘DI,n
â€œ vH,n qinf
U
PUÂµ,a
SÃ‘U I,n â€œ vH,n qinf ` Î²U U ÂµptU Iuq ` Î²DU ÂµptDIuq,

Using the DDPG method described above, we train the neural networks by picking at each episode
a random initial distribution Âµ and a random sequence of common noises 0 . Fig. 4 displays the
evolution of the population when using the learnt control starting from five initial distributions of
the testing set and one initial distribution of the training set. The testing set of initial distributions is:
tp0.25, 0.25, 0.25, 0.25q, p1, 0, 0, 0q, p0, 0, 0, 1q, p0.3, 0.1, 0.3, 0.1q, p0.5, 0.2, 0.2, 0.1qu. Consistently with the
case without common noise, we see that the distribution always evolves towards a configuration in which
there is no defended agents, and the proportion of undefended infected and undefended susceptible are
roughly 0.43 and 0.57 respectively. Due to the common noise, the distribution is not perfectly stable;
it oscillates around these values. Figure 5 shows from two perspectives the evolution of the mean field
state dynamics when applying the learnt optimal control. The initial distributions are on a uniform
grid of the simplex and time steps are distinguished by colors. We see that for any initial distribution,
as time increases, the mean field states concentrate around the aforementioned point, which lies on an
edge of the simplex.
6.2. Example 2: Discrete distribution planning.

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

0.6

1.0

0.5

0.2

0.6
0.4

0.1

0.2

0.0

0.0
0

2

4

t

6

8

10

0.4

0.5
DI
DS
UI
US

0.0
2

4

t

6

8

10

0.1
0.0
4

t

6

8

(d) Test distribution 4

10

4

t

6

8

10

0.5

DI
DS
UI
US

0.3
0.2

0.4

DI
DS
UI
US

0.3
0.2
0.1
0.0

0.0
2

2

(c) Test distribution 3

0.1
0

0

0.6

0.4
distribution

distribution

0.6

0.2

0.4

(b) Test distribution 2

0.5

0.3

0.6

0.2

0

(a) Test distribution 1

DI
DS
UI
US

0.8
distribution

distribution

DI
DS
UI
US

0.3

distribution

0.8

0.4

1.0

DI
DS
UI
US

distribution

32

0

2

4

t

6

8

(e) Test distribution 5

10

0

2

4

t

6

8

10

(f) Random training distribution

Figure 4. Cyber security example: Evolution of the distribution in the presence of
common noise when applying the control learnt by DDPG on a testing set of five initial
distributions and one random initial distribution of the training set.

(a) Scatter plot of the distribution trajectories. The color gradient correspond to time.

(b) Scatter plot with projection on each dimension of a density estimate using Gaussian kernel density estimation.

Figure 5. Cyber security example: Trajectories of the distributionâ€™s last three coordinates (DS, UI, US) in the simplex, when subject to common noise and controlled by the
control learnt with DDPG. The initial distributions are from a uniform discretization of
the simplex with mesh size 0.1 in each dimension.

MODEL FREE MEAN FIELD RL

33

We now consider an MFC problem in which the goal is to match a target distribution. We take a model
with Nstates â€œ 10 states and 3 actions (left, stay, right): X â€œ t1, . . . , 10u and A â€œ tL, S, Ru. When at the
leftmost state (resp. rightmost), the agents cannot go left (resp. right). The incurred to a representative
agent is 1 if they move (i.e., they use action left or right) plus the L2 distance between the population
distribution and a target distribution. Here we chose: p0, 0, 0.05, 0.1, 0.2, 0.3, 0.2, 0.1, 0.05, 0, 0q for the
target distribution. There is no idiosyncratic noise. A key point is that, in this setting, except for some
specific pairs of initial and target distributions, it is not possible for the population to match the target
distribution unless the agents are allowed to randomize their actions at the individual level. So we use
PpAqX for the level-1 action space. Hence the action space is naturally continuous and this justifies,
here again, the use of the DDPG method.
We present results without and with common noise in the dynamics. In the second case, the common
noise is an i.i.d. additive 0n at each time step n, with 0n â€œ Â´1 with probability 0.05, 0n â€œ 1 with
probability 0.05 and 0n â€œ 0 with probability 0.9. In both cases, the initial distributions for training are
picked randomly as follows. First we pick xmin and xmax uniformly at random between 1 and Nstates .
This determines a sub-set (with periodicity) txmin , . . . , xmax u. For each point in the sub-interval, a value
is picked independently and uniformly at random in r0, 1s. Then the discrete distribution is normalized
to have total mass 1. For numerical reasons, we stop after a finite number of time steps. Here we took
100 time steps.
Figures 6 and 7 present the results obtained without and with common noise, respectively. In each
case, the results are for five different testing distributions: four fixed test distributions, as well as one
random distribution. The left column displays the state distribution: initial distribution in green, target
distribution in red, last distribution in purple, and the average over the last few steps before terminal
time in blue. We see that the last distribution is very close to the target one so the learnt control is
successful. The two columns in the middle display the control distribution at time 0 and at terminal
time. For each state, the probability of picking each action is represented by a vertical bar, with one
color per possible action. We see that, at initial time, the most likely choice is to move to the right (resp.
left) for states on the left (resp. right) of the domain. At terminal time, the most likely choice is to stay
at the current location, because the target distribution has been reached. The right column displays
the trajectory of the common noise that has affected the distribution in the present run (constant equal
to 0 in Figure 6). On Figure 7, we see that even when the common noise is rather strong, the learnt
control manages to move the initial distribution close to the target distribution. At the bottom of each
figure is displayed the evolution of the training reward (the negative of the MFC cost) along the training
iterations (also called episodes) of DDPG.
6.3. Example 3: Swarm motion.
We then turn our attention to a model in continuous state and action spaces. More precisely, we
consider a model of swarm motion with aversion to crowded regions introduced in [6] (in the context
of mean field games). Although many variants are possible, we define the model in the following way
in order to have an analytical solution that can be used to assess the convergence of our proposed
method. We take the interval r0, 1s with periodic boundary condition, i.e. the unit torus T, as the
state space S. The action space is A â€œ R. The dynamics of a typical agent is driven by (1) with
F px, a, Âµ, e, e0 q â€œ a ` e ` e0 . In other words, the central planner chooses the velocity of each agent.
The instantaneous reward of a typical agent at location x and using action a while the populationâ€™s
state is Âµ, is defined as: f px, a, Âµq â€œ Â´ 12 |a|2 ` Ï•pxq Â´ lnpÂµpxqq. Here, the first term penalizes a large
velocity (it can be interpreted as a kind of cost proportional to the kinetic energy of the agent), Ï•
encodes spatial preferences (by giving a lower cost for certain positions in space), and the last term

34

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

models crowd aversion (it penalizes the fact of being at a location where the density of agents is high).
We choose:
â€œ
â€°
Ï•pxq â€œ Â´2Ï€ 2 Â´ sinp2Ï€xq ` | cosp2Ï€xq|2 ` 2 sinp2Ï€xq,
We consider that there is no common noise (Îµ0n â€œ 0 for all n), and the idiosyncratic noises Îµn have
a Gaussian distribution. We obtain a model which, in continuous time, admits an explicit ergodic
solution that we can use as a benchmark. Indeed, in this case the optimal ergodic control is given
by aÌƒpxq â€œ 2Ï€ cosp2Ï€xq
and the ergodic distribution of the corresponding MKV dynamics has density
ÅŸ
1
Âµpxq â€œ e2 sinp2Ï€xq { e2 sinp2Ï€x q dx1 .
The action space being continuous, here again the use of DDPG is justified. To implement this
approach, we however need a finite dimensional representation of the distribution in order to pass it to
the policy network and the value function network. We replace PpTq by a finite dimensional simplex
Ppt0, 1{Np , . . . , 1 Â´ 1{Np , 1uq corresponding to a uniform discretization of T with Np ` 1 points. The
environment (whose inner working is not known to the learning agent) needs to compute the evolution
of the distribution. This evolution can be directly simulated with a deterministic method based, for
example, on a finite difference scheme as in [2]. However, in practice, it is likely that the environment
would not work in this way but would rather correspond to moving forward a large population of agents
(e.g., robots). This induces extra approximations. To illustrate that our method can be applied in such
situations, here we chose to implement the environment using a probabilistic approach based on Monte
Carlo simulations for a large number of particles on S. Then, to prepare the input for the Q-function,
we project their positions on t0, 1{Np , . . . , 1 Â´ 1{Np , 1u and approximate the mean field distribution by
a histogram. We recall that the DDPG method uses this environment as a black-box and, for a given
action aÌƒ P RNp , can only access the resulting new distribution and the associated reward. The actor and
critic networks have been implemented using a feedforward fully connected architecture with 2 hidden
layers of width at most 300 neurons. We used random initial states at each episode, and the noise used
on the action is a Gaussian noise with mean 0 and variance 0.02. We used Adam optimizer with initial
learning rate 0.0001 and minibatches of size 16.
Figure 8 presents results obtained using this method after 160 episodes. The system has been trained
on initial distributions which are Gaussian with random mean and random variance. As illustrated in
the figures (left column), the system has learnt how to drive this type of initial distributions towards
the analytical stationary distribution and then how to use an approximation of the stationary optimal
control (right column) in order to keep the system in the stationary regime. The middle column displays
the learnt control for the initial distribution. It is not expected to match the optimal ergodic control,
which should be applied when the distribution has reached the ergodic regime and here it is provided
only for the sake of comparison.

References
[1] Achdou, Y., Camilli, F., and Capuzzo-Dolcetta, I. (2012). Mean field games: numerical methods for the
planning problem. SIAM J. Control Optim., 50(1):77â€“109.
[2] Achdou, Y. and Capuzzo-Dolcetta, I. (2010). Mean field games: numerical methods. SIAM J. Numer. Anal.,
48(3):1136â€“1162.
[3] Achdou, Y. and LaurieÌ€re, M. (2016). Mean Field Type Control with Congestion (II): An augmented Lagrangian
method. Appl. Math. Optim., 74(3):535â€“578.
[4] Agram, N., Bakdi, A., and Oksendal, B. (2020). Deep learning and stochastic mean-field control for a neural
network model. Available at SSRN 3639022.

MODEL FREE MEAN FIELD RL

35

[5] Al-Aradi, A., Correia, A., Naiff, D. d. F., Jardim, G., and Saporito, Y. (2019). Applications of the deep
galerkin method to solving partial integro-differential and hamilton-jacobi-bellman equations. arXiv preprint
arXiv:1912.01455.
[6] Almulla, N., Ferreira, R., and Gomes, D. (2017). Two numerical approaches to stationary mean-field games.
Dyn. Games Appl., 7(4):657â€“682.
[7] Anahtarci, B., Kariksiz, C. D., and Saldi, N. (2020). Q-learning in regularized mean-field games. arXiv
preprint arXiv:2003.12151.
[8] Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S., and Munos, R. (2016). Increasing the action gap:
New operators for reinforcement learning. In Thirtieth AAAI Conference on Artificial Intelligence.
[9] Bensoussan, A., Frehse, J., and Yam, S. C. P. (2013). Mean field games and mean field type control theory.
Springer Briefs in Mathematics. Springer, New York.
[10] Bertsekas, D. P. and Shreve, S. (2004). Stochastic optimal control: the discrete-time case.
[11] Cardaliaguet, P., Delarue, F., Lasry, J.-M., and Lions, P.-L. (2019). The master equation and the convergence problem in mean field games, volume 201 of Annals of Mathematics Studies. Princeton University Press,
Princeton, NJ.
[12] Carmona, R. and Delarue, F. (2018a). Probabilistic theory of mean field games with applications. I, volume 83
of Probability Theory and Stochastic Modelling. Springer, Cham. Mean field FBSDEs, control, and games.
[13] Carmona, R. and Delarue, F. (2018b). Probabilistic theory of mean field games with applications. II, volume 84
of Probability Theory and Stochastic Modelling. Springer, Cham. Mean field games with common noise and master
equations.
[14] Carmona, R., Hamidouche, K., LaurieÌ€re, M., and Tan, Z. (2020). Policy optimization for linear-quadratic
zero-sum mean-field type games. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 1038â€“
1043. IEEE.
[15] Carmona, R. and LaurieÌ€re, M. (2019). Convergence analysis of machine learning algorithms for the numerical
solution of mean field control and games: Iiâ€“the finite horizon case. arXiv preprint arXiv:1908.01613. To appear
in Annals of Probability.
[16] Carmona, R. and LaurieÌ€re, M. (2021). Convergence analysis of machine learning algorithms for the numerical
solution of mean field control and games i: The ergodic case. SIAM Journal on Numerical Analysis, 59(3):1455â€“
1485.
[17] Chassagneux, J.-F., Crisan, D., and Delarue, F. (2014). A probabilistic approach to classical solutions of the
master equation for large population equilibria. arXiv:1411.3009.
[18] Elie, R., Perolat, J., LaurieÌ€re, M., Geist, M., and Pietquin, O. (2020). On the convergence of model free
learning in mean field games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
7143â€“7150.
[19] Even-Dar, E. and Mansour, Y. (2003). Learning rates for Q-learning. J. Mach. Learn. Res., 5:1â€“25.
[20] Farahmand, A.-m. (2011). Action-gap phenomenon in reinforcement learning. In Advances in Neural Information Processing Systems, pages 172â€“180.
[21] Fouque, J.-P. and Zhang, Z. (2020). Deep learning methods for mean field control problems with delay.
Frontiers in Applied Mathematics and Statistics, 6:11.
[22] Fu, Z., Yang, Z., Chen, Y., and Wang, Z. (2019). Actor-critic provably finds nash equilibria of linear-quadratic
mean-field games. In International Conference on Learning Representations.
[23] Gao, B. and Pavel, L. (2017). On the properties of the softmax function with application in game theory
and reinforcement learning. arXiv preprint arXiv:1704.00805.
[24] Germain, M., Mikael, J., and Warin, X. (2019). Numerical resolution of mckean-vlasov fbsdes using neural
networks. arXiv preprint arXiv:1909.12678.
[25] Gu, H., Guo, X., Wei, X., and Xu, R. (2019). Dynamic programming principles for mean-field controls with
learning. arXiv preprint arXiv:1911.07314.
[26] Gu, H., Guo, X., Wei, X., and Xu, R. (2020). Mean-field controls with q-learning for cooperative marl:
Convergence and complexity analysis. arXiv preprint arXiv:2002.04131.

36

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

[27] Guo, X., Hu, A., Xu, R., and Zhang, J. (2019). Learning mean-field games. Advances in Neural Information
Processing Systems, 32:4966â€“4976.
[28] Huang, M., MalhameÌ, R. P., and Caines, P. E. (2006). Large population stochastic dynamic games: closedloop McKean-Vlasov systems and the Nash certainty equivalence principle. Commun. Inf. Syst., 6(3):221â€“251.
[29] Kallenberg, O. (2017). Random measures, theory and applications. Springer.
[30] Kolokoltsov, V. N. and Bensoussan, A. (2016). Mean-field-game model for botnet defense in cyber-security.
Appl. Math. Optim., 74(3):669â€“692.
[31] Lasry, J.-M. and Lions, P.-L. (2007). Mean field games. Jpn. J. Math., 2(1):229â€“260.
[32] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2016).
Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning
Representations (ICLR 2016).
[33] Motte, M. and Pham, H. (2019). Mean-field markov decision processes with common noise and open-loop
controls. arXiv preprint arXiv:1912.07883.
[34] Perrin, S., PeÌrolat, J., LaurieÌ€re, M., Geist, M., Elie, R., and Pietquin, O. (2020). Fictitious play for mean
field games: Continuous time analysis and applications. Advances in Neural Information Processing Systems.
[35] Ruthotto, L., Osher, S. J., Li, W., Nurbekyan, L., and Fung, S. W. (2020). A machine learning framework for
solving high-dimensional mean field game and mean field control problems. Proceedings of the National Academy
of Sciences, 117(17):9183â€“9193.
[36] Subramanian, J. and Mahajan, A. (2019). Reinforcement learning in stationary mean-field games. In
Proceedings. 18th International Conference on Autonomous Agents and Multiagent Systems.

MODEL FREE MEAN FIELD RL

Figure 6. Example 2: Discrete distribution planning. Case without common noise; 5
testing distributions. Column 1: state distribution; columns 2 and 3: action distribution
at initial and terminal time; column 4: common noise trajectory (identically 0 here).
Bottom: evolution of the reward during training.

37

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN
State distribution

Action distribution on initial state distribution

0.30

0.20
0.15
0.10

0.6
0.4

10

0.0

12

State distribution

0.15

0.8

0.5
0.4
0.3

x

8

10

0.0

12

State distribution

0.20
0.15

8

10

0.0

12

0.20
0.15
0.10
0.05
x

8

10

12

0.30

2

4

6

x

8

10

0.20
0.15
0.10

6

x

8

10

12

12

0

20

40

t

60

80

100

Common noise trajectory
1.0

Left
Stay
Right

Common noise

0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0

2

4

6

x

8

10

12

0

Action distribution on last state distribution

20

40

t

60

80

100

Common noise trajectory
4

Left
Stay
Right

Common noise

3

1

0.2

0

0.8

0.0

0.0

x

8

10

12

1.0

Left
Stay
Right

0.6
0.4

0.0

6

x

8

10

12

0.8

4

6

x

8

10

40

t

60

80

Common noise

1
2
3
4
5
6
7

2

4

6

x

8

10

12

0

Action distribution on last state distribution

20

40

t

60

80

Common noise

2

0.4

4
6
8

2

4

6

1000

1200

1400

x

8

10

12

0

20

40

t

60

80

60
reward

80
100
120
140
160
200

400

600

800
episode

100

Common noise trajectory
0

Left
Stay
Right

40

0

100

Common noise trajectory

0.6

0.0

12

20

0

Left
Stay
Right

0.2
2

0

0.4
0.2

6

4

0.6

0.2
4

1
2

Action distribution on last state distribution

0.4

2

2

0.4

1.0

Left
Stay
Right

0.2

0.05

10

0.6

0.0

12

0.8
distribution

0.25

4

0.8

Action distribution on initial state distribution
target
init
last
avg50

2

1.0

Left
Stay
Right

0.6

State distribution

0.35

12

0.8
distribution

0.25

8

0.4

Action distribution on initial state distribution

0.30

6

10

1.0

target
init
last
avg20

0.35

4

8

0.3

State distribution
0.40

2

x

0.4

0.1
x

6

0.5

0.05
6

x

0.6

0.2

4

4

0.7

0.10

2

2

0.8

distribution

0.25

6

0.6

0.0

Action distribution on initial state distribution
target
init
last
avg20

0.30

4

0.2

distribution

6

2.0
2

Action distribution on last state distribution

distribution

4

1.5

1.0

distribution

2

1.0

0.2
0.0

12

0.1

0.35
distribution

10

0.2

0.40

distribution

8

0.5

0.4

0.6

0.05

distribution

x

Left
Stay
Right

0.7

0.10

0.00

6

0.8

distribution

distribution

0.20

0.00

4

0.0

0.6

Action distribution on initial state distribution
target
init
last
avg20

0.25

0.00

2

0.5

CN

8

Common noise

CN

x

Common noise trajectory
1.0

Left
Stay
Right

CN

6

Action distribution on last state distribution

CN

4

distribution

2

0.30

0.00

0.8

0.2

0.05
0.00

1.0

Left
Stay
Right

0.8
distribution

0.25
distribution

1.0

target
init
last
avg20

distribution

0.35

CN

38

Figure 7. Example 2: Discrete distribution planning. Case with common noise; 5
testing distributions. Column 1: state distribution; columns 2 and 3: action distribution
at initial and terminal time; column 4: common noise trajectory (identically 0 here).
Bottom: evolution of the reward during training.

100

MODEL FREE MEAN FIELD RL

Figure 8. Swarm motion: Left: Distribution induced, middle: control learnt for distribution at time 0, right: control learnt for distribution at terminal time. For three
different initial distributions. Red: ergodic analytical solution; green: initial distribution; purple: distribution at terminal time. Bottom: rewards.

39

40

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

Appendix A. Auxiliary results for Section 2
Lemma 39. Given a level-0 control process a, there exists a level-0 action process Î± which is a realization of
a. Moreover, if another level-0 action process Î±1 is also a realization of a, then for every n Ä› 0 and for every
bounded Borel measurable function h : A Ã‘ R, we have
Å¼
â€œ
â€°
1
c
(37)
E hpÎ±n q | Gn â€œ
hpÎ±qan pdÎ±q â€œ E rhpÎ±n q | Gnc s ,
P Â´ a.s..
A

Proof. By definition, for each n Ä› 0, an is of the form:
an pdÎ±q â€œ Îºan pU , Ï‘nÂ´1 , Ï‘0n , Îµn , Îµ0n qpdÎ±q,

P Â´ a.s. ,

for some measurable function Îºan on Î¥ Ë† Î˜nÂ´1 Ë† pÎ˜0 qn Ë† E n Ë† pE 0 qn with values in PpAq. Let us denote by Î¾n
the random element pU , Ï‘nÂ´1 , Ï‘0n , Îµn , Îµ0n q, by ÏA the Blackwell-Dubins function (see Lemma 2) of the space A.
Let us set Un â€œ hpÏ‘n q and:
Î±n pÏ‰q :â€œ ÏA pÎºan pÎ¾n pÏ‰qq, Un pÏ‰qq.
a
Then, Î±n is Gn -measurable, and because the ÏƒÂ´field Gnc â€œ ÏƒtÎ¾n u is independent of Un , we have LpÎ±n |Gnc q â€œ
Îºan pÎ¾n q â€œ an , P-almost surely. Equality (37) directly follows the definition of a conditional distribution.

Lemma 40. For each n Ä› 0, the law of the random measure P0pXn ,Î±n q depends only upon the open-loop policy Ï€
as long as Î± is a realization of the control process generated by Ï€.
Proof. Let Î± be an action
â€œ ` processË˜â€°which is a realization of the control process generated by Ï€. For each integer
n Ä› 0, we compute E Î¦ P0pXn ,Î±n q for a family of bounded measurable functions Î¦ on PpS Ë† Aq which generate
the Borel Ïƒ-field of PpS Ë† Aq. For the sake of definiteness we work with functions Î¦ of the form:
m Å¼
Åº
Ï•j px, Î±q Âµpdx, dÎ±q
Î¦pÂµq â€œ
jâ€œ1 SË†A

for a finite set Ï•1 , Â¨ Â¨ Â¨ , Ï•m of bounded continuous functions on S Ë† A. We have:
m Å¼
â€Åº
Ä±
â€œ `
Ë˜â€°
E Î¦ P0pXn ,Î±n q â€œ E
Ï•j px, Î±q P0pXn ,Î±n q pdx, dÎ±q
(38)

jâ€œ1 SË†A
m
â€Åº

Ä±
â€œ
E Ï•j pXn , Î±n q |ÏƒtÏ‘0n , Îµ0n u .

â€œE
jâ€œ1

Now for each j P t1, Â¨ Â¨ Â¨ , mu we have
Ä±
â€œ
E Ï•j pXn , Î±n q |ÏƒtÏ‘0n , Îµ0n u
Å¼
Å¼
`
Ë˜
â€œ Â¨ Â¨ Â¨ Ï•j Xn pu, Î¸nÂ´1 , en , Î¸0n , e0n q, Î±n pu, Î¸nÂ´1 , en , Î¸0n , e0n , Î¸n q
Ë‡
Ë‡
n
P
pduqP
pdÎ¸
qÎ½
pde
qP
pdÎ¸
q
Ë‡ 0 0 0 0
U
Ï‘
Ï‘
n
nÂ´1
n
n
(39)
nÂ´1
Î¸ n â€œÏ‘n ,en â€œÎµn
Å¼
Å¼ Â´Å¼
Â¯
`
Ë˜
â€œ Â¨Â¨Â¨
Ï•j Xn pu, Î¸nÂ´1 , en , Î¸0n , e0n q, Î± Ï€n pdÎ± |u, Î¸nÂ´1 , en , Î¸0n , e0n q
A
Ë‡
Ë‡
PU pduqPÏ‘nÂ´1 pdÎ¸nÂ´1 qÎ½ n pden qË‡ 0 0
0
0
Î¸ n â€œÏ‘n ,en â€œÎµn

where we made explicit the dependence of Xn on Î¾n â€œ pU, Ï‘nÂ´1 , Îµn , Ï‘0n , Îµ0n q and Î±n on pÎ¾n , Ï‘n q. This shows that
the left hand side of (39), and hence the left hand side of (38) only depend upon the action process Î± â€œ pÎ±n qnÄ›0
through the conditional distribution Ï€n pdÎ±|u, Î¸nÂ´1 , en , Î¸0n , e0n q. From this we conclude that if two action processes
are realizations of control processes generated by the same the open-loop policy Ï€, the corresponding random
measures P0pXn ,Î±n q have the same distribution.


MODEL FREE MEAN FIELD RL

41

Remark 41. For every pÂµ, aÌ„, e0 q P Î“Ì„ Ë† E 0 , FÌ„ pÂµ, aÌ„, e0 q is defined as a probability measure on S such that for every
bounded and Borel measurable function Ï† : S Ã‘ R,
Å¼
Å¼
Â´
Â¯
(40)
FÌ„ pÂµ, aÌ„, e0 qpdx1 qÏ†px1 q â€œ
aÌ„pdx, dÎ±qÎ½pdeqÏ† F px, Î±, aÌ„, e, e0 q .
S

SË†AË†E

It is straightforward to check that FÌ„ is Borel measurable. See for example [10, Proposition 7.29] for a proof.
Lemma 42. Assume (H1). It holds:
â€š FÌ„ is Borel measurable and for every e0 P E 0 , FÌ„ pÂ¨, Â¨, e0 q is continuous in its remaining variables.
â€š fÂ¯ is bounded and lower semi-continuous.
Proof. The measurability
ofË˜ FÌ„ was argued earlier (see after Definition 12), so we only argue the continuity for
`
e0 P E 0 fixed. Let pÂµn , aÌ„n q nÄ›0 be a sequence in Î“Ì„ which converges weakly toward pÂµ, aÌ„q. Since Âµn â€œ pr1 paÌ„n q for
each n Ä› 0, we necessarily have Âµ â€œ pr1 paÌ„q so that pÂµ, aÌ„q P Î“Ì„. We pick a continuous bounded function Ï† : S ÃÃ‘ R
and we show that:
Å¼
Å¼
(41)
lim
Ï†px1 qFÌ„ pÂµn , aÌ„n , e0 qpdx1 q â€œ
Ï†px1 qFÌ„ pÂµ, aÌ„, e0 qpdx1 q.
nÃ‘8 S

S

Using Skorohodâ€™s characterization of weak convergence of probability measures, we have the existence of random
variables pYn , Î²n q converging P-almost surely toward some pY, Î²q and such that PpYn ,Î²n q â€œ aÌ„n for each n Ä› 0 and
PpY,Î²q â€œ aÌ„. Consequently, the integral in the left hand side of (41) can be rewritten as:
Å¼
Å¼
`
Ë˜
1
0
1
Ï†px qFÌ„ pÂµn , aÌ„n , e qpdx q â€œ
Ï† F px, Î±, aÌ„n , e, e0 q aÌ„n pdx, dÎ±qÎ½pdeq
S
Å¼SË†AË†E
Â´ â€œ `
Ë˜â€°Ë˜
â€œ
E Ï† F pXn , Î±n , aÌ„n , e, e0 q Î½pdeq,
E

Ë˜â€°Ë˜
ÅŸ Â´ â€œ `
which converges toward E E Ï† F pXn , Î±n , aÌ„n , e, e0 q Î½pdeq by Lebesgueâ€™s dominated convergence theorem because F pÂ¨, Â¨, Â¨, e, e0 q is continuous and Ï† is bounded continuous.
The lower semi-continuity of the one stage cost function fÂ¯ follows from [10, Proposition 7.31 (a)] which only
requires the lower semi-continuity of the original one-stage cost function f instead of the full continuity assumption
posited in Assumption (H1).

1
1
Lemma 43. Let Ï€Ì„ P Î Ì„. For
â€œÅ™ everyn Âµ P SÌ„, let
â€°two pairs of state and action processes
â€° pÂµ, â€œaÌ„q
Å™ and npÂµÂ¯ , aÌ„1 q be
Â¯
generated by pÏ€Ì„, Âµq. Then E nÄ›0 Î³ f pÂµn , aÌ„n q â€œ E nÄ›0 Î³ f pÂµn , aÌ„1n q .

Proof. For any fixed initial Âµ P SÌ„, by definition of the pair`of state Ë˜and action
processes
generated by pÏ€Ì„, Âµq, we
Ë˜
`
show by induction that, for all n Ä› 0, LpÂµn q â€œ LpÂµ1n q and L pÂµn , aÌ„n q â€œ L pÂµ1n , aÌ„1n q . For n â€œ 0, Âµ0 â€œ Âµ10 â€œ Âµ P SÌ„.
Assume that for some n Ä› 0, we have LpÂµn q â€œ LpÂµ1n q, then for every bounded and Borel measurable function
Ï† : SÌ„ Ë† AÌ„ Ã‘ R
E rÏ†pÂµn , aÌ„n qs â€œ E rE rÏ†pÂµn , aÌ„n q | Âµn ss
â€Å¼
ïš¾
â€œE
Ï†pÂµn , aÌ„q.LpaÌ„n | Âµn qpdaÌ„q
â€Å¼AÌ„
ïš¾
â€œE
Ï†pÂµn , aÌ„q.Ï€Ì„n pÂµn qpdaÌ„q
â€Å¼AÌ„
ïš¾
Ë˜â€°
â€œ â€œ
â€°â€°
â€œ `
1
1
â€œE
Ï†pÂµn , aÌ„q.Ï€Ì„n pÂµn qpdaÌ„q â€œ E E Ï†pÂµ1n , aÌ„1n q | Âµ1n â€œ E Ï† Âµ1n , aÌ„1n .

(42)

AÌ„

`

Ë˜

`

Ë˜

`

Ë˜

`

Ë˜

So L pÂµn , aÌ„n q â€œ L pÂµ1n , aÌ„1n q . Since Îµ0n`1 is independent of pÂµn , aÌ„n q and pÂµ1n , aÌ„1n q, L pÂµn , aÌ„n , Îµ0n`1 q â€œ L pÂµ1n , aÌ„1n , Îµ0n`1 q ,
which implies that the law of Âµn`1 â€œ FÌ„ pÂµn , aÌ„n , Îµ0n`1 q is equal to the law of FÌ„ pÂµ1n , aÌ„1n , Îµ0n`1 q â€œ Âµ1n`1 . Hence the
conclusion.



42

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

Appendix B. Proofs for Section 4.1
Proof of Lemma 23. Let us denote by Î¶n`1 the right hand side of (18), and let Ï† : S Ã‘ R, hn : pÎ˜0 Ë† E 0 qn Ã‘ R
and Ïˆn`1 : E 0 Ã‘ R be arbitrary bounded Borel measurable functions. We have:
â€
ïš¾
Å¼
0
0
0
1
1
E Ïˆn`1 pÎµn`1 qhn pÏ‘n , Îµn q Ï†px qÎ¶n`1 pdx q
S
â€
ïš¾
Å¼
Â´
Â¯
0
0
0
1
0
0
0
1
â€œ E Ïˆn`1 pÎµn`1 qhn pÏ‘n , Îµn q Ï†px q FÌ„ pPXn , PpXn ,Î±n q , Îµn`1 q pdx q
S
â€
Å¼
Â´
Â¯ïš¾
0
0
0
0
0
0
PpXn ,Î±n q pdx, dÎ±qÎ½pdeqÏ† F px, Î±, PpXn ,Î±n q , e, Îµn`1 q
â€œ E Ïˆn`1 pÎµn`1 qhn pÏ‘n , Îµn q
SË†AË†E
â€
Å¼
Å¼
Â´
Â¯ïš¾
0
0
0
0
0
0
0
0
PpXn ,Î±n q pdx, dÎ±qÏ† F px, Î±, PpXn ,Î±n q , e, e q
â€œ
Î½pdeqÎ½ pde qÏˆn`1 pe qE hn pÏ‘n , Îµn q
SË†A
EË†E 0
â€
Â´
Ä±Ä±
â€
Ë˜Â¯ Ë‡Ë‡
â€œ E Ïˆn`1 pÎµ0n`1 qE hn pÏ‘0n , Îµ0n qÏ† F pXn , Î±n , P0pXn ,Î±n q , Îµn`1 , Îµ0n`1 Ë‡ Fn0 , Îµn`1 , Îµ0n`1
â€œ
â€°
â€œ E Ïˆn`1 pÎµ0n`1 qhn pÏ‘0n , Îµ0n qÏ†pXn`1 q ,
where the first equality is by definition of Î¶n`1 , the second equality is by the definition of FÌ„ in terms of the system
function F of the original MFC, the third equality is by the fact that Îµ0n`1 is independent of all the other random
quantities, the fourth equality is by definition of the conditional probability P0pXn ,Î±n q , and the last equality is by
Ë˜
the tower property of conditional expectation, the fact that Xn`1 â€œ F pXn , Î±n , P0pXn ,Î±n q , Îµn`1 , Îµ0n`1 and the fact
that pÎµn`1 , Îµ0n`1 q is independent of pXn , Î±n q and P0pXn ,Î±n q is measurable with respect to Fn0 â€œ ÏƒtÏ‘0n , Îµ0n u. This
shows that Î¶n`1 â€œ P0Xn`1 .

Proof of Lemma 24. We prove equation (19) by induction. Equality (19) holds for n â€œ 0 by the assumptions on
pÎ¶, Î·Ì„q. For the sake of an argument by induction, let us assume that (19) holds for some n Ä› 0. We first show that
LpÎ¶n`1 q â€œ LpP0Xn`1 q. Since Îµ0n`1 is independent of Fn0 , for every bounded Borel measurable function Ïˆ : SÌ„ Ã‘ R,
it holds:
Â¯Ë‡
â€ â€ Â´
Ä±Ä±
Ë‡
ErÏˆpÎ¶n`1 qs â€œ E E Ïˆ FÌ„ pÎ¶n , Î·Ì„n , Îµ0n`1 q Ë‡ Î¶n , Î·Ì„n
â€Å¼
ïš¾
â€œE
ÏˆpÂµqP pÎ¶n , Î·Ì„n qpdÂµq
â€Å¼SÌ„
ïš¾
Â´
Â¯
0
0
â€œE
ÏˆpÂµqP PXn , PpXn ,Î±n q pdÂµq
â€ SÌ„`
Ë˜Ä±
â€œ E Ïˆ P0Xn`1 ,
where the second equality is by definition of P in (9), the third equality is by the induction hypothesis, and the
last equality is due to Lemma 23. So LpÎ¶n`1 q â€œ LpP0Xn`1 q. We then consider the joint law of pÎ¶n`1 , Î·Ì„n`1 q. By the
`
Ë˜
assumption that LpÎ·Ì„n |Î¶n q â€œ Îºn pÎ¶n q, we have that pÎ¶n`1 , Î·Ì„n`1 q and P0Xn`1 , P0pXn`1 ,Î±n`1 q share the same regular
version Îºn of the conditional probability. We conclude that (19) holds for n ` 1 instead of n.


MODEL FREE MEAN FIELD RL

43

Appendix C. Proofs for Section 4.2
Proof of Lemma 28. We prove this statement by showing that there exist Ï€ P Î OL and an open-loop action
process Î± generated by Ï€ such that: J Ï€ â€œ J Î± â€œ J Ï€Ìƒ . Let Âµ0 P PpSq and let pX, Î±q be a pair of state and action
processes generated by pÏ€Ìƒ, Âµ0 q. Let a be the PpAq-valued process given by:
an â€œ Ï€Ìƒn pXn , P0Xn , Ï‘0n q,

n Ä› 0.

We recall that În , Î¾n and U are defined in Â§ 2.2.3. Since a is adapted to Gc , it is an admissible level-0 control
process, and for every n Ä› 0, there exists a Borel measurable function Ï€n : În Ã‘ A satisfying
an â€œ Ï€n pU , pÏ‘k , Ï‘0k , Îµk`1 , Ï‘0k`1 qkâ€œ0,...,nÂ´1 , Ï‘0n q â€œ Ï€n pÎ¾n q,

P Â´ a.s. .

a

Moreover, since Î± is generated by Ï€Ìƒ, it is adapted to G and satisfies
LpÎ±n | Gnc q â€œ Ï€Ìƒn pXn , P0Xn , Ï‘0n q â€œ Ï€n pÎ¾n q,

P Â´ a.s.

n Ä› 0.

So Î± can be viewed as an open-loop action process generated by Ï€. Meanwhile, the state process X constructed
by equation (1) is also a state process associated with pÎ±, Âµ0 q (see Definition 5). Therefore, by definition of the
value function associated to an open-loop policy Ï€, we have:
Â«
ff
Ã¿
`
Ë˜
n
0
Ï€Ìƒ
Î±
Î³ f Xn , Î±n , PpXn ,Î±n q .
J pÂµ0 q â€œ J pÂµ0 q â€œ E
nÄ›0


Appendix D. Disintegration of kernels
Given a probability measure P on a measurable space pC, Cq and a kernel K from pC, Cq to pD, Dq, the
composition of measure P and kernel K, denoted by P bÌ‚ K, is defined as a measure on the product space
pC Ë† D, C b Dq such that for every non-negative measurable function f : C Ë† D Ã‘ R` ,
Å¼
Å¼
pP bÌ‚ Kqf â€œ
P pdxq
f px, yqKpx, dyq.
C

D

Similarly, for a probability kernel Âµ : G Ã‘ PpCq and a probability kernel K : G Ë† C Ã‘ PpDq, the composition
of kernels Âµ and K, denoted by Âµ bÌ‚ K, is defined as a kernel from pG, Gq to pC Ë† D, C b Dq such that for every
s P G and for every non-negative measurable function f : C Ë† D Ã‘ R` ,
Å¼
Å¼
pÂµ bÌ‚ Kqpsqf â€œ
Âµps, dxq
f px, yqKps, x, dyq.
C

D

Appendix E. Details on Q-learning, Section 5
E.1. Proof of Theorem 36.
Proof of Theorem 36. Recall that we denote by JË‡Ëš and QÌŒËš respectively the state value function and the stateaction value function of the projected MFC problem defined by (32)â€“(33).
We first note that, for every pÂµ, aÌƒq P S Ë† A,
Ë‡
`
Ë˜
`
Ë˜Ë‡Ë‡ Ë‡
`
Ë˜
`
Ë˜Ë‡
Ë‡
Ë‡QÌŒNepi ProjSÌŒ pÂµq, aÌƒ Â´ QÌƒËš Âµ, aÌƒ Ë‡ Ä Ë‡QÌŒNepi ProjSÌŒ pÂµq, aÌƒ Â´ QÌŒËš ProjSÌŒ pÂµq, aÌƒ Ë‡
Ë‡ `
Ë˜
`
Ë˜Ë‡Ë‡
Ë‡
` Ë‡QÌŒËš ProjSÌŒ pÂµq, aÌƒ Â´ QÌƒËš ProjSÌŒ pÂµq, aÌƒ Ë‡
Ë‡ `
Ë˜
`
Ë˜Ë‡Ë‡
Ë‡
` Ë‡QÌƒËš ProjSÌŒ pÂµq, aÌƒ Â´ QÌƒËš Âµ, aÌƒ Ë‡ .
We then split the proof into three steps, which consist in bounding from above each term in the right hand side.
Step 1. We first analyze the difference between QÌŒNepi and QÌŒËš . This comes from standard convergence results
on Q-learning for finite state-action spaces. More precisely, under Assumptions (H3) and (H5), with our choice

44

RENEÌ CARMONA, MATHIEU LAURIEÌ€RE & ZONGJUN TAN

of learning rates, and given that Nepi is of order (34), we can apply Theorem 4 and Corollary 34 in [19] for
asynchronous Q-learning and polynomial learning rates, and we obtain that, with probability at least 1 Â´ Î´,
Ë‡
Ë‡
Ë‡
Ë‡
}QÌŒNepi Â´ QÌŒËš }8 â€œ
sup Ë‡QÌŒNepi pÂµÌŒ, aÌƒq Â´ QÌŒËš pÂµÌŒ, aÌƒqË‡ Ä Îµ.
pÂµÌŒ,aÌƒqPSÌŒË†A

Step 2. We then turn our attention to the difference between QÌŒËš and QÌƒËš . The analysis amounts to say that
the projection on SÌŒ realized at each step does not perturb too much the value function. Recall that for some
0
0
given common noise Îµ0 , the operator Î¦ÌŒÎµ : SÌŒ Ë† A Ã‘ SÌŒ is given by Î¦ÌŒÎµ pÂµÌŒ, aÌƒq â€œ ProjSÌŒ Ë FÌ„ pÂµÌŒ, ÂµÌŒ bÌ‚ aÌƒ, Îµ0 q. Likewise,
0
we denote the transition dynamic with FÌ„ by a function Î¦Îµ : S Ë† A Ã‘ S such that:
0

Î¦Îµ pÂµ, aÌƒq â€œ FÌ„ pÂµ, Âµ bÌ‚ aÌƒ, Îµ0 q,

@pÂµ, aÌƒq P S Ë† A.

Let us start by noting that, for every pÂµÌŒ, aÌƒq P SÌŒ Ë† A,
Ë‡
Ë‡
Ë‡
Ë‡ Ëš
Ë‡QÌŒ pÂµÌŒ, aÌƒq Â´ QÌƒËš pÂµÌŒ, aÌƒqË‡
ff
Â«
Ë‡
Ë‡
0
Ë‡ Ë‡Ëš Îµ0
Ë‡
Ä Î³E Ë‡J pÎ¦ÌŒ pÂµÌŒ, aÌƒqq Â´ JÂ¯Ëš pÎ¦Îµ pÂµÌŒ, aÌƒqqË‡
Â«
ff
Ë‡
Ë‡ Ë‡
Ë‡
0
0
0
Ë‡ Ë‡Ëš Îµ0
Ë‡
Ë‡
Ë‡
Ëš
Îµ
Ëš
Îµ
Ëš
Îµ
Ä Î³E Ë‡J pÎ¦ÌŒ pÂµÌŒ, aÌƒqq Â´ JÂ¯ pÎ¦ÌŒ pÂµÌŒ, aÌƒqqË‡ ` Ë‡JÂ¯ pÎ¦ÌŒ pÂµÌŒ, aÌƒqq Â´ JÂ¯ pÎ¦ pÂµÌŒ, aÌƒqqË‡
Â«
ff
â€â€º
Ë‡
â€º
Â´ 0
Â¯
Â´ 0
Â¯Ë‡
Ë‡
Ë‡
â€º
â€º Îµ0
Ëš
Îµ
1
Ëš
Îµ
1
Îµ0
QÌŒ
Î¦ÌŒ
Ä Î³E Ë‡ inf
pÂµÌŒ,
aÌƒq,
aÌƒ
Â´
inf
QÌƒ
Î¦ÌŒ
pÂµÌŒ,
aÌƒq,
aÌƒ
`
Î³L
Ë‡
Â¯Ëš E â€ºÎ¦ÌŒ pÂµÌŒ, aÌƒq Â´ Î¦ pÂµÌŒ, aÌƒqâ€º
J
1
1
aÌƒ PA

aÌƒ PA

ïš¾
,

dS

where the last inequality holds by Lipschitz continuity of JÂ¯Ëš on S, see Assumption (H4).
The second term in the last inequality can be bounded using the simplex discretization properties and Assumption (H3):
â€
Ä±
â€œ
â€°
0
0
E }Î¦ÌŒÎµ pÂµÌŒ, aÌƒq Â´ Î¦Îµ pÂµÌŒ, aÌƒq}dS â€œ EÎµ01 }ProjSÌŒ Ë FÌ„ pÂµÌŒ, aÌƒ, Îµ0 q Â´ FÌ„ pÂµÌŒ, aÌƒ, Îµ0 q}dS Ä ÎµS .
0

For the first term, let ÂµÌŒ1 â€œ Î¦ÌŒÎµ pÂµÌŒ, aÌƒq P SÌŒ to alleviate the notation, and let us consider aÌƒËš1 P A and aÌƒËš2 P A
satisfying:
`
Ë˜
`
Ë˜
QÌŒËš pÂµÌŒ1 , aÌƒËš1 q â€œ inf
QÌŒËš ÂµÌŒ1 , aÌƒ1
and
QÌƒËš pÂµÌŒ1 , aÌƒËš2 q â€œ inf
QÌƒËš ÂµÌŒ1 , aÌƒ1 .
1
1
aÌƒ PA

aÌƒ PA

The existence of aÌƒËš1 and aÌƒËš2 is guaranteed respectively by finiteness of S Ë† A.
We observe that
QÌŒËš pÂµÌŒ1 , aÌƒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q
Â´
Â¯ Â´
Â¯
â€œ QÌŒËš pÂµÌŒ1 , aÌƒËš1 q Â´ QÌŒËš pÂµÌŒ1 , aÌƒËš2 q ` QÌŒËš pÂµÌŒ1 , aÌƒËš2 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q
Ë‡
Ë‡
Ë‡
Ë‡
Ä0`
sup Ë‡pQÌŒËš Â´ QÌƒËš qpÂµÌŒ, aÌƒqË‡
pÂµÌŒ,aÌƒqPSÌŒË†A
Ëš

Ä }QÌŒ Â´ QÌƒËš }8 .
On the other hand,
Â´
Â¯ Â´
Â¯
QÌŒËš pÂµÌŒ1 , aÌƒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q â€œ Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš2 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš1 q Â´ QÌƒËš pÂµÌŒ1 , aÌƒËš1 q Â´ QÌŒËš pÂµÌŒ1 , aÌƒËš1 q Ä› Â´}QÌŒËš Â´ QÌƒËš }8 .
Combining the above bounds yields that for every pÂµÌŒ, aÌƒq P SÌŒ Ë† A,
Ë‡
Ë‡
Â´
Â¯
Ë‡ Ëš
Ë‡
Ë‡QÌŒ pÂµÌŒ, aÌƒq Â´ QÌƒËš pÂµÌŒ, aÌƒqË‡ Ä Î³ }QÌŒËš Â´ QÌƒËš }8 ` Î³LJÂ¯Ëš ÎµS .

MODEL FREE MEAN FIELD RL

45

Consequently,

Î³
L Â¯Ëš ÎµS .
1Â´Î³ J
Step 3. Last, we look at the difference between QÌƒËš pProjSÌŒ pÂµq, aÌƒq and QÌƒËš pÂµ, aÌƒq. For every Âµ P S and aÌƒ P A,
letting ÂµÌŒ â€œ ProjSÌŒ pÂµq to alleviate the notation, we have }ÂµÌŒ Â´ Âµ}dS Ä ÎµS . We obtain
Ë‡
Ë‡
Ë‡ Ëš
Ë‡
Ë‡QÌƒ pÂµÌŒ, aÌƒq Â´ QÌƒËš pÂµ, aÌƒqË‡
Ë‡
Ë‡
Â«Ë‡
Ë‡ ff
Ë‡
Ë‡
Ë‡
Ë‡
Ë‡
Ë‡Ëœ
Ëš
1
Ëš
1
Ë‡
QÌƒ
pÎ¦pÂµÌŒ,
aÌƒq,
aÌƒ
q
Â´
inf
QÌƒ
pÎ¦pÂµ,
aÌƒq,
aÌƒ
q
Ä Ë‡f pÂµÌŒ, aÌƒq Â´ fËœpÂµ, aÌƒqË‡ ` Î³E Ë‡Ë‡ inf
Ë‡
aÌƒ1 PA
aÌƒ1 PA
Ë‡
Ë‡
Ë‡â€°
â€œË‡
Ä LfËœ}ÂµÌŒ Â´ Âµ}dS ` Î³E Ë‡JÂ¯Ëš pFÌ„ pÂµÌŒ, aÌƒ, Îµ0 q Â´ JÂ¯Ëš pFÌ„ pÂµÌŒ, aÌƒ, Îµ0 qqË‡
â€œ
â€°
Ä LfËœÎµS ` Î³LJÂ¯Ëš E }FÌ„ pÂµÌŒ, aÌƒ, Îµ0 q Â´ FÌ„ pÂµ, aÌƒ, Îµ0 q}dS
}QÌŒËš Â´ QÌƒËš }8 Ä

Ä pLfËœ ` Î³LJÂ¯Ëš LFÌ„ qÎµS ,
where we used the Lipschitz continuity of fËœ, JÂ¯Ëš , FÌ„ and the assumption on SÌŒ, see Assumptions (H3), (H4) and
the simplex discretization properties.

E.2. DDPG algorithm. In Algorithm 2, we describe the DDPG method for MFC with our notation.

Algorithm 2: DDPG for MFC
Data: A number of episodes Nepi ; a length T for each episode; a minibatch size Nbatch ; a learning rate Ï„ .
Result: A strategy function for central planner represented by the target network Ï€Ï‰1 1 .
1 begin
2
Randomly initialize parameters Î¸ and Ï‰ for critic network QÎ¸ and actor network Ï€Ï‰
3
Initialize Î¸1 Ã Î¸ and Ï‰ 1 Ã Ï‰ for target networks Q1Î¸1 and Ï€Ï‰1 1
4
for k â€œ 0, 1, . . . Nepi Â´ 1 do
5
Initial distribution ÂµÌŒ0
6
Initialize replay buffer Rbuffer
7
for n â€œ 0, 1, . . . T Â´ 1 do
8
Select an action aÌ„n â€œ Ï€Ï‰ pÂµÌŒn q ` an`1 P RNp , where an`1 is the exploration noise
9
Execute aÌ„n , observe cost cn â€œ fËœn pÂµÌŒn , aÌ„n q and ÂµÌŒn`1
10
Store transition pÂµÌŒn , aÌ„n , cn , ÂµÌŒn`1 q in Rbuffer
11
Sample a random minibatch of Nbatch transitions pÂµÌŒi , aÌ„i , ci , ÂµÌŒi`1 q from Rbuffer
12
Set yi â€œ ci ` Î³Q1Î¸1 pÂµÌŒi`1 , Ï€Ï‰1 1 pÂµÌŒi`1 qq, for i â€œ 1 . . . , Nbatch
Å™
1
13
Update the critic by minimizing the loss: LpÎ¸q â€œ Nbatch
pyi Â´ QÎ¸ pÂµÌŒi , aÌ„i qq2
i

14

Update the actor policy using the sampled policy gradient âˆ‡Ï‰ J:
1 Ã¿
âˆ‡Ï‰ JpÏ‰q Â«
âˆ‡aÌ„ QÎ¸ pÂµÌŒi , Ï€Ï‰ pÂµÌŒi qqâˆ‡Ï‰ Ï€Ï‰ pÂµÌŒi q
Nbatch i
Update target networks: Î¸1 Ã Ï„ Î¸ ` p1 Â´ Ï„ qÎ¸1 and Ï‰ 1 Ã Ï„ Ï‰ ` p1 Â´ Ï„ qÏ‰ 1

15

return Ï€Ï‰1 1

Program in Applied and Computational Mathematics & ORFE

