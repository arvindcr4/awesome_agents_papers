Here is the extracted information:

**PAPER:** Curiosity-driven Exploration by Self-supervised Prediction
**TITLE:** Curiosity-driven Exploration by Self-supervised Prediction
**RESEARCH_METHOD:** 06_curiosity_exploration
**METHOD_DESCRIPTION:** This paper proposes a curiosity-driven exploration method that uses self-supervised prediction to learn an intrinsic reward signal. The method uses a neural network to predict the next state of the environment given the current state and action, and the error in this prediction is used as the intrinsic reward signal. This approach allows the agent to explore the environment without relying on external rewards.

**KEY_CONTRIBUTIONS:**

* Proposes a curiosity-driven exploration method that uses self-supervised prediction to learn an intrinsic reward signal.
* Demonstrates the effectiveness of the method in two environments: VizDoom and Super Mario Bros.
* Shows that the method can learn useful exploration policies without external rewards.
* Evaluates the generalization of the learned policies to new scenarios.
* Compares the method to other exploration methods, including variational information maximization (VIME) and count-based exploration.
