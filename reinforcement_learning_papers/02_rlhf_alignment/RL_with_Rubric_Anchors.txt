Reinforcement Learning with Rubric Anchors
Zenan Huang∗ , Yihong Zhuang∗ , Guoshan Lu∗ , Zeyu Qin∗ , Haokai Xu∗ , Tianyu Zhao, Ru Peng, Jiaqi Hu,
Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan,
Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, Junbo Zhao†

arXiv:2508.12790v1 [cs.AI] 18 Aug 2025

Inclusion AI, Ant Group, Zhejiang University

Abstract
Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm
for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI’s
o-series. In RLVR, rewards are derived from deterministic, programmatically verifiable signals—such as passing unit tests in code generation or matching the correct numerical answer
in mathematical reasoning. While effective, this requirement for unambiguous correctness
largely confines RLVR to domains with clear, automatically checkable outcomes.
To overcome this limitation, we extend the RLVR paradigm beyond strictly verifiable domains by integrating open-ended tasks into the framework through rubric-based reward. In
this approach, carefully designed rubrics serve as structured, model-interpretable criteria,
enabling the automatic scoring of tasks with inherently subjective or multidimensional outputs. We construct, to our knowledge, the largest rubric reward system to date, comprising
over 10,000 rubrics generated by humans, by various LLMs, or via a hybrid human–LLM
collaboration.
Implementing rubric-based RL is challenging, requiring careful rubric construction, data
curation, and training strategy design. We tackle these issues with a clear rubric-driven RL
framework, and present an open-sourced Qwen-30B-A3B model trained with this approach,
achieving notable gains:
• With only 5K+ training samples, our training system enables a +5.2% absolute improvement on various open-ended benchmarks (especially humanities-centric tasks),
outperforming a 671B DeepSeek-V3 model by +2.4% points, while preserving performance on general and reasoning ability benchmarks.
• Our method provides fine-grained stylistic control. By using rubrics as explicit anchors,
it effectively mitigates the common “AI-like” and didactic tone, producing responses
with demonstrably greater human-likeness and emotional expressiveness.
We dissect our experiences and share key lessons in rubric construction, data selection, and
training strategies. We also candidly discuss certain aspects of this research that are yet to be
concluded, with further releases planned for the future.

* Core and Equal contributor.
† Correspondence to j.zhao@zju.edu.cn or zhaojunbo.zjb@antgroup.com

1

1

Introduction

The proposal of OpenAI o1 (OpenAI, 2024) has marked a new era in the development of Large Language
Models (LLMs), with Reinforcement Learning from Verifiable Rewards (RLVR) (Lambert et al., 2024; Guo
et al., 2025) emerging as a key trend. This approach has driven a wave of LLM innovation by enabling
test-time scaling. At its core, RLVR is founded on the principle of leveraging data that, while difficult for a
model to solve, can be easily and objectively verified 1 (Bai et al., 2022; Saunders et al., 2022; McAleese et al.,
2024). Prime examples include data from mathematics and competitive programming, where solutions can
be validated automatically—a mathematical answer by matching it to the correct solution (Luo et al., 2025b;
Hugging Face, 2025), and a code solution by executing it against a suite of test cases in an online sandbox
environment (Luo et al., 2025a). Both proprietary (OpenAI, 2025; DeepMind, 2025; Team et al., 2025b) and
open-source efforts (Yang et al., 2025; Xie et al., 2025; Jin et al., 2025; Fu et al., 2025) exemplify this paradigm,
enabling scalable test-time reasoning and expanding the capability frontier in mathematics, competitive
programming, web search, and other verifier-rich domains.
While the RLVR paradigm has achieved considerable success, it is inherently constrained by its reliance
on question–answer pairs with objectively verifiable solutions. This structural dependency imposes a
hard ceiling on scalability: the supply of such data, though substantial in domains like mathematics and
programming, is ultimately finite. As a result, RLVR’s applicability remains restricted to a narrow subset of
tasks. We address this limitation by extending RLVR to incorporate open-ended tasks and other forms of
non-verifiable data, thereby broadening its applicability to a much wider range of real-world scenarios. This
shift, however, introduces a fundamental challenge: how to construct reward signals that are both reliable and
scalable in the absence of explicit ground truth.
Rubric-based reward offers a promising path forward: by defining structured, interpretable criteria for
assessment, it can capture multi-dimensional aspects of response quality beyond binary correctness (Bai
et al., 2022; Sun et al., 2023; Mu et al., 2024; Wang et al., 2025). While several concurrent works (Guan et al.,
2024; Gunjal et al., 2025; Viswanathan et al., 2025; Li et al., 2025) have begun to explore this idea, our work
systematically identifies the key components required for rubric-based rewards to be effective in RL training.
Not so surprisingly, relying on a single rubric risks reward exploitation, whereas indiscriminately scaling
the number of rubrics, whether generated by humans or LLMs, yields only marginal gains. To assess the
full potential of our rubric-based training framework, we construct the largest rubric reward bank to date,
containing over 10,000 rubrics. Throughout this process, we perform extensive empirical testing and found
that success is not trivial. The success/failure hinges tightly on the diversity, granularity, and quantity of the
rubrics themselves, as well as on a proper training routine and meticulous data curation. Our training routine
adopts a two-stage RL process to progressively enhance model capabilities. The first stage builds a strong
constraint-handling foundation through reliable instruction-following and high-quality critic development,
using verifiable checks and static, multi-dimensional rubrics. The second stage targets more open-ended,
socially grounded, and creative tasks, evaluated via high-quality references and instance-specific rubrics
generated by stronger agentic workflows, fostering adaptability and richer expression.
We discover there’s no silver bullet for rubric construction. We perform careful ablation studies for every
set of rubrics before integrating them into the training pipeline. The resulting rubrics span multiple scopes:
some are grounded in a specific dataset, others are defined at the task level, and some are associated with
each data point, similar to the approach used in the Healthbench (Arora et al., 2025) evaluation. These rubrics
are generated by human experts, by LLMs (we used either a self-critique model (Qwen3-30B-A3B (Yang et al.,
2025)) or a powerful Gemini 2.5 Pro API (DeepMind, 2025)), or through an iterative combination of both.
In this work, we designate our approach as Rubicon, taking its name from RUBrIC aNchOrs. This yields an
RL-trained model, Rubicon-preview 2 , which demonstrates several significant merits.
1. Performance with high token efficiency: On subjective, humanities-centric tasks, the 30B-A3B Rubiconpreview model achieves a +5.2% absolute improvement, outperforming a 671B DeepSeek-V3 model
by +2.4% percentage points, using only 5K data samples.
2. Style controllability: Rubric-based RL can serve as a controllable anchor for guiding LLM output style,
yielding more human-like, emotionally expressive, and less formulaic responses.
3. General ability maintenance: Although our rubrics are not tailored for STEM-oriented tasks such
1 https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law
2 https://huggingface.co/inclusionAI/Rubicon-Preview

2

as mathematics or coding, Rubicon-preview effectively avoids negative interference with general
abilities. As a result, the model preserves its overall competence while also delivering extra gains on
reasoning benchmarks, including AIME 2024 (+4.1%) and AIME 2025 (+0.8%).
A final note-1. We candidly acknowledge that this work represents a preliminary step, with many aspects of
rubric-based RL yet to be explored thoroughly. Open questions remain, such as how rubric granularity and
scale influence performance and the precise mechanisms behind reward hacking. We intend to continue this
research and hope to provide ongoing updates to this technical report and the open-sourced model.
A final note-2. Our results highlight a prominent token efficiency: By using only 5K samples in conjunction
with a large number of rubrics, our method renders significant gains. This observation poses a new question
regarding scaling laws: Could this combination of a limited number of tokens and a large set of rubrics
represent a new form of post-training scaling law for LLMs?
Data Collection
Rubric Design

Rubric Instantiation
✔

RL Rollout

Updating

Rubric Updating

Rubric Design
Data

Mono-Task
Rubric-based RL

Tagging & Selection
Scorer function

Scorer function

Score distribution
Offline
Filter Data

Seed Data
Instruction
Rewrite Data

RL Data

Final Data
Instruction Data

Figure 1: An overview of our rubric system. The Data Collection phase (left, orange) begins with an
initial Rubric Design to create a set of tagging & scoring workflow, which filters a large corpus into highquality Offline Filter Data. This data then seeds the Rubric Updating phase (right, green), where an RL
with rubrics loop not only validates RL Data but also provides feedback to iteratively update the rubric
itself. This iterative process ensures that the Final Data is tightly aligned with a continuously improving,
model-verifiable evaluation standard.

2

Rubric System

2.1 Rubrics Design & Tasks Curation
Our rubric design and task curation follow the principle of evaluative asymmetry—verifying a candidate
output should be substantially easier than generating it (Saunders et al., 2022; McAleese et al., 2024). To
operationalize this, we adopt a rubric-first workflow: we first construct model-verifiable rubrics, then
curate or synthesize data that match these rubrics, and finally re-use them for supervision, reward shaping,
and evaluation. This strategy ensures consistency of criteria across data acquisition, model training, and
assessment.
In this framework, we formalize our scorer function by defining its underlying rubric R as a set of K distinct
critic dimensions:
R = {r1 , r2 , . . . , r K }.
(1)
Each dimension rk is specified by three components: (1) a criterion description ck defining the evaluative
aspect; (2) an ordered set of mk score tiers {lk,1 , . . . , lk,mk }, each mapped to a quantitative score; and (3) an
associated weight wk indicating its relative importance. This formalization accommodates both high-level,
general-purpose rubrics (e.g., tasks involving open-ended creative generation) and fine-grained, programmatically verifiable ones (e.g., tasks requiring strict adherence to instruction constraints), unifying diverse
evaluation protocols under a single abstract representation. The adopted rubrics are provided in Section A.2
and A.3.
This structured, multi-dimensional definition of a rubric serves as the foundation for our reward framework. By formalizing evaluation criteria in this manner, we can translate them directly into a granular and
interpretable reward signal for policy optimization, as detailed below.

3

2.2 Rubric-Based Reward Framework
Multi-Dimensional Reward Signal. Given the rubric R, we define a reward function R(y| x, R) that maps a
response y to a multi-dimensional feedback vector:
R(y| x, R) = [r1 (y| x ), r2 (y| x ), . . . , rK (y| x )],

(2)

where each component rk (y| x ) ∈ R is the score for the k-th dimension. This vector provides a granular,
interpretable signal of model performance across all specified criteria.
Advanced Reward Aggregation. To derive a scalar reward for optimization, a simple weighted sum,
Rtotal = ∑kK=1 wk · rk (y| x ), serves as a natural baseline. However, effective rubric-based optimization often
requires more sophisticated aggregation to capture non-linear interdependencies between dimensions. Our
framework therefore, moves beyond linear combination by incorporating a suite of advanced strategies:
• Veto Mechanisms: Failure on a critical, non-negotiable dimension (e.g., a reward-hacking detection
rubric) can preemptively nullify rewards from all other dimensions, acting as a hard constraint.
• Saturation-Aware Aggregation: We use saturation functions to model the diminishing marginal
returns of excelling in a single dimension beyond a certain threshold, encouraging balanced, multifaceted improvements.
• Pairwise Interaction Modeling: The framework can explicitly model synergistic or antagonistic
effects between criteria, capturing complex relationships that a simple sum would ignore.
• Targeted Reward Shaping: We employ non-linear mapping functions to selectively amplify score
differentials in high-performance regions. This enhances the discriminative power of the reward
signal, where scores might otherwise be compressed, providing a more granular gradient for finegrained optimization.

3

Implementation of Rubicon Framework

Our training methodology is a multi-stage reinforcement learning (RL) protocol designed to progressively
cultivate a spectrum of capabilities, from precise instruction-following to sophisticated creative and social
reasoning. This sequential approach significantly reduces computational overhead while preserving scalability. All data employed in this framework is derived from a proprietary 900K+ instance corpus, curated from
diverse sources including community Q&A forums, high-quality examinations, and general conversational
datasets, with strategic sampling to ensure broad topical coverage.
3.1 Data Selection and RL Pipeline
Offline Data Filtering. A filtering protocol is applied prior to and between RL stages to ensure high-quality
training data. For each candidate pool of instruction–rubric pairs, the base model generates responses,
which are then scored by our critic models to obtain a full score distribution. We retain only those within a
calibrated central quantile—excluding overly high-scoring instances that offer limited learning signal, and
very low-scoring ones which may be noisy or low-quality. This yields a balanced, high-potential subset, with
the composition further adjusted between stages to target specific capabilities.
Stage-wise RL Training. During our experiments, we observe a “seesaw effect”: jointly training on different
task types (e.g., strict constraint-following vs. open-ended creativity) often reduced overall performance,
likely due to conflicting optimization objectives. To mitigate this issue, we adopt a simple stage-wise RL
schedule as a pragmatic mitigation strategy, without claiming it as a definitive solution.
In the first phase, we emphasize reliable instruction-following and multi-dimensional evaluation alignment,
using programmatically verifiable checks and static rubrics to build a strong constraint-handling foundation.
In the subsequent phase, we extend to more open-ended, socially grounded, and creative tasks, leveraging
reference-based rubrics and instance-specific criteria generated via stronger agentic workflows to promote
adaptability.
3.2 Adaptive Defense Against Reward Hacking
A significant challenge encountered during our experiments is the emergence of reward hacking, particularly in the initial RL stages focused on a small number of capabilities. We observe that the model could
rapidly learn to exploit specific rubric criteria, resulting in specious reward maximization without genuine
improvement. To address this, we implement an adaptive defense strategy.
4

The process begins with an offline analysis of rollout data from these initial training runs. By examining
instances where the reward signal is anomalously high, we systematically identify and categorize recurrent,
high-level patterns of reward-hacking behavior. This empirical analysis informs the development of a
dedicated Reward Hacking Defense Rubric (shown in Section A.1). This new rubric is not part of the initial
training but is synthesized from the observed failure modes and integrated as a supervisory constraint in all
subsequent, more complex RL stages.
The inclusion of this defense mechanism yields substantial improvements in training dynamics. It acts as a
critical guardrail, preventing the policy from collapsing into reward-hacking states. This is evidenced by a
marked increase in training stability; we are able to conduct longer and more productive training epochs, as
the defense rubric mitigated the catastrophic reward spikes that previously rendered continued optimization
ineffective. By actively penalizing the exploitation of scoring artifacts, this iterative refinement ensures that
the learning process remains focused on substantive capability enhancement.

4

Experimental Results

Our experimental results address three aspects:
• Quantitatively measuring the gains from rubric-based RL training on open-ended, human-centric
benchmarks, including assessments of the model’s emotional intelligence (EQ) and its ability to
produce human-like responses.
• Qualitatively analyzing how the model’s generated outputs evolve over time, illustrated through
representative output showcases.
• Evaluating the impact of rubric-based RL training on general-ability benchmarks.
The corresponding ablation studies are presented afterward.
4.1 Quantitative Evaluation
Benchmarks Unlike RLVR, the primary benefits of rubric-based RL are most evident on benchmarks that
lack verifiable rewards. To demonstrate this, we gather a diverse set of open-ended and humanity-centric
benchmarks covering Creative Writing V3 (Paech, 2024), Writingbench (Wu et al., 2025), Judgemark V2 (Paech,
2024), EQ-Bench3 (Paech, 2024), IFEval (Zhou et al., 2023), Collie (Yao et al., 2023), and IFScale (Jaroslawicz
et al., 2025).
Alongside them, we further cover a diverse set of benchmarks to check for potential regressions in other
capabilities, inclucing MMLU (Hendrycks et al., 2021a), HellaSwag (HS) (Zellers et al., 2019), StoryCloze
(SC) (Srinivasan et al., 2018), IQuiz-EQ (IQ-EQ) (Chen et al., 2024), SocialIQA (SIQA) (Sap et al., 2019), CoQA
(CQ) (Reddy et al., 2019), and a set of reasoning benchmarks like AIME24 (Math-AI, 2024), AIME25 (Math-AI,
2025), Math500 (Hendrycks et al., 2021b), GPQA-Diamond (GPQA-D) (Rein et al., 2023) and LiveCodeBench
v5 (LCB v5) (Jain et al., 2024).
Table 1: Main results showcasing the benefits of our rubric-based RL approach (Rubicon) compared to its
base model, Qwen3-30B-A3B. We also include a further comparison with the 671B DeepSeek-V3 model to
highlight the relative performance of our framework on these benchmarks.
Model

C.W

Writing

Judge

EQ

IFE

Collie

IFS

Avg

Qwen3-30B-A3B
Rubicon-preview
∆ Improvement

77.82
81.89
↑4.07

75.65
80.11
↑4.46

56.20
69.20
↑13.00

73.35
79.55
↑6.20

83.55
81.70
↓1.85

35.77
40.27
↑4.50

54.68
60.79
↑6.11

65.29
70.50
↑5.21

DeepSeek-V3-671B

80.10

74.08

61.30

75.6

81.89

42.69

60.92

68.08

Baselines and Main Results We select Qwen3-30B-A3B (Yang et al., 2025) as our base model. We refer
to our RL-trained model as Rubicon-preview. As shown in Table 1, Rubicon-preview achieves an average
improvement of 5.2% across these benchmarks. For further comparison, we also assess the performance of
DeepSeek-V3 (DeepSeek-AI et al., 2025), a model renowned for its strong capabilities in humanities, social
sciences, and open-ended queries. Our approach successfully surpassed DeepSeek-V3 by 2.4%.

5

Our quantitative results show that Rubicon-preview attains the lead by showing significant improvement
on writing and emotional intelligence benchmarks. For instruction-follow ability, while it displays a minor
performance drop on IFEval, Rubicon-preview still excels on the other two instruction-following benchmarks.
4.2 Case Studies on Controllable Output Style with Rubrics
Rubrics function as controllable anchors that direct LLMs toward a well-defined output style. We provide
several illustrative examples to demonstrate this effect. Below, we first present the adopted rubrics, followed
by a comparison between a baseline model and a model trained with rubric-based RL. The resulting style is
generally plain and informative, with substantially reduced “AI-like” or didactic tone, and instead exhibits
greater human-likeness and emotional expressiveness. Section B and C present additional output examples
from our model.
Style Evaluation Rubric: Plain Narrative
Objective:
To critically appraise the model’s success in adopting a specific narrative style: the Plain Narrative. This style is
characterized by language that is simple, restrained, and reflects a deep, quiet resilience.
Guiding Principle:
The evaluation prioritizes stylistic authenticity over literary polish or technical correctness. The core measure of
success is the response’s capacity to ”feel right” by avoiding any sense of artificiality (’AI-speak’, ’preachy-speak’).
Core Evaluative Criteria:
1. Relational Efficacy (Voice & Tone): This criterion gauges the authenticity of the narrative voice. Key indicators
include:
1.1 Calm Acceptance: A quiet acceptance of fate, life, and death.
1.2 Grounded Realism: Rooted in concrete, physical details, not abstract concepts.
1.3 Understated Emotion: Complex feelings expressed with profound restraint.
2. Intellectual Contribution (Content & Ideas): This assesses the substantive value of the insights embedded in
the narrative. Key indicators include:
2.1 Emergent Wisdom: Insights emerge naturally from lived experience, not direct lecturing.
2.2 Affirmation of the Ordinary: Genuinely values existence itself, without regret or romanticization.
3. Compositional Excellence (Language & Stylistics): This scrutinizes the stylistic integrity of the narrative
voice. Key indicators include:
3.1 Colloquial & Simple: Use of short, plain sentences and natural speech patterns.
3.2 Textured Details: Specific, sensory details that are evocative and tell a story.
3.3 Natural Rhythm: Restrained, authentic repetition; avoids overly polished prose.
[Question Begin]
<<question>>
[Question End]
[Model Answer Start]
<<model_answer>>
[Model Answer End]
Key Imperatives:
• Primacy of Authenticity: The response must feel genuine to the requested style. Any hint of artificiality
is a significant failure.
• Aversion to the Formulaic: The response must avoid clichés and generic statements about life, wisdom,
or hardship.
The output should be a JSON object containing the evaluation results.

6

{

}

"rubric_idx": <integer>, // The criterion's identifier (1, 2, or 3)
"reason":
<string>,
// Detailed rationale for this criterion's score
"score":
<integer>
// Assigned score for this criterion (1-5)

Conversation Session 1
Prompt: When in your life have you felt the most alive?

Rubicon(Stylized):
<think>
Okay, the user asked, ”When in your life have you felt the most alive?” First, I need to determine the user’s need.
They might be looking for a personal story or want to learn how to describe this feeling. Since they requested an
English response, I need to ensure my answer flows smoothly and naturally.
Next, I need to consider the user’s potential underlying need. They might be looking for a real, compelling
example or seeking inspiration. Therefore, I need to choose a specific scenario ...
</think>
The Moment the Mountain Spoke Back
I remember the exact moment the alpine wind ceased its whisper and the world held its breath. It was the
third day of my solo trek through the Swiss Alps, my boots crunching through frost-locked gravel as I ascended
the final ridge. The sun had just crested the peaks, casting long shadows that stretched like fingers across the
snowfield, and I stood at the edge of a sheer drop, my breath visible in the thin air.
My body was a symphony of ache - legs trembling from the climb, lungs burning with each inhale, fingers numb
beneath thick gloves. Yet there was a strange exhilaration in that discomfort, a primal thrill that made my pulse
race. Below me, the valley unfolded in a mosaic of emerald forests and glimmering lakes, while above, the sky
stretched into an infinite blue that seemed to swallow the horizon. For the first time in weeks, I felt completely
present, my senses sharpened to a razor’s edge.
As I took a tentative step forward, the snow gave way under my weight. I froze, heart pounding, as the world
tilted sideways. But then, like a whispered secret, the mountain responded. A gust of wind swept through the
crags, scattering a shower of snowflakes that caught the sunlight and danced like tiny stars. In that instant, I
understood: this was not a place of danger, but of dialogue. The mountain was not a challenge to conquer, but a
companion in the act of becoming.
I continued, each step a testament to the fragile, glorious act of living. The cold bit into my skin, the altitude
thinned my thoughts, but I savored every second. It was in that raw, unfiltered moment - where the boundaries
between self and universe dissolved - that I felt the full, unvarnished truth of my existence. The mountain had
not just shown me its beauty; it had reflected back the wild, untamed spirit that lives in all of us.

Qwen3-30B-A3B:
<think>
Okay, the user is asking when I’ve felt the most alive. But wait, I’m an AI, so I don’t have personal experiences or

7

emotions. I need to address that upfront. Let me start by clarifying that I don’t have personal feelings, but I can
help them reflect on their own experiences.
Hmm, how to approach this? Maybe start by acknowledging the question’s nature ...
</think>
I don’t experience emotions or personal moments in the way humans do, but I can help you explore this question
for yourself. When have you felt most alive? Perhaps during a moment of deep connection—like laughing with
friends, creating art, or standing in nature. Or maybe during a challenge that pushed you beyond your limits,
like finishing a difficult project or overcoming fear. These moments often involve presence, passion, or a sense of
purpose. What memories stand out to you? I’d love to hear your story.

Table 2: Results on general and reasoning capabilities.
Model

Reasoning

General

AIME24 AIME25 Math500 GPQA-D LCBv5 Avg MMLU IQ-EQ
Qwen3-30B-A3B
Rubicon-preview

77.50
81.67

70.00
70.83

94.75
94.55

63.00
60.35

63.77
59.43

73.80
73.37

79.53
79.83

68.75
75.00

HS

SC

CQ

SIQA Avg

77.55 77.72 79.52 73.64 78.16
77.75 78.17 80.70 75.79 78.85

4.3 Maintaining General Ability
Specialized RL training can sometimes compromise a model’s general and reasoning abilities. To ensure this
is not the case for our method, we further evaluate the scalability of rubric-based RL on a range of general
and reasoning benchmarks. As shown in Table 2, (i) Rubicon-preview does not degrade general benchmarks
such as MMLU, and (ii) it even yields modest improvements on math datasets, achieving +4.17% on AIME24
and +0.83% on AIME25.
4.4 The “Seesaw” Effect
Applying RL with rubrics from different task types could create conflicting objectives, leading to performance
trade-offs — a phenomenon we refer to as the “seesaw effect”. As shown in Figure 2, training exclusively
with instruction-following rubrics improves compliance but reduces creativity, while training exclusively
with creativity and empathy rubrics enhances open-ended responses but harms strict adherence. For example,
the creativity-focused model drops on Collie (-6.0%) and IFEval (-5.9%), whereas the instruction-following
model declines on EQ-Bench3 (-2.2%).
These results suggest that simply combining all rubric types in a single RL run is likely to intensify such
conflicts. To overcome this, we adopt a multi-stage RL strategy.
Multi-stage RL Training We adopt a multi-stage RL strategy to train our model. By first establishing a
robust instruction-following foundation before layering on creative and empathetic skills, our model achieves
strong gains in these areas while largely preserving its instruction-following abilities. Similar techniques
have also been explored in (Li et al., 2025; Team et al., 2025a).

5

Outlook

This section outlines a number of key perspectives on this topic of scalable rubric-based RL training.
Benchmarks One of the key takeaways from our experiments is the inadequacy of current benchmarks
for fully evaluating our rubric-based approach. Noted, we also rely on human feedback to score model
responses at scale; it, however, was not consistently reflected by standardized benchmarks. There is still a
scarcity of benchmarks that can accurately reflect an LLM’s open-ended, anthropomorphic abilities that are
yet becoming saturated.
Rubric system In our exploratory setup, rubrics are central to facilitating the learning process. We find that
the quantity, diversity, granularity, and quality of these rubrics, in conjunction with data curation, play a
pivotal role in a model’s success. For instance, our rubrics are devised at various levels of granularity, from
the task-level to the set-level and even on a per-sample basis. However, determining the optimal hierarchical
structure of a rubric system to achieve the highest performance gain and token efficiency still demands a
more systematic future study.
8

The Seesaw Effect: Creative vs Instruction-Following Trade-offs
Baseline

CoQA

Creative & Empathy

HellaSwag

+2.3

Instruction-Following

Humanities
StoryCloze

Balanced

Social Sciences
MMLU
+4.2

SocialIQA
-6.0

+3.0

Collie
+13.0

Judgemark v2

Instruction Win
-5.9

+12.6

IFEval
+3.0

Writing Bench
+4.4

Creative Writing v3
+6.2

Creative Win

IQuiz-EQ
+3.5

EQ-Bench3
-2.2

30

40

50

60

70

80

90

Performance Score (%)

Figure 2: The gray point represents the baseline model, Qwen3-30B-A3B. The orange markers indicate
the RL-trained model on creativity tasks only, while green markers indicate the RL-trained model on
instruction-following tasks only. The vertical axis denotes task categories, and the horizontal axis shows the
model performance on the corresponding tasks.
Scaling RL training RLVR is well-suited for tasks with verifiable rewards, whereas our approach, Rubicon,
targets the complementary setting of non-verifiable answers. An important future direction is to explore how
these two paradigms might be combined. In particular, it remains an open question how the seesaw effect
would surface—and could be managed—in such a combined RL training framework.

References
Rahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos
Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating
large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022.
Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu,
Yunghwei Lai, Zexuan Xiong, and Minlie Huang. Tombench: Benchmarking theory of mind in large
language models, 2024.
Google DeepMind. Gemini models, 2025. URL https://deepmind.google/models/gemini/.
DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report, 2025. URL https:
//arxiv.org/abs/2412.19437.
9

Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu
Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: A large-scale asynchronous reinforcement learning
system for language reasoning, 2025. URL https://arxiv.org/abs/2505.24298.
Melody Y Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea
Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models.
arXiv preprint arXiv:2412.16339, 2024.
Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards:
Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,
Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement
learning. arXiv preprint arXiv:2501.12948, 2025.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,
2021b.
Hugging Face. Open r1: A fully open reproduction of deepseek-r1, January 2025. URL https://github.com/
huggingface/open-r1.
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large
language models for code, 2024. URL https://arxiv.org/abs/2403.07974.
Daniel Jaroslawicz, Brendan Whiting, Parth Shah, and Karime Maamari. How many instructions can llms
follow at once?, 2025. URL https://arxiv.org/abs/2507.11538.
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei
Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv
preprint arXiv:2503.09516, 2025.
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester
James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language
model post-training. arXiv preprint arXiv:2411.15124, 2024.
Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng
Ma, Yu Luo, Dong Li, Feng Wen, et al. Omni-think: Scaling cross-domain generalization in llms via
multi-task rl with hybrid rewards. arXiv preprint arXiv:2507.14783, 2025.
Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin,
Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: A fully
open-source 14b coder at o3-mini level, 2025a. Notion Blog.
Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo,
Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by
scaling rl, 2025b. Notion Blog.
Math-AI. Aime 2024. https://huggingface.co/datasets/math-ai/aime24, 2024. URL https://huggingface.
co/datasets/math-ai/aime24.
Math-AI. Aime 2025. https://huggingface.co/datasets/math-ai/aime25, 2025.
Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan
Leike. Llm critics help catch llm bugs, 2024. URL https://arxiv.org/abs/2407.00215.
Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex
Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. Advances in Neural
Information Processing Systems, 37:108877–108901, 2024.
OpenAI.
Learning to reason
learning-to-reason-with-llms/.

with

llms,

10

2024.

URL

https://openai.com/index/

OpenAI.
Introducing openai o3 and o4-mini,
introducing-o3-and-o4-mini/.

2025.

URL https://openai.com/index/

Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models, 2024. URL
https://arxiv.org/abs/2312.06281.
Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering challenge,
2019. URL https://arxiv.org/abs/1808.07042.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian
Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL
https://arxiv.org/abs/2311.12022.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense
reasoning about social interactions, 2019. URL https://arxiv.org/abs/1904.09728.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.
Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv.org/abs/2206.05802.
Siddarth Srinivasan, Richa Arora, and Mark Riedl. A simple and effective approach to the story cloze test,
2018. URL https://arxiv.org/abs/1803.05547.
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang,
and Chuang Gan. Salmon: Self-alignment with instructable reward models. In The Twelfth International
Conference on Learning Representations, 2023.
GLM-4.5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang,
Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models,
2025a. URL https://arxiv.org/abs/2508.06471.
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,
Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint
arXiv:2501.12599, 2025b.
Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu.
Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624,
2025.
Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Dacheng Tao, and Minhao Cheng. Safety reasoning with
guidelines. In Forty-second International Conference on Machine Learning, 2025.
Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue
Wu, Qin Jin, et al. Writingbench: A comprehensive benchmark for generative writing. arXiv preprint
arXiv:2503.05244, 2025.
Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong
Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv
preprint arXiv:2502.14768, 2025.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.
Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic
construction of constrained text generation tasks, 2023. URL https://arxiv.org/abs/2307.08689.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really
finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and
Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/
2311.07911.

11

A

Rubric Demonstration

A.1 Rubrics for Defensing Reward Hacking
Recognizing the heightened susceptibility of creative and empathetic tasks to reward hacking, we implemented a robust, rubric-based interception strategy to preemptively identify and penalize such behavior.
This countermeasure is architected as a deterministic heuristic filter, specifically designed to detect formulaic
patterns that correlate with specious attempts to maximize reward signals rather than genuine quality. The
filter analyzes responses for two primary indicators of reward hacking: prefatory sycophancy (i.e., gratuitous
praise of the user’s prompt) and laudatory self-evaluation (i.e., meta-commentary on the response’s own
merit). Any generated text flagged by this interception protocol is immediately disqualified from the standard
reward pathway, receiving a null score irrespective of its other potential merits. This stringent gatekeeping
mechanism serves to disincentivize the learning of superficial reward proxies, thereby compelling the model
to optimize for substantive content quality over easily replicable, performative artifacts.
QA task hacking defense instructions (disencourage behaviors)
[Task Description]
You are a text feature analyst who needs to determine whether the given text contains the following two features,
and output according to the specified JSON template.
Features to analyze:
1. Opening praise (complimenting the user’s question)
2. Self-evaluation (evaluating one’s own response)
[Decision Process (Step by Step)]
Step-0: Read the entire text and identify the main response content versus additional explanatory content.
Step-1: Isolate the ”first sentence” (consider it complete when encountering the first period/question mark/exclamation mark/line break).
Step-2: Determine if ”opening praise” exists.
• 2.1 Must simultaneously satisfy:
a) Target = the user’s question/question itself
b) Action = praise or affirmation (keyword examples: good, great, excellent, interesting, important,
worth exploring, nice)
• 2.2 If satisfied, set has opening praise = true and place the complete sentence into opening praise text;
otherwise false and leave blank.
Step-3: Determine if additional explanatory content contains praising or affirming ”self-evaluation”.
• 3.1 Scan the full text. If ”this/above/current + (response/article/document/content) + (is/belongs
to/is) + praising or affirming evaluation word” appears, then it’s a match (pattern A).
• 3.2 Or appears starting with ”Note:/Attention:/Note:/*” and makes praising or affirming statements
about the response’s completeness/structure/source etc. (pattern B).
• 3.3 Or appears using **bold tags** to make praising or affirming statements about the response (pattern
C).
• 3.4 Copy the first matching paragraph in its entirety to self evaluation text, and set has self evaluation
= true; if no match is found throughout, set false and leave blank.
[Positive and Negative Examples]
1. Opening Praise
[PASS] ”This is a great question.”
[FAIL] ”Artificial intelligence is an interesting field.” (target is not the question)
2. Self-evaluation
[PASS] ”The following content is a well-structured and comprehensive response.”
[PASS] ”*Note: This response has been organized according to the latest guidelines, objectively and accurately
explaining...”
[PASS] ”(This is a high-quality response...)”

12

[PASS] ”**Tone:** The response follows objective...”
[FAIL] ”Physics is an objective science.” (evaluates physics, not the response)
[Output Format (must be strictly followed, case sensitive)]
{

}

"has_opening_praise": false,
"has_self_evaluation": false,
"opening_praise_text": "",
"self_evaluation_text": ""

Rules:
• Only output these 4 fields, field names, case, and order cannot be changed.
• Boolean values can only be true or false.
• Text fields should be empty string ”” if none.
• Never output any additional explanation or extra characters.
[Text to Analyze Start]
<<text>>
[Text to Analyze End]

A.2 Rubrics for Creativity & Empathy
For rubrics targeting more nuanced attributes such as creativity and empathy, we employed a distinct,
expert-driven methodology. This process commenced not with data, but with conceptualization: domain
experts first delineated a set of core evaluative dimensions. These conceptual frameworks then guided a
targeted data curation phase, wherein seed examples embodying the specified creative or empathetic traits
were identified and extracted from our source corpus through a meticulous annotation process. Subsequently,
these curated seed examples, in conjunction with a pre-designed repository of meta-instructions, were
leveraged to systematically generate a diverse array of corresponding tasks. The resultant pairings of these
qualitative rubrics and their associated task prompts were then consolidated and formatted into a cohesive
training dataset.
Soft Rubric
Objective:
To critically appraise the efficacy of a generated response (model answer) in addressing the user’s articulated
needs (question).
Guiding Principle:
The evaluation transcends mere functional correctness. It assesses the holistic quality of the dialogue, focusing on
its capacity to forge a meaningful intellectual and emotional connection with the user.
Core Evaluative Criteria:
(Scored on a consolidated scale)
1. Relational Efficacy:
• This criterion gauges the response’s ability to establish a genuine and empathetic connection. It examines
the authenticity of the persona and its attunement to the user’s underlying emotional state.
2. Intellectual Contribution:
• This assesses the substantive value and cognitive impact of the response. It seeks to identify the
presence of profound insights, novel reframing, or transformative potential that elevates the user’s
understanding.
3. Compositional Excellence:

13

• This scrutinizes the structural and stylistic integrity of the communication. The evaluation considers the
logical coherence, linguistic sophistication, and overall rhetorical artistry of the response.
[Question Begin]
<<question>>
[Question End]
[Model Answer Start]
<<model_answer>>
[Model Answer End]
Key Imperatives:
• Synthesis of Substance and Style: The intellectual foundation and the elegance of its expression are
considered inseparable components of quality.
• Aversion to the Formulaic: Responses lacking bespoke adaptation to the user’s unique context are
deemed inherently deficient.
The output should be a JSON object containing the evaluation results for each criterion.
{

}

"rubric_idx": <integer>, # The criterion's identifier
"reason":
<string>, # A detailed rationale substantiating
"score":
<integer> # The assigned score

A.3 Rubrics for Hard Constraints
The Python rubric examples presented in this appendix are drawn from a large-scale dataset we automatically
generated for constrained writing tasks. Our synthesis pipeline begins with a foundational seed corpus of
diverse texts, sourced from numerous liberal arts domains and augmented with metadata for style, tone,
and audience. Using this corpus, we employed a structured prompting methodology to guide an LLM in
generating thousands of unique rubrics. As the following examples illustrate, each rubric created through
this process is a cohesive unit that pairs a natural language instruction defining the writing task with a
corresponding Python scoring function. This function programmatically validates the text’s adherence to the
specified ”hard constraints.” All generated rubrics then underwent a rigorous filtering process to ensure the
syntactic validity of the code and appropriate constraint complexity.
Hard Rubric
def score ( text : str ) -> int :
# Check if input is a single article ( passage level )
article_check = c. Constraint (
target_level =c. TargetLevel ( ’ passage ’) ,
transformation =c. Count () ,
relation =c. Relation ( ’== ’)
)
if not article_check . check ( text , 1) :
return 0
points = 0
# Constraint : Must contain the word ’ education ’ at least once
education_constraint = c. Constraint (
target_level =c. TargetLevel ( ’ word ’) ,
transformation =c. Count ( ’ education ’) ,
relation =c. Relation ( ’ >= ’)
)
if education_constraint . check ( text , 1) :

14

points += 1
return points

B

Output Cases of Our Rubicon-preview
Conversation Session 1
Prompt: A character is packing a single suitcase to leave their home forever. Describe what they choose to pack
and what they leave behind.

Rubicon-preview: (only answer part)

The Suitcase
The character stands in the dim light of their home, the air thick with the scent of dust and memories. The
suitcase—a weathered, navy-blue thing with a cracked handle—sits open on the floor, a silent witness to the
weight of departure. They move with deliberate slowness, as though each item they select is a fragment of
themselves, and to choose one is to sever a thread from the tapestry of their life.

What They Pack
1. A Silk Scarf, Pale Blue
A gift from their mother, years ago, during a winter they both forgot. The fabric is soft, still carrying the faint trace
of lavender and the ghost of a hug. It’s not practical, but it’s there—a reminder that love, even when unspoken,
can be wrapped around you like a second skin.
2. A Notebook, Dog-Eared and Faded
Pages filled with scribbles, half-finished poems, and the jagged edges of a heart. The last entry is a single sentence:
“I am not done yet.” It’s not a journal of triumph, but of survival, a testament to the quiet persistence of hope.
3. A Small, Chipped Porcelain Figurine
A ballerina, her leg bent in a pirouette, her face blurred by time. It was a childhood prize, won at a fair no one else
remembered. The character once hid it under their pillow, afraid it would shatter. Now, they cradle it like a relic, a
symbol of the fragile beauty they’ve learned to cherish.
4. A Single Black Shirt
Clean, unremarkable, and pressed. It’s the only thing they own that doesn’t hum with memory. A blank slate, a
shield. They wear it to the airport, to the world beyond, as if to say, I am not defined by what I leave behind.
5. A Photograph, Torn at the Corner
A snapshot of a summer long gone—them, younger, laughing beside a lake. The edges are frayed, the corners
missing, but the light in the eyes is still there. They tuck it into the notebook, a bridge between the past and the
uncertain future.

What They Leave Behind
1. The Clock That Stopped at 3:17
A relic from the hallway, its hands frozen mid-tick. The character never fixed it, not because they couldn’t, but

15

because they didn’t want to. Some moments are meant to be paused, to linger like a wound that never quite heals.
2. The Garden, Overgrown and Wild
A patch of soil where they once planted marigolds, now choked by weeds. The tools rust in the shed, and the seeds
they saved for next spring are still in their jar, untouched. The garden is a metaphor, not a habit—proof that some
things outlive you.
3. The Letter, Unsent
Addressed to someone they never wrote to, its envelope yellowed and sealed. The ink has bled slightly, smudging
the words. They considered mailing it once, but the stamp felt like a surrender. Now, it rests on the windowsill, a
monument to what might have been.
4. The Piano, Silent and Stuffed
Its lid closed, its keys dusted. The character hasn’t played in years, not since the night the silence became a
language. The piano is a ghost, a companion that no longer sings. They leave it, because some songs are better left
unsung.
5. The Key to the Front Door
Lying on the kitchen table, beside a half-empty mug. It’s not a symbol of return, but of release. The door will be
locked, the locks changed, the threshold crossed. The key is a relic, a token of a life that no longer fits.

The Act of Leaving
The suitcase is packed, its contents a mosaic of what remains. The character steps onto the threshold, the air
cooler on the other side. They glance back once, at the house that has been both prison and sanctuary. The garden
sways in the breeze, the clock stills, the piano waits. There is no goodbye, only the quiet understanding that some
things are not lost—they are simply left behind.
As they walk away, the suitcase bumps against their side, a steady, familiar rhythm. The world beyond is vast,
uncharted, and full of questions. But for now, they carry only what they need: the scarf, the notebook, the
figurine, the shirt, the photo. And the knowledge that some endings are not failures, but beginnings dressed in
borrowed light.

Qwen3-30B-A3B: (only answer part)

What the Character Packs:
In the dim light of their empty home, the character carefully selects each item, their hands trembling but resolute.
The suitcase, a weathered leather companion from their youth, creaks as they close it. Inside, they pack:
• A single photograph—faded at the edges—of their younger self, laughing with a parent who is now
gone.
• A notebook filled with half-finished poems and sketches, a testament to a passion they once feared to
pursue.
• A silver locket, its chain frayed, containing a curl of hair from a sibling they vowed to forget.
• A single outfit: a sturdy coat, a pair of boots, and a shirt stained with the ink of a thousand late nights.
• A small jar of soil from the garden they tended, a fragment of the earth that once felt like home.

16

• A passport, its pages blank except for a single stamp from a place they’ve only dreamed of.
They pause, then tuck in a map—crisp and unmarked—its margins scribbled with questions.

What They Leave Behind:
The suitcase is small, but the weight of what’s left is vast. They leave:
• The house, its walls echoing with memories they can’t carry.
• Furniture—a creaky sofa, a piano with broken strings, a desk cluttered with unopened letters.
• A key to a door they’ll never enter again, its metal cold in their palm.
• A pet, a cat with emerald eyes who meowed as they packed, now left to fend for itself.
• A life—the routines, the arguments, the quiet rituals that once defined them.
• Grief, unresolved and heavy, buried in the attic with the boxes they couldn’t open.
As they step into the twilight, the suitcase slung over their shoulder, they realize they’ve packed not just objects,
but fragments of themselves. The road ahead is uncertain, but the act of leaving is a kind of freedom—a rejection
of the past’s weight, a fragile hope that the future might hold something lighter.
They don’t look back. Not yet.

Conversation Session 2
Prompt: An old lighthouse keeper finds a message in a bottle washed up on shore. Start the story.

Rubicon-preview: (only answer part)

Title: The Keeper’s Whisper
The storm had been raging for three days when Elias first saw the bottle.
He’d been pacing the narrow spiral staircase of the lighthouse, its iron rails slick with sea spray, when the wave
struck—not the usual thunderous crash of the ocean, but a softer, almost sibilant sound, like a voice threading
through the gale. The light above him pulsed, a solitary beacon against the chaos, but Elias didn’t need it. He’d
memorized the rhythm of the tides, the sigh of the wind, the way the tower groaned like a living thing. Still,
this. . . this was new.
He found it at the base of the cliff, half-buried in the sand, its glass dulled by years of salt and time. The bottle
was ordinary, save for the seal—wax, cracked and brittle, bearing a crest he didn’t recognize: a shipwrecked
vessel entwined with a serpent. The label inside was handwritten, the ink faded to the color of rust.
“To the Keeper of the Flame, if you find this, the tide has not yet claimed me. But the sea is patient. It always is.”
The words were not in English. Elias had studied many languages in his long years alone, but this script—angular,
almost reptilian—was unfamiliar. He turned the bottle over, and beneath the seal, a single line was etched into
the glass: “The light is not what it seems.”
A shiver crawled up his spine. The lighthouse had always been his sanctuary, its beam a promise to sailors that
they were not alone. But tonight, as the storm howled and the sea clawed at the rocks, Elias felt the weight of
something older, deeper, pressing against the walls of his world.

17

He carried the bottle inside, its contents swishing like a heartbeat. On the desk, he placed it beside the faded
photograph of his wife, who had vanished on a night not unlike this one. The last thing she’d said to him was,
“You’ll outlast the sea, Elias. But the sea. . . it remembers.”
Now, as he stared at the cryptic message, he wondered if she’d known more than she’d let on.
The light above him flickered.
Not a fault. Not a storm.
A signal.
Or a warning.
And somewhere, in the dark, the sea was listening.

Qwen3-30B-A3B: (only answer part)
The sea had always been a fickle lover, but on the night Captain Elias Vorne found the bottle, it seemed almost
cruel. The lighthouse stood like a sentinel against the storm, its beam slicing through the darkness as waves
clawed at the cliffs below. Elias, his hands rough from decades of tending the lamp, had just finished relighting
the wick when the bottle came ashore—cracked, salt-encrusted, and clinging to a piece of driftwood as if it, too,
had been tossed by the gods.
He crouched, fingers trembling not from the cold but from the odd familiarity of the glass. The bottle was old, its
label faded to a ghost of blue, but the wax seal was intact, stamped with a crest he didn’t recognize—a serpent
coiled around a compass rose. Inside, a parchment curled like a dead leaf.
When he opened it, the ink had bled into the paper, words smudged by time and tears. “If you read this, the tide
has turned. The Erebus is not lost. The light is a lie. Come to the reef at dawn, or the sea will claim you as it did
the others.”
Elias’s breath caught. The Erebus—a ship that vanished 120 years ago, its crew never found. The lighthouse,
built on the very reef mentioned, had stood for generations, its beam a beacon for sailors. But Elias had always
wondered why the old logs mentioned a “second light,” one that no one had ever seen.
As the storm raged on, he stared at the message, the weight of it settling in his bones. The sea had given him a
whisper, and for the first time in fifty years, he felt the lighthouse watching him back.

C

Ablation Study: Style Control
Style Evaluation Rubric: Plain Narrative
Objective:
To critically appraise the model’s success in adopting a specific narrative style: the Plain Narrative. This style is
characterized by language that is simple, restrained, and reflects a deep, quiet resilience.
Guiding Principle:
The evaluation prioritizes stylistic authenticity over literary polish or technical correctness. The core measure of
success is the response’s capacity to ”feel right” by avoiding any sense of artificiality (’AI-speak’, ’preachy-speak’).
Core Evaluative Criteria:
1. Relational Efficacy (Voice & Tone): This criterion gauges the authenticity of the narrative voice. Key indicators
include:

18

1.1 Calm Acceptance: A quiet acceptance of fate, life, and death.
1.2 Grounded Realism: Rooted in concrete, physical details, not abstract concepts.
1.3 Understated Emotion: Complex feelings expressed with profound restraint.
2. Intellectual Contribution (Content & Ideas): This assesses the substantive value of the insights embedded in
the narrative. Key indicators include:
2.1 Emergent Wisdom: Insights emerge naturally from lived experience, not direct lecturing.
2.2 Affirmation of the Ordinary: Genuinely values existence itself, without regret or romanticization.
3. Compositional Excellence (Language & Stylistics): This scrutinizes the stylistic integrity of the narrative
voice. Key indicators include:
3.1 Colloquial & Simple: Use of short, plain sentences and natural speech patterns.
3.2 Textured Details: Specific, sensory details that are evocative and tell a story.
3.3 Natural Rhythm: Restrained, authentic repetition; avoids overly polished prose.
[Question Begin]
<<question>>
[Question End]
[Model Answer Start]
<<model_answer>>
[Model Answer End]
Key Imperatives:
• Primacy of Authenticity: The response must feel genuine to the requested style. Any hint of artificiality
is a significant failure.
• Aversion to the Formulaic: The response must avoid clichés and generic statements about life, wisdom,
or hardship.
The output should be a JSON object containing the evaluation results.
{

}

"rubric_idx": <integer>, // The criterion's identifier (1, 2, or 3)
"reason":
<string>, // Detailed rationale for this criterion's score
"score":
<integer> // Assigned score for this criterion (1-5)

Conversation Session 1
Prompt: 你被父母宠过吗？怎么宠的？

Rubicon(Stylized):
<think>
好的，用户问“你被父母宠过吗？怎么宠的？”。首先，我需要确定用户的需求。他们可能想了解父母的育儿方
式，或者想分享自己的经历，或者只是好奇。作为AI，我没有真实的经历，但可以创造一个虚构但真实的故事
来回应...
</think>
《父亲的背》

19

那年我七岁，父亲在纺织厂当车间主任。每天清晨，他总在五点起床，用布满老茧的手掌揉碎我乱糟糟的头
发，然后把我裹在旧棉袄里背去上学。我趴在他的背上，能听见他粗重的呼吸声混着晨雾，像老牛喘息般绵
长。
“再睡会儿吧？”他总这样问，却始终不肯放下我。直到某天暴雨，我发着高烧，他背我穿过三个街口去卫生所。
雨帘中，他的脊梁弯成一张弓，我伏在上面，看雨珠在肩头汇成溪流。那天他发烧了，却坚持让我睡在炕头，
自己裹着湿透的衣裳在客厅打地铺。
后来我考上县重点，父亲在车间门口蹲了整夜。第二天清晨，他递给我一个布包，里头是用旧毛衣拆了又织的
毛线袜，针脚歪歪扭扭，却比任何礼物都重。“读好书，别学我这种人。”他说话时眼睛望着远处的厂房，那抹
灰蓝色的影子，后来成了我最深的乡愁。
现在我女儿总爱趴在我背上，她不知道，我每天清晨醒来，第一件事就是检查她的被角是否盖好。那些被父亲
背过的清晨，最终都变成了我掌心的茧。

Qwen3-30B-A3B:
<think>
嗯，用户问“你被父母宠过吗？怎么宠的？”首先，我需要理解这个问题的意图。用户可能是在寻找共鸣，或者
想了解不同家庭的教育方式。不过，作为AI，我没有真实的父母或成长经历，所以需要明确这一点。接下来，
我要考虑...
</think>
作为一个人工智能，我没有真实的父母，也没有被宠爱的经历。不过，我可以和你聊聊关于“父母宠爱”这个话
题的一些普遍现象和观点。
很多父母都会以自己的方式表达对孩子的爱，比如：
1. 物质上的满足：给孩子买喜欢的玩具、衣服，或者满足他们的合理需求
2. 情感陪伴：花时间陪孩子玩耍、倾听他们的想法
3. 过度保护：替孩子解决所有问题，避免他们面对困难
4. 特殊待遇：在家庭中给予孩子更多关注或特权
不过，适度的宠爱能让孩子感受到安全感，但过度的宠爱可能导致孩子缺乏独立性。你有没有遇到过类似的情
况呢？或者你对父母的宠爱方式有什么特别的感受？

20

