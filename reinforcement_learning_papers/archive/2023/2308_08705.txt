Partially Observable Multi-Agent Reinforcement Learning
with Information Sharing

arXiv:2308.08705v4 [cs.LG] 28 Oct 2025

Xiangyu Liu†

Kaiqing Zhang†

Abstract
We study provable multi-agent reinforcement learning (RL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use
of computationally intractable oracles, we advocate leveraging the potential information-sharing
among agents, a common practice in empirical multi-agent RL, and a standard model for multiagent control systems with communication. We first establish several computational complexity
results to justify the necessity of information-sharing, as well as the observability assumption
that has enabled quasi-polynomial time and sample single-agent RL with partial observations, for
tractably solving POSGs. Inspired by the inefficiency of planning in the ground-truth model, we
then propose to further approximate the shared common information to construct an approximate
model of the POSG, in which an approximate equilibrium (of the original POSG) can be found
in quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop
a partially observable multi-agent RL algorithm whose time and sample complexities are both
quasi-polynomial. Finally, beyond equilibrium learning, we extend our algorithmic framework
to finding the team-optimal solution in cooperative POSGs, i.e., decentralized partially observable
Markov decision processes, a more challenging goal. We establish concrete computational and
sample complexities under several structural assumptions of the model. We hope our study could
open up the possibilities of leveraging and even designing different information structures, a wellstudied notion in control theory, for developing both sample- and computation-efficient partially
observable multi-agent RL.

1

Introduction

Recent years have witnessed the fast development of reinforcement learning (RL) in a wide range of
applications, including playing Go games (Silver et al., 2017), robotics (Lillicrap et al., 2016; Long
et al., 2018), video games (Vinyals et al., 2019; Berner et al., 2019), and autonomous driving (ShalevShwartz et al., 2016; Sallab et al., 2017). Many of these application domains by nature involve multiple decision-makers operating in a common environment, with either aligned or misaligned objectives
that are affected by their joint behaviors. This has thus inspired surging research interests in multiagent RL (MARL), with both deeper theoretical and empirical understandings (Busoniu et al., 2008;
Zhang et al., 2021a; Hernandez-Leal et al., 2019).
One central challenge of multi-agent learning in these applications is the imperfection of information, or more generally, the partial observability of the environments and other decision-makers.
Specifically, each agent may possess different information about the state and action processes while
making decisions. For example, in vision-based multi-robot learning and autonomous driving, each
agent only accesses a first-person camera to stream noisy measurements of the object/scene, without accessing the observations or past actions of other agents. This is also sometimes referred to
† University of Maryland, College Park. Email: {xyliu999,kaiqing}@umd.edu. A preliminary version of the paper has
been accepted to the International Conference on Machine Learning (ICML) 2023 (Liu and Zhang, 2023).

1

as information asymmetry in game theory and decentralized decision-making (Behn and Ho, 1968;
Milgrom and Roberts, 1987; Nayyar et al., 2013a; Shi et al., 2016). Despite its ubiquity in practice,
theoretical understandings of MARL in partially observable settings remain scant. This is somewhat
expected since even in single-agent settings, planning and learning under partial observability suffer
from well-known computational and statistical hardness results (Papadimitriou and Tsitsiklis, 1987;
Mundhenk et al., 2000; Jin et al., 2020). The challenge is known to be amplified for multi-agent
decentralized decision-making (Witsenhausen, 1968; Tsitsiklis and Athans, 1985). Existing partially
observable MARL algorithms with finite-time/sample guarantees either only apply to a small subset of highly structured (tree-like) problems (Zinkevich et al., 2007; Kozuno et al., 2021), or require
computationally intractable oracles (Liu et al., 2022b).
With these hardness results that can be doubly exponential in the worst case, even a quasipolynomial (time and sample complexity) algorithm could represent a non-trivial improvement in
partially observable MARL. In particular, we ask and attempt to answer the following question:
Can partially observable MARL be made both statistically and computationally efficient?
We provide some results towards answering the question positively, by leveraging the potential information sharing among agents, together with a careful compression of the shared information. Indeed,
the idea of information sharing has been widely used in empirical MARL, e.g., centralized training
that aggregates all agents’ information for more efficient training (Lowe et al., 2017; Rashid et al.,
2020); it has also been widely used to model practical multi-agent systems in decentralized control,
e.g., those with delayed communication among agents (Witsenhausen, 1971; Nayyar et al., 2010). We
detail our contributions below.
Contributions. We study provable multi-agent RL under the framework of partially observable
stochastic games (POSGs), with potential information sharing among agents. First, we establish several computational complexity results of solving POSGs in the presence of information sharing, justifying its necessity, together with the necessity of the observability assumption made in the recent
literature, which enabled single-agent partially observable RL without computationally intractable
oracles. Second, we propose to further approximate the shared common information to construct
an approximate model, and characterize the computational complexity of planning in this model. We
show that for several standard information-sharing structures, a simple finite-memory compression
can lead to expected approximate common information models in which planning an approximate
equilibrium (in terms of solving the original POSG) has quasi-polynomial time complexity. Third,
based on the planning results, we develop a partially observable multi-agent RL algorithm whose
time and sample complexities are both quasi-polynomial, which we refer to as being quasi-efficient for
short (given that polynomial-complexity algorithms are generally deemed as being efficient). Fourth,
beyond equilibrium learning, we extend our framework and algorithm to finding the team-optimal
solution in cooperative POSGs, i.e., decentralized partially observable Markov decision processes
(Dec-POMDPs), a more challenging goal. To this end, we identify several structural assumptions
on the model under which quasi-efficient planning and learning become attainable. To the best of
our knowledge, this is the first study of provable partially observable MARL with (quasi-)efficiency,
with both sample and computational complexities. Key to our results is to carefully incorporate insights from both information structures/sharing, a well-studied framework in decentralized stochastic
control theory, and the tractability conditions investigated in recent reinforcement learning theory.

1.1

Related Work

Information sharing in theory and practice. The idea of information-sharing and the study of
more general information structures have been extensively studied in decentralized stochastic control
2

(Witsenhausen, 1971; Nayyar et al., 2010, 2013b), as well as dynamic games (Nayyar et al., 2013a;
Gupta et al., 2014; Ouyang et al., 2016). The common-information-based approach in the seminal
works Nayyar et al. (2013a,b) provided significant inspiration for our work. The information-sharing
structures in these works have enabled backward-induction-based planning algorithms even in this decentralized setting. Performance bounds of information compression in such a framework were later
derived in Mao et al. (2020); Kao and Subramanian (2022). However, neither computation nor sample
complexities of the algorithms were analyzed in these works. On the other hand, information-sharing
has become a normal practice in empirical MARL (Lowe et al., 2017; Sunehag et al., 2018; Rashid
et al., 2020), usually instantiated via the so-called centralized training, where all agents’ information
was shared in the training to improve learning efficiency. However, information-sharing/structure
has not been fully investigated in the theoretical studies of MARL.
Decentralized stochastic control and decision-making. Decentralized stochastic control and
decision-making are known to have unique challenges, compared to the single-agent and centralized counterpart, since the seminal works Witsenhausen (1968); Tsitsiklis and Athans (1985). In
particular, Tsitsiklis and Athans (1985) showed that variations of the classical “team decision problem” can be NP-hard. Later, Bernstein et al. (2002) showed that planning in Dec-POMDPs, a special
class of POSGs with an identical reward function shared across agents, can be NEXP-hard in finding the team-optimal solution. Hansen et al. (2004) provided a popular POSG planning algorithm,
though without any complexity guarantees. There also exist other approximate/heuristic algorithms
for solving POSGs (Emery-Montemerlo et al., 2004; Kumar and Zilberstein, 2009; Horák et al., 2017).
RL in partially observable environments. It is known that in general, planning in even singleagent POMDPs can be PSPACE-complete (Papadimitriou and Tsitsiklis, 1987) and thus computationally hard. Statistically, learning POMDPs can also be hard in general (Krishnamurthy et al.,
2016; Jin et al., 2020). There has thus been a growing body of literature on RL in POMDPs with
additional assumptions, e.g., Azizzadenesheli et al. (2016); Jin et al. (2020); Liu et al. (2022a). However, these works only focused on statistical efficiency, and the algorithms usually required computationally intractable oracles. More recently, Golowich et al. (2022b) has identified the condition of
γ-observability in POMDPs (firstly introduced in Even-Dar et al. (2007)), which enabled a quasipolynomial-time planning algorithm for such POMDPs. Subsequently, Golowich et al. (2022a) has
developed an RL algorithm based on the planning one in Golowich et al. (2022b), which was both
sample and computation (quasi-)efficient. The key enabler of these (quasi-)efficient algorithms is
the use of the finite-memory policy class, whose (near-)optimality has also been studied lately in Kara
(2022); Kara and Yüksel (2022), under different assumptions on both the transition dynamics and the
observation channels. Rather than statistical and computational complexity guarantees, Subramanian et al. (2022) has analyzed the performance bounds of general approximate information states
(AIS) in partially observable environments. Our finite-memory compression may be viewed as a
kind of AIS, although it does not satisfy the uniform approximation conditions in Subramanian et al.
(2022) (and also in Mao et al. (2020); Kao and Subramanian (2022)). In fact, relaxing such conditions
to expected versions is the key to obtaining our (quasi-)efficient sample and computational complexities (cf. Remark 2). Other information compression results include Tang et al. (2024) for dynamic
games, and Sinha and Mahajan (2023); Cai et al. (2024) for RL with asymmetric information.
Provable multi-agent reinforcement learning. There has been a fast-growing literature on provable MARL algorithms with sample efficiency guarantees, e.g., Bai et al. (2020); Liu et al. (2021);
Zhang et al. (2020); Xie et al. (2020); Zhang et al. (2021b); Wei et al. (2021); Daskalakis et al. (2020);
Jin et al. (2024); Song et al. (2021); Daskalakis et al. (2022); Mao et al. (2022); Leonardos et al. (2022);

3

Zhang et al. (2021c); Ding et al. (2022); Chen et al. (2023). However, these works have been exclusively focused on the fully observable setting of Markov/stochastic games. The only MARL algorithms under partial observability that enjoy finite-sample guarantees, to the best of our knowledge,
are those in Liu et al. (2022b); Kozuno et al. (2021). However, the algorithm in Kozuno et al. (2021)
only applied to POSGs with certain tree-structured transitions, while that in Liu et al. (2022b) required computationally intractable oracles. In general, information-sharing/structure has not been
fully investigated in the theoretical studies of MARL with finite-sample and computation complexities.
One exception is Kao et al. (2022), which exploited a special hierarchical information structure in the
bandits and MDP settings. Another exception is Altabaa and Yang (2024), which appeared online
after the acceptance of the conference version of this paper (Liu and Zhang, 2023), and also incorporated (general) information structure considerations into the algorithm design and analyses. However,
the algorithms in Altabaa and Yang (2024) also required computationally intractable oracles, with a
focus on statistical-tractability only.
Independent result in Golowich et al. (2023). We note that after the acceptance to ICML 2023 of
the preliminary version of the paper, an updated version of Golowich et al. (2022b) in its proceedings
form appeared online, i.e., Golowich et al. (2023). In Golowich et al. (2023), a quasi-polynomial-time
planning algorithm for solving a class of partially observable stochastic games was also discussed.
There are several differences compared to our results. First, in the class of POSGs considered in
Golowich et al. (2023), the observation is identical for all the agents, and each agent has access
to the joint action history of all the agents. Notably, this setting exactly corresponds to the fullysharing/symmetric-information case covered by our information-sharing framework (see Example 3 in
Section 3). Moreover, we study both Nash equilibrium (NE) in cooperative/zero-sum games and correlated equilibrium (CE), coarse correlated equilibrium (CCE) in general-sum games, while Golowich
et al. (2023) only focused on finding CCE in general-sum games; we also establish a result for learning equilibria in POSGs with both quasi-polynomial sample and computational complexities, while
Golowich et al. (2023) only focused planning with model knowledge. Additionally, we also establish
results for team-optimum learning for Dec-POMDPs under certain structural conditions.
Notation. For two sets B and D, we define B \ D as the set of elements that are in B but not in
D. We use ∅ to denote the empty set and [n] := {1, · · · , n}. For integers a ≤ b, we denote a sequence
(xa , xa+1 , · · · , xb ) by xa:b . If a > b, then it denotes an empty sequence. When the sequence index starts
from m and ends at n, we will treat xa:b as xmax{a,m}:min{b,n} . For an event E, we use 1 to denote the
indicator function such that 1(E) = 1 if the event E is true and 0 otherwise. For a finite set B, we
let ∆(B) denote the set of distributions over
h p(x)B.i For two probability distributions p, q, we define the
2-Rényi divergence as D2 (p||q) := log Ex∼p q(x) . We also define p ≪ q if q(x) = 0 implies p(x) = 0.

2

Preliminaries

2.1

POSGs and information sharing

Model. Formally, we define a finite-horizon POSG with n agents by a tuple G =
(H, S, {Ai }ni=1 , {Oi }ni=1 , T, O, µ1 , {ri }ni=1 ), where S denotes the state space with |S| = S, Ai denotes the
action space for the i th agent with |Ai | = Ai , and H denotes the length of each episode. We denote by
ah := (a1,h , · · · , an,h ) the joint action of all the n agents at time step h, and by A = A1 × · · · × An the joint
Q
action space with |A| = A = ni=1 Ai . We use T = {Th }h∈[H] to denote the collection of the transition
matrices, so that Th (· | s, a) ∈ ∆(S) gives the probability of the next state if joint action a is taken at
state s and step h. In the following discussions, for any given a, we treat Th (a) ∈ R|S|×|S| as a matrix,
4

where each row gives the probability for the next state. We use µ1 to denote the distribution of the
initial state s1 , and Oi to denote the observation space for the i th agent with |Oi | = Oi . We denote by
Q
o := (o1 , . . . , on ) the joint observation of all the n agents, and by O := O1 ×· · ·×On with |O| = O = ni=1 Oi .
We use O = {Oh }h∈[H+1] to denote the collection of the joint emission matrices, so that Oh (· | s) ∈ ∆(O)
gives the emission distribution over the joint observation space O at state s and step h. For notational convenience, we will at times adopt the matrix convention, where Oh is a matrix with rows
Oh (· | s). We also denote by Oi,h (· | s) ∈ ∆(Oi ) the marginalized emission for the i th agent at state s.
Finally, ri = {ri,h }h∈[H] is a collection of reward functions, so that ri,h (sh , ah ) ∈ [0, 1] is the reward of the
i th agent given the state sh and (joint) action ah taken at step h. This general formulation of POSGs
includes several important subclasses. For example, decentralized partially observable Markov decision processes (i.e., Dec-POMDPs) are POSGs where the agents share a common reward function,
i.e., ri = r, ∀i ∈ [n]; zero-sum POSGs are POSGs with n = 2 and r1 + r2 = 1. Note that we require
r1 + r2 to be 1 instead of 0 to be consistent with our assumption that ri,h ∈ [0, 1] for each i ∈ {1, 2} and
h ∈ [H], and this requirement does not lose any optimality as one can always subtract the constantsum offset to attain a zero-sum structure. Hereafter, we may use the terminology cooperative POSG
and Dec-POMDP interchangeably.
Information sharing, common and private information. The i th agent at step h in the POSG maintains its own information, τi,h , a collection of (potentially partial) historical observations and actions
at step h, namely, τi,h ⊆ {o1 , a1 , o2 , · · · , ah−1 , oh }, and the collection of such histories at step h is denoted
by Ti,h . In many practical examples (see some concrete ones in Section 3), agents may share part
of the history with each other, which may introduce more structures in the game that enable both
sample and computation efficient learning. The information sharing splits the full history into the
common/shared and the private information for each agent. The common information at step h is a
subset of the joint history τh : ch ⊆ {o1 , a1 , o2 , · · · , ah−1 , oh }, which is available to all the agents in the system, and the collection of the common information is denoted as Ch and we define Ch = |Ch |. Given
the common information ch , each agent also has the private information pi,h = τi,h \ ch , where the
collection of the private information for the i th agent is denoted as Pi,h and its cardinality as Pi,h . The
joint private information at step h is denoted as ph , where the collection of the joint private history
Q
is given by Ph = P1,h × · · · × Pn,h and the corresponding cardinality is Ph = ni=1 Pi,h . We allow ch or pi,h
to take the special value of ∅ when there is no common or private information. In particular, when
Ch = {∅}, the problem reduces to a general POSG without any favorable information structure; when
Pi,h = {∅}, every agent holds the same history, and it reduces to a POMDP when the agents share a
common reward function, for which the goal is usually to find the team-optimal policy.
Throughout, we also assume that the common information and private information evolve over
time properly, as formalized below.
Assumption 1 (Evolution of common and private information). We assume that common information
and private information evolve over time as follows:
• Common information ch is non-decreasing with time, that is, ch ⊆ ch+1 for all h. Let zh+1 = ch+1 \ ch .
Thus, ch+1 = ch ∪ zh+1 . Further, we have
zh+1 = χh+1 (ph , ah , oh+1 ),

(2.1)

where χh+1 is a fixed transformation. We use Zh+1 to denote the collection of all zh+1 at step h.
• Private information evolves according to:
pi,h+1 = ξi,h+1 (pi,h , ai,h , oi,h+1 ),
where ξi,h+1 is a fixed transformation.
5

(2.2)

Equation (2.1) states that the increment in the common information, and thus the common information at the next step ch+1 , depends on the “new” information {ah , oh+1 } generated between steps h
and h + 1 and part of the “old” information ph . The incremental common information can be generated by certain sharing and communication protocols among agents. Equation (2.2) implies that
the evolution of private information only depends on the newly generated private information ai,h
and oi,h+1 . These evolution rules are standard in the literature (Nayyar et al., 2013a,b), specifying
the source of common information and private information. Based on such evolution rules, we define {fh }h∈[H+1] and {gh }h∈[H+1] , where fh : Ah−1 × O h → Ch and gh : Ah−1 × O h → Ph for h ∈ [H + 1],
as the mappings that map the joint history to common information and joint private information,
respectively.

2.2

Policies and value functions

We define a stochastic policy for the i th agent at step h as:
πi,h : Ωh × Pi,h × Ch → ∆(Ai ),

(2.3)

where Ωh is a space of random seeds shared among agents. The corresponding policy class is denoted
as Πi,h . Hereafter, unless otherwise noted, when referring to policies, we mean the policies given in
the form of (2.3), which map the available information of the i th agent, i.e., the private information
and the common information, together with the potentially local random seed ωi,h ∈ Ωh , to the
distribution over her actions. We further denote by Πi = ×h∈[H] Πi,h the policy space for the i th agent
ei,
and Π as the joint policy space. As a special case, we define the space of deterministic policy as Π
e i maps the private information and common information to a deterministic action for
ei ∈ Π
where π
th
e
the i agent, and denote the joint space of such policies as Π.
One important concept in the common-information-based framework is called the prescription
(Nayyar et al., 2013b,a), defined for the i th agent at step h as γi,h : Pi,h → ∆(Ai ). With such a prescription function, agents can take actions purely based on their local private information. We define
Γi,h as the function class for prescriptions, and Γh as the function class of joint prescriptions. Intuitively, the partial function πi,h (· | ωi,h , ·, ch ) is a prescription given some ωi,h and ch . We will define
πi as a sequence of policies for the i th agent at all steps h ∈ [H], i.e., πi = {πi,1 , · · · , πi,H }. A (potentially correlated) joint policy is denoted as π = π1 ⊙ π2 · · · ⊙ πn ∈ Π. A product policy is denoted
as π = π1 × π2 · · · × πn ∈ Π if the distributions of drawing each seed ωi,h for different agents are
independent. Furthermore, sometimes, we might resort to deterministic joint policies with joint history as input (which could potentially go beyond Π): π = {π1 , π2 , · · · , πH }, where πh is defined as:
πh : Ah−1 × O h → A. We denote the collection of such policies as Πdet , and note that Π ⊆ ∆(Πdet ).
For any policy π ∈ ∆(Πdet ) and event E, we write PGs1:h ,a1:h−1 ,o1:h ∼π1:h−1 (E) to denote the probability of E
when {s1:h , a1:h−1 , o1:h } is drawn from a trajectory following the policy π1:h−1 from step 1 to h − 1 in the
π
,G
model G. We will use the shorthand notation Ph 1:h−1 (·) if the definition of {s1:h , a1:h−1 , o1:h } is evident.
At times, if the time index h is evident, we will write it as Pπ,G
h (·). If the event E does not depend
G
on the choice of π, we will use Ph (·) and omit π. Moreover, we will write EGs1:h ,a1:h−1 ,o1:h ∼π [·] or EGπ [·]
to denote the expectations over the trajectories under policy π, and use the shorthand notation EG [·]
if the expectation does not depend on the choice of π. Furthermore, if we are given some model M
M
M
(other than G), the notation of PM
h (·), Eπ [·], and E [·] are defined in the same way. We will hereafter
use strategy and policy interchangeably. We are now ready to define the value function for each agent
under our framework:
Definition 1 (Value function with information sharing). For each agent i ∈ [n] and step h ∈ [H], given
common information ch and joint policy π = {πi }ni=1 ∈ Π, the value function conditioned on the common
6

information of the i th agent is defined as:
 H
X


π,G
Vi,h
(ch ) := EGπ 

h′ =h



ri,h′ (sh′ , ah′ ) ch  ,

where the expectation is taken over the randomness from the model G, policy π, and the random seeds.
π,G
π,G
For any cH+1 ∈ CH+1 : Vi,H+1
(cH+1 ) := 0. For the value function at the first step, we denote Vi,1
(∅) :=
P
π,G
G
H
G
E [Vi,h (c1 )] = Eπ [ h=1 ri,h (sh , ah )], where the expectation is taken over the randomness of c1 , which is a
function of o1 and does not depend on π.
π,G
Correspondingly, we can define the prescription-value function Qi,h
(ch , γh ), a generalization of
th
the action-value function in MDPs, indicating the expected return for the i agent when all the agents
firstly adopt the prescriptions {γj,h }j∈[n] at step h and then follow π (cf. Definition 12).

2.3

Solution concepts

With the definition of the value functions, we can accordingly define the solution concepts, ϵ-NE (and
similarly ϵ-CCE, ϵ-CE), and ϵ-team optimum under the information-sharing framework as follows.
Definition 2 (ϵ-approximate Nash equilibrium with information sharing). For any ϵ ≥ 0, a product
policy π⋆ ∈ Π is an ϵ-Nash equilibrium of the POSG G if
!
⋆
πi′ ×π−i
,G
π⋆ ,G
⋆
NE-gap(π ) := max max
Vi,1
(∅) − Vi,1 (∅) ≤ ϵ.
′
i

πi ∈Πi

Definition 3 (ϵ-approximate coarse correlated equilibrium with information sharing). For any ϵ ≥ 0,
a joint policy π⋆ ∈ Π is an ϵ-approximate coarse correlated equilibrium of the POSG G with information
sharing if:
!
⋆
π′ ×π−i
,G

CCE-gap(π⋆ ) := max max
Vi,1i
′
i

πi ∈Πi

⋆

π ,G
(∅) − Vi,1
(∅) ≤ ϵ.

Definition 4 (ϵ-approximate correlated equilibrium with information sharing). For any ϵ ≥ 0, a joint
policy π⋆ ∈ Π is an ϵ-approximate correlated equilibrium of the POSG G with information sharing if:
!
⋆
(φi ⋄πi⋆ )⊙π−i
,G
π⋆ ,G
⋆
CE-gap(π ) := max max Vi,1
(∅) − Vi,1 (∅) ≤ ϵ,
i

φi

where φi is called a strategy modification and φi = {φi,h,ch ,pi,h }h,ch ,pi,h , with each φi,h,ch ,pi,h : Ai → Ai being
a mapping from the action set to itself. The space of φi is denoted as Φi . The composition φi ⋄ πi will work
as follows: at the step h, when the i th agent is given ch and pi,h , the action chosen to be (a1,h , · · · , ai,h , · · · , an,h )
will be modified to (a1,h , · · · , φi,h,ch ,pi,h (ai,h ), · · · , an,h ). Note that this definition extends those in Song et al.
(2021); Liu et al. (2021); Jin et al. (2024) to our settings when there exists common information, and is a
natural generalization of the definition in the normal-form game case (Roughgarden, 2010).
Definition 5 (ϵ-approximate team-optimum in Dec-POMDPs with information sharing). When the
reward functions ri,h are identical for all i ∈ [n], i.e., ri,h = rh , the POSG reduces to a Dec-POMDP, and a
e is an ϵ-approximate team-optimal policy if: V π⋆ ,G (∅) ≥ max ′ e V π′ ,G (∅) − ϵ, where we have
policy π⋆ ∈ Π
1
π ∈Π 1
omitted the agent index for the value function.
It is also worth noting that, under given information-sharing structures, the team-optimal solution is always a NE in the Dec-POMDP setting, and in general, a NE is always a CE, and a CE is
always a CCE.
7

3

Information Sharing in Applications

The information-sharing structure can indeed be common in real-world applications. For example, for a self-driving car to avoid collision and successfully navigate, the other cars from the same
fleet/company would usually communicate with each other (possibly with delays) about the road situation. The separation between common information and private information then arises naturally
(Gong et al., 2016). Similar examples can also be found in cloud computing and power systems (Altman et al., 2009). Here, we outline several representative information-sharing structures that were
firstly introduced by Nayyar et al. (2013a) and can fit into our algorithmic framework.
Example 1 (One-step delayed sharing). At any step h ∈ [H + 1], the common and private information
are given as ch = {o1:h−1 , a1:h−1 } and pi,h = {oi,h }, respectively. In other words, the agents share all the
action-observation history until the previous step h − 1, with only the new observation being the private
information. This model has been shown useful for power control (Altman et al., 2009).
Example 2 (State controlled by one controller with asymmetric delay sharing). We assume there are 2
agents for convenience. It extends naturally to n-agent settings. Consider the case where the state dynamics
are controlled by agent 1, i.e., Th (· | sh , a1,h , a2,h ) = Th (· | sh , a1,h , a′2,h ) for any a2,h , a′2,h . For the cooperative
setting that aims to find approximate team-optimum later (cf. Section 6), we additionally assume, for
P
this example, that the reward function has an additive structure, i.e., rh (sh , ah ) = j∈[n] rj,h (sh , aj,h ) for
some functions {rj,h }j∈[n] . The information structure is given as ch = {o1,1:h , o2,1:h−d , a1,1:h−1 }, p1,h = ∅,
p2,h = {o2,h−d+1:h }, i.e., agent 1’s observations are available to agent 2 instantly, while agent 2’s observations
are available to agent 1 with a delay of d ≥ 1 time steps. We will regard d as a constant throughout. This
kind of asymmetric sharing is common in network routing (Pathak et al., 2008), where packages arrive at
different hosts with different delays, leading to asymmetric delay sharing among the hosts.
Example 3 (Symmetric information game). Consider the case when all observations and actions are
available for all the agents, and there is no private information. Essentially, we have ch = {o1:h , a1:h−1 } and
pi,h = ∅. We will also denote this structure as fully sharing hereafter.
Example 4 (Information sharing with one-directional-one-step delay). Similar to the previous cases,
we also assume there are 2 agents for ease of exposition, and the case can be generalized to multi-agent cases
straightforwardly. Similar to the one-step delay case, we consider the situation where all observations of
agent 1 are available to agent 2, while the observations of agent 2 are available to agent 1 with one-step
delay. All the past actions are available to both agents. That is, in this case, ch = {o1,1:h , o2,2:h−1 , a1:h−1 }, and
agent 1 has no private information, i.e., p1,h = ∅, and agent 2 has private information p2,h = {o2,h }.
Example 5 (Uncontrolled state process). Consider the case where the state transition does not depend on
the actions, that is, Th (· | sh , ah ) = Th (· | sh , a′h ) for any sh , ah , a′h , h. For the reward, we assume it still depends
on the action of one agent to avoid trivial solutions at each step h. An example of this case is the information
structure where controllers share their observations with a general delay of d ≥ 1 time steps. In this case,
the common information is ch = {o1:h−d } and the private information is pi,h = {oi,h−d+1:h }. Such information
structures can be used to model repeated games with incomplete information (Aumann et al., 1995).

4

Hardness and Planning with Exact Model

4.1

Hardness on finding equilibria

Recently, reference Golowich et al. (2022b) considered observable POMDPs (firstly introduced in
Even-Dar et al. (2007)) that rule out the ones with uninformative observations, for which computationally (quasi)-efficient algorithms can be developed. In the hope of obtaining computational
8

(quasi)-efficiency for POSGs (including Dec-POMDPs), we thus make a similar observability assumption on the joint emission matrix as below. Note that this is weaker than making the assumption on
the individual emission matrix of each agent.
Assumption 2 (γ-observability). Let γ > 0. For h ∈ [H], we say that the matrix Oh satisfies the γobservability assumption if for each h ∈ [H], any b, b′ ∈ ∆(S),
⊤ ′
′
O⊤
h b − Oh b 1 ≥ γ b − b 1 .

A POSG (Dec-POMDP) satisfies γ-observability if all its Oh for h ∈ [H] do so.
Examples of an observation matrix which satisfies γ-observability include the random channel
which outputs the hidden state with probability γ, and otherwise outputs a random state uniformly
(i.e., from a “noisy sensor”) or an extra dummy observation ∅ deterministically (i.e., from a “failure mode”). Meanwhile, although the tractability of NE/CE/CCE in normal-form games has been
extensively studied, its formal tractability in POSGs has been less studied. Here by the following
proposition, we show that both Assumption 2 and some favorable information-sharing structure are
necessary for NE/CE/CCE to be computationally tractable, even for the special classes of zero-sum
POSGs and cooperative POSGs. Specifically, they are necessary in the sense that missing either one of
them would make seeking approximate NE/CE/CCE computationally hard, whose proof is deferred
to Section D.1.
Proposition 1. For zero-sum or cooperative POSGs with only information-sharing structures, or only
Assumption 2, but not both, computing ϵ-NE/CE/CCE is PSPACE-hard.
Hence, we will now focus on planning and learning under these assumptions.

4.2

Planning with strategy-independent common belief

For both optimal and equilibrium policy computation, it is known that backward induction is one of
the most useful approaches for solving (fully-observable) stochastic games. However, the essential
impediment to applying backward induction in asymmetric-information/partially observable dynamic
games is the fact that an agent’s posterior beliefs about the system state and about other agents’
information may depend on the strategies used by the agents in the past. If the nature of system
dynamics and the information structure of the game ensure that the agents’ posterior beliefs are
strategy independent, then a backward induction can be derived for equilibrium computation (Nayyar
et al., 2013a; Gupta et al., 2014). We formalize this conceptual argument as the following assumption.
Assumption 3 (Strategy independence of beliefs). Consider any step h ∈ [H], any choice of joint policies
π ∈ Π, and any realization of common information ch that has a non-zero probability under the trajectories
′
generated by π1:h−1 . Consider any other policies π1:h−1
, which also give a non-zero probability to ch . Then,
π

,G

π′

,G

we assume that: for any such ch ∈ Ch , and any ph ∈ Ph , sh ∈ S, Ph 1:h−1 (sh , ph | ch ) = Ph 1:h−1 (sh , ph | ch ) .
This assumption has been made in the literature (Nayyar et al., 2013a; Gupta et al., 2014), which
is related to the notion of one-way separation in stochastic control, that is, the estimation (of the state
in standard stochastic control and of the state and private information in our case) in Assumption 3 is
independent of the control strategy. For more detailed discussions, we refer to Nayyar et al. (2013a).
Before proceeding with further analysis, we introduced some common examples in Section 3 that
satisfy this assumption (see Nayyar et al. (2013a) and also Section D.4).
With Assumption 3, we are able to develop the planning algorithm (summarized in Algorithm
1) with the following time complexity. The algorithm is based on value iteration on the common
9

information space, which runs in a backward way, enumerating all possible ch at each step h and
computing the corresponding equilibrium in the prescription space. Note that a value-iteration
algorithm for NE computation was firstly also studied in Nayyar et al. (2013a), over the space of
common-information-based beliefs (instead of that of common information). By planning over the
common-information space, we can establish its computational complexity, which was not established in Nayyar et al. (2013a), and enables a more efficient planning algorithm later by truncating
the common information properly (cf. Section 5.1). We now establish the computational complexity
of Algorithm 1 more concretely.
Theorem 1. Fix ϵ > 0. For the POSG G that satisfies Assumptions 1 and 3, given access to the belief
PGh (sh , ph | ch ), Algorithm 1 computes an ϵ-NE if G is zero-sum or cooperative, and an ϵ-CE/CCE if G is
general-sum, with time complexity maxh∈[H] Ch · poly(S, A, Ph , H, 1ϵ ).
To prove this, we will prove a more general theorem (see Theorem 2 later), of which Theorem 1 is
a special case. This theorem characterizes the dependence of computational complexity on the cardinality of the common information set and private information set. To get a sense of how large Ch Ph
could be, we consider one common scenario where each agent has perfect recall, i.e., she remembers
what she did in prior moves, and also remembers everything that she knew before.
Definition 6 (Perfect recall). We say that agent i has perfect recall if for any h ∈ [H], it holds that
{ai,1:h−1 , oi,1:h } ⊆ τi,h , and τi,h ⊆ τi,h+1 .
If each agent has perfect recall as defined above, we can show that Ch Ph must be exponential in
the horizon index h. Proof of the result below can be found in Section D.1.
Lemma 1. Fix any h ∈ [H], and suppose Assumption 1 holds. Then, if each agent has perfect recall as given
in Definition 6, then for any information-sharing structure, we have Ch Ph ≥ (OA)h−1 .
From this result, together with Theorem 1, we know that the computational complexity of such a
naive planning algorithm must suffer from the exponential dependence of Ω((OA)h ). This negative
result implies that it is barely possible to get computational efficiency for planning in the true model
G, since the cardinality Ch Ph has to be very large oftentimes. Meanwhile, it is worth noting that for
obtaining Theorem 1, we have not yet leveraged our Assumption 2. Thus, this negative result is in
line with our fundamental hardness results in Proposition 1.

5

Planning and Learning with Approximate Common Information

5.1

Computationally (quasi-)efficient planning

Previous exponential complexity comes from the fact that Ch and Ph could not be made simultaneously
small in the standard scenario with perfect recall. To address this issue, we propose to further compress the information available to the agent under certain regularity conditions, while approximately
maintaining the optimality of the policies computed/learned from the compressed information. Notably, there is a trade-off between compression error and computational tractability. We show next that
by properly compressing only the common information, we can obtain efficient planning (and learning) algorithms with favorable suboptimality guarantees. To introduce the idea, we first define the
approximate common information model in our setting.
Definition 7 (Approximate common information model). We define an expected approximate common information model of G as


M,z
M
b
b
M := {Ch }h∈[H+1] , {φh+1 }h∈[H] , {Ph }h∈[H] , Γ ,b
r ,
10

bh is the space of approximate common
where Γ = ×h∈[H] Γh is the function class for joint prescriptions, C
M,z b
bh and {γi,h }i∈[n] ∈
information at step h, Ph : Ch × Γh → ∆(Zh+1 ) gives the probability of zh+1 given b
ch ∈ C
M
M
}i∈[n],h∈[H] ,
Γh , with Zh+1 being the space of incremental common information. Similarly, for b
r = {b
ri,h
M b
bh and {γi,h }i∈[n] ∈ Γh . We
b
r : Ch × Γh → [0, 1] gives the reward of the i th agent at step h given b
ch ∈ C
i,h

bh | for any h ∈ [H + 1]. We say M is an (ϵr (M), ϵz (M))-expected-approximate common
bh := |C
denote C
information model of G with the approximate common information defined by {b
ch }h∈[H+1] for some
compression functions {Compressh }h∈[H+1] that yield b
ch = Compressh (ch ), if it satisfies the following:
bh+1
• It evolves in a recursive manner, i.e., for each h ∈ [H], there exists a transformation function φ
such that
bh+1 (b
b
ch+1 = φ
ch , zh+1 ),
(5.1)
where we recall that zh+1 = ch+1 \ ch is the common information increment.
• It suffices for approximately evaluating the performance, i.e., for any i ∈ [n] and h ∈ [H], any prescription γh ∈ Γh and joint policy π′ ∈ Πdet , it holds that
M
EGa1:h−1 ,o1:h ∼π′ EG [ri,h (sh , ah ) | ch , γh ] −b
ri,h
(b
ch , γh ) ≤ ϵr (M).

(5.2)

• It suffices for approximately predicting common information increment: for any h ∈ [H], γh ∈ Γh ,
ch , γh ), we have
π′ ∈ Πdet , and for PGh (zh+1 | ch , γh ) and PM,z
h (zh+1 |b
ch , γh ) 1 ≤ ϵz (M).
EGa1:h−1 ,o1:h ∼π′ PGh (· | ch , γh ) − PM,z
h (· |b

(5.3)

Remark 1. The approximate model M defined above can be treated as a (fully-observable) stochastic
bh }h∈[H+1] , Γ is the joint action space, the composition of {PM,z }h∈[H] and
game, where the state space is {C
h
M
bh+1 }h∈[H] yields the state transition kernel, and b
(b
ch , γh ) is the reward of the i th agent at step h given
{φ
ri,h
state b
ch and joint action γh .
Remark 2. Note that related definitions in Kao and Subramanian (2022); Mao et al. (2020); Subramanian
et al. (2022) required the total variation distance between PGh (· | ch , γh ) and PM,z
ch , γh ) to be uniformly
h (· |b
bounded for all ch . In fact, this kind of compression may be unnecessary and computationally intractable
when it comes to efficient planning. Firstly, some common information ch may have very low visitation
frequency under any policy π, which means that we can allow large variation between true common belief
and approximate common belief for these ch , which are inherently less important for the decision-making
problem. Secondly, even in the single-agent setting, where ch = {a1:h−1 , o1:h }, the size of such approximate
information with errors uniformly bounded for all {a1:h−1 , o1:h } may not be sub-exponential even under
Assumption 2, as shown by Example B.2 in Golowich et al. (2022b). Therefore, for some kinds of common
information, it is actually not possible to reduce the order of complexity through the approximate common
belief with errors uniformly bounded.
Although we have characterized what conditions the expected approximate common information
model M should satisfy to well approximate the underlying G, it is in general unclear how to construct
such an M, i.e., mainly how to define ({PM,z
r M ), even if we are already given certain compresh }h∈[H] ,b
sion functions. To address this, in the following, we provide a way to construct ({PM,z
r M ) from
h }h∈[H] ,b
M,c
an approximate belief over the state and the private information {Ph (sh , ph |b
ch )}h∈[H] .

11

Definition 8 (Model-belief consistency). We say the expected approximate common information model
M is consistent with some belief {PM,c
ch )}h∈[H] if it satisfies the following for all i ∈ [n], h ∈ [H]:
h (sh , ph |b
PM,z
ch , γh ) =
h (zh+1 |b

X
sh ,ph ,ah ,oh+1 :
χh+1 (ph ,ah ,oh+1 )=zh+1



PM,c
ch )
h (sh , ph |b

n
Y

γj,h (aj,h | pj,h ) ×

j=1

X


Th (sh+1 | sh , ah )Oh+1 (oh+1 | sh+1 ) ,

sh+1

(5.4)
M
b
ri,h
(b
ch , γh ) =

X
sh ,ph ,ah

PM,c
ch )
h (sh , ph |b

n
Y

γj,h (aj,h | pj,h )ri,h (sh , ah ).

(5.5)

j=1

With such an expected approximate common information model, similar to Algorithm 1, we
develop a value-iteration-type algorithm (see pseudocode in Algorithm 3) running on the model M
instead of G, which outputs an approximate NE/CE/CCE, enjoying the following guarantees. The
key benefit of requiring the model M to be consistent with some belief is that under this condition,
the stage game in Algorithm 3 can be formulated as a multi-linear game of polynomial size, thus
computing its equilibrium is computationally tractable (cf. Section D.2).
Theorem 2. Fix ϵr , ϵz , ϵe > 0. Given any (ϵr , ϵz )-expected-approximate common information model M for
the POSG G under Assumptions 1 and 3. Furthermore, if M is consistent with some given approximate
belief {PM,c
ch )}h∈[H] (in the sense of Definition 8), then there exists an algorithm, Algorithm 3,
h (sh , ph |b
that can output an ϵ-NE if G is zero-sum or cooperative, or ϵ-CE/CCE if G is general-sum, where ϵ :=
bh · poly(S, A, Ph , H, 1 ).
2Hϵr + H 2 ϵz + Hϵe , with time complexity maxh∈[H] C
ϵ
e

As a sanity check, by choosing the compression function as the identity mapping, Theorem 2
recovers Theorem 1.
Planning in observable POSGs without intractable oracles. Theorem 2 applies to any expected
approximate common information model as given in Definition 7, by substituting the corresponding
bh . Note that it does not provide a way to construct such expected approximate common information
C
models that ensure the computation complexity in the theorem is (quasi-)polynomial.
Next, we show that in several natural and standard information structure examples, a simple
finite-memory compression can attain the goal of computing ϵ-NE/CE/CCE without computationally intractable oracles, where we refer to Section D.4 for the concrete form of the finite-memory
compression. Based on this, we present the corresponding quasi-polynomial time complexities as
follows.
Theorem 3. Fix ϵ > 0. Under Assumption 2, for all the information-sharing structures in Section 3, there
exists a quasi-polynomial time algorithm that can compute an ϵ-NE if G is zero-sum or cooperative, and an
ϵ-CE/CCE if G is general-sum.

5.2

Statistically (quasi-)efficient learning

Until now, we have been assuming the full knowledge of the model G (the transition kernel, emission,
and reward functions). In this full-information setting, we are able to construct some model M to
approximate the true model G according to the conditions we identified in Definition 7. However,
when we only have access to the samples drawn from the POSG G, it is difficult to directly construct
such a model M due to the lack of the model specification. To address this issue, we propose to
construct a specific expected approximate common information model that depends on the policies
f 1:H ). For such a model, one
π1:H that generate the data for such a construction, which is denoted by M(π
12

could simulate and sample by running policies π1:H in the true model G. The choice of π1:H will be
f 1:H ) to be a good approximation of G.
specified later to ensure M(π
Compared to Golowich et al. (2022a), there are several key technical challenges our analysis needs
to address: firstly, Golowich et al. (2022a) only considered finite-memory approximation for POMDPs,
where the sample complexity can be easily characterized by the length of the finite memory. In contrast, our goal is to deal with a more general compression scheme such that it can handle different
common information structures, for which we need to define a generalized quantity that can characterize the sample complexity under general compression schemes (cf. Definition 10). Secondly,
and more importantly, Golowich et al. (2022a) essentially learned the transition and reward of the
approximate model by simply enumerating all possible actions, which corresponds to enumerating
f 1:H )
f 1:H
M(π
bh to learn PM(π ),z (· |b
(b
ch , γh ), if one
ch , γh ) and b
r
all possible prescriptions γh ∈ Γh for each b
ch ∈ C
h

i,h

naively applies its algorithm and analyses to our setting. This will lead to an exponential sample
complexity since even the number of possible deterministic prescriptions is APh (while all possible
randomized prescriptions are even larger and infinitely many). To address this challenge, we identify
a decomposition on the aforementioned quantities to separately learn the distributions of private information and the next observation. Analyzing such a separate learning procedure requires a careful
bh and ph ∈ Ph that are rarely visited by πh .
examination of those b
ch ∈ C
f 1:H ), we present the following defTo introduce the aforementioned approximate model M(π
inition, where the key is to introduce a set of approximate common information-based beliefs
h
,G
{Pπ
(sh , ph |b
ch )}h∈[H] , which is generated by running a certain policy πh ∈ ∆(Πdet ) under the true
h
model G.
f (as in
Definition 9 (Policy-dependent approximate common information model). Given a model M
1:H
h
det
f
Definition 7) and H joint policies π , where each π ∈ ∆(Π ) for h ∈ [H], we say M is a policyf 1:H ), if it is consistent
dependent expected approximate common information model, denoted as M(π
h

,G
(sh , ph |b
ch )}h∈[H] (as per Definition 8).
with the policy-dependent belief {Pπ
h

Now we present the main theorem for learning under an expected approximate common inforf 1:H ). A major difference from the analysis for planning in Section 5.1 is that, we
mation model M(π
need to explore the space of approximate common information, which is a function of a sequence
of observations and actions, and we propose to characterize the length of the approximate common
information as defined below.
Definition 10 (Length of approximate common information). Given the compression functions
{Compressh }h∈[H+1] , we define the integer b
L > 0 as the minimum length such that there exists a mapb
b
min{
L,h}
min{
L,h}
b
b
ping fh : A
×O
→ Ch such that for each h ∈ [H + 1] and joint history {o1:h , a1:h−1 }, we have
b
fh (xh ) = b
ch , where xh = {a
, · · · , ah−1 , oh }.
b ,o
b
max{h−L,1}

max{h−L,1}+1

Such an b
L will help characterize our final sample complexity, since we need to do exploration for
the steps after h − b
L, and b
L characterizes the cardinality of the space to be explored. With this defif 1:H ), i.e., mainly learning the two
b
nition of L, we develop Algorithm 6, which learns the model M(π
1:H
1:H
f
f
quantities PM(π ),z and b
r M(π ) , by executing policies π1:H in the true model G, with the following
sample complexity.
f 1:H ), and
Theorem 4. Suppose the POSG G satisfies Assumptions 1 and 3. Given H policies π1:H , M(π
h
h
det
b
L as in Definition 10, where each π ∈ ∆(Π ), π b = Unif(A) for h ∈ [H]. Fix the parameters
h−L:h
δ1 , θ1 , θ2 , ζ1 , ζ2 , ϵe > 0 for Algorithm 6, and some φ > 0, define the approximation error for estimating
f 1:H ) using samples under the policies π1:H as ϵapx (π1:H ,b
M(π
L, ζ1 , ζ2 , θ1 , θ2 , φ). Then, Algorithm 6, can
learn an ϵ-NE if G is zero-sum or cooperative, and an ϵ-CE/CCE if G is general-sum, with probability at
13

bh , H, A, O, 1 , 1 , 1 , 1 ) · log 1 ,
least 1 − δ1 , with a sample complexity N0 = poly(maxh∈[H] Ph , maxh∈[H] C
ζ 1 ζ 2 θ1 θ2
δ1
1:H
2
1:H
2
1:H
f
f
b
where ϵ := Hϵr (M(π )) + H ϵz (M(π )) + (H + H)ϵapx (π , L, ζ1 , ζ2 , θ1 , θ2 , φ) + Hϵe .
A detailed version of the theorem is in Section C.2. This meta-theorem establishes a sample comf 1:H ) in an onplexity guarantee of learning expected approximate common information model M(π
1:H
line exploration setting, which holds for any compression functions and policies π , whose choices
are specified next.
Sample (quasi-)efficient learning in POSGs without intractable oracles. Now we apply this
meta-theorem, and obtain quasi-polynomial time and sample complexities for learning the ϵNE/CE/CCE, for several standard information structures.
Theorem 5. Under Assumption 2, for all the information-sharing structures in Section 3, there exists a
multi-agent RL algorithm that learns an ϵ-NE if G is zero-sum or cooperative, and an ϵ-CE/CCE if G
is general-sum, with probability at least 1 − δ, with both quasi-polynomial time and sample complexities
Cγ −4 log SHO
γϵ

(AO)

log 1δ for some universal constant C > 0.1

Due to space constraints, a detailed version of the theorem is presented in Section C.2, with proof
provided in Section D.5. Note that our algorithm is computationally (quasi-)efficient, in contrast to
the only existing sample-efficient MARL algorithm for POSGs in Liu et al. (2022b), which relied on
computationally intractable oracles.

6

Finding Team-Optimum in Dec-POMDPs

Until now, we have primarily focused on solving equilibria in POSGs. One notable subclass of POSGs
are the Dec-POMDPs, for which a stronger (than equilibrium) solution concept of team-optimum (cf.
Definition 5) is usually preferred. Our algorithmic framework developed in Section 5.1 for planning
can be readily extended to computing the team optimal solution, where the only modification is to
replace the equilibrium-computation subroutine at each step h over the prescription space in Algorithm 3 by a joint-maximization one over the prescription space. Specifically, we only need to replace
the line 9 of Algorithm 3 by its line 11, i.e., the following step:


ch , γ1,h , · · · , γn,h ),
π1,h (· |b
ch , ·), · · · , πn,h (· |b
ch , ·) ← arg max Qh⋆,M (b
γ1,h ,··· ,γn,h

(6.1)

where we omit the agent index for the Q-function, since all the agents share the same Q-function for
the Dec-POMDP setting.
Unfortunately, although we can show in Proposition 8 that such a Q-value is linear w.r.t. each
γi,h , it is not necessarily concave w.r.t. {γ1,h , · · · , γn,h } jointly. Thus, implementing this maximization
subroutine can be computationally intractable. In fact, it is an NP-hard problem without additional
assumptions.
Proposition 2. Without additional assumptions, even with n = 2 agents, solving Equation (6.1) is NPhard.
Proof of Proposition 2 is deferred to Section D.7. Hence, it seems hopeless to solve Equation (6.1)
efficiently. Fortunately, many Dec-POMDPs in real-world applications enjoy certain structures that
1 Note that throughout the paper, we regard the delay d in the examples in Section 3 as a constant. In fact, as shown in
the full version of the theorem in Section C.2, d is allowed to grow logarithmically with the horizon H without changing
the order of the computational or sample complexities.

14

can be exploited for efficient computation. Specifically, we identify several (sets of) assumptions
below, under which solving Equation (6.1) can be computationally tractable. Note that since we need
to do planning in the approximate model M, which is oftentimes constructed based on the original
model G and some approximate belief {PM,c
ch )}h∈[H] , we will necessarily need assumptions on
h (sh , ph |b
these two quantities, for which we refer to as the Part (1) and Part (2) of the assumptions below,
respectively.
Condition 1: Turn-based structures. Part (1). For G, we assume that at each step h, there is only
one agent, denoted as ctt(h) ∈ [n] that can affect the state transition. Hence, the transition dynamics
take the forms of Th : S × Actt(h) → S. Meanwhile, since only agent ctt(h) can affect the transition,
we assume the increment of the common information zh+1 in Assumption 1 is only a function of
(ph , actt(h),h , oh+1 ), i.e., zh+1 = χh+1 (ph , actt(h),h , oh+1 ) instead of χh+1 (ph , ah , oh+1 ). In other words, since at
step h, agents other than ctt(h) do not affect the transition, we assume their actions are not shared. For
the reward, we additionally assume that the reward function has an additive structure, i.e., rh (sh , ah ) =
P
j∈[n] rj,h (sh , aj,h ) for some functions {rj,h }j∈[n] . Part (2). For the approximate belief, we do not impose
any assumption. Note that such turn-based structures have been common in the (fully-observable)
stochastic game settings (Filar and Vrieze, 2012; Bai and Jin, 2020).
Condition 2: Nested information-sharing. Part (1). For G, we do not impose any assumption.
Part (2). For the approximate belief, we assume that all the agents form a hierarchy according to
the private information they possess. Without loss of generality, we assume for each i, j ∈ [n] such
ij
ij
that for j < i, it holds that pj,h = Yh (pi,h ) for some deterministic function Yh . More formally, the apij

ch ) is the posterior
ch ) = 1, where PM,c
proximate belief satisfies that PM,c
h (pj,h | pi,h ,b
h (pj,h = Yh (pi,h ) | pi,h ,b

distribution induced by the joint distribution PM,c
ch ). In other words, the σ -algebra generated
h (sh , ph |b
th
by the private information of the i agent includes that of the j th agent. This structure has also been
studied in Peralez et al. (2024) with a heuristic search approach.
Condition 3: Factorized structures. Part (1). For G, we assume that the state sh at each step h ∈ [H]
can be partitioned into n local states, i.e., sh = (s1,h , s2,h , · · · , sn,h ). Meanwhile, the transition kerQ
nel takes the product form of Th (sh+1 | sh , ah ) = ni=1 Ti,h (si,h+1 | si,h , ai,h ), the emission also takes the
Q
product form of Oh (oh | sh ) = ni=1 Oi,h (oi,h | si,h ), and the reward function can be decoupled into n
P
terms such that rh (sh , ah ) = i∈[n] ri,h (si,h , ai,h ). Part (2). For the approximate belief, we assume the
approximate common information and its increment can be factorized so that b
ch = (b
c1,h , · · · ,b
cn,h ),
b
zi,h+1 = (z1,h+1 , · · · , zn,h+1 ), and their evolutions additionally satisfy that b
ci,h+1 = φi,h+1 (b
ci,h , zi,h+1 ),
b
zi,h+1 = χi,h+1 (pi,h , ai,h , oi,h+1 ) for some functions φi,h+1 and χi,h+1 . Correspondingly, the approximate
Q
belief needs to satisfy that PM,c
ch ) = ni=1 PM,c
ci,h ), for some functions {PM,c
h (sh , ph |b
i,h (si,h , pi,h |b
i,h }i∈[n],h∈[H] .
Under each of these conditions, Equation (6.1) can be solved exactly with time complexity
poly(S, A, Ph ). The key insight into why these conditions suffice is that, they make solving the joint
maximization in Equation (6.1) equivalent to either solving individual maximization for each agent, or
sequential maximization across agents that can be solved via dynamic programming. Formal statements can be found in Proposition 11, Proposition 12, and Proposition 13 for each condition, respectively. The key insight into why these conditions suffice is that, they make solving the joint maximization in Equation (6.1) equivalent to either solving individual maximization for each agent, or sequential
maximization across agents that can be solved via dynamic programming. Once Equation (6.1) can
be solved computationally efficiently, computation of the team optimum of Dec-POMDPs becomes
tractable, under the same algorithmic framework as Section 5.1.

15

Theorem 6. Fix ϵ > 0, and consider a Dec-POMDP G satisfying Assumption 2, then all the examples in
Section 3 except the one-step delayed sharing case satisfy either Condition 1 or Condition 2 in Section 6.
Hence, Equation (6.1) can be solved in time complexity poly(S, A, Ph ) for these cases. Correspondingly,
there exists a quasi-polynomial time algorithm that can compute an ϵ-team optimal policy of G. For the
one-step delayed sharing case, if one additionally assumes that the Dec-POMDP G satisfies Part (1) of
Condition 3, then there also exists a quasi-polynomial time algorithm that can compute an ϵ-team optimal
policy of G, and moreover, the time complexity is polynomial (instead of exponential) in the number of
agents n.
Extension to learning settings without model knowledge. With the planning oracle for DecPOMDPs developed above, our framework of learning in POSGs can be readily extended to learning in Dec-POMDPs accordingly, achieving both quasi-polynomial time and sample complexities for
learning the approximate team-optimal policy. Due to space constraints, we defer the detailed results
to Section D.7.

7

Technical Details

In this section, we present the proofs for the main results introduced before. More details can be
found in the Appendices.

7.1

Proof of Theorem 2

For notational simplicity, we present the main proofs for the NE/CCE case, and the CE case can be
derived similarly.
b⋆ under M. As we mentioned in Section 5.1, AlgoStep 1: Evaluating the equilibrium gap of π
b⋆ enjoys the standard
rithm 3 essentially performs value iteration on M, where the output policy π
guarantee of value iteration for (fully-observable) stochastic games at each step h ∈ [H], with the state
bh and action being γh ∈ Γh . The formal result is stated as follows.
being b
ch ∈ C
b⋆ , satisfies that
Lemma 2. Fix the input M and ϵe > 0 for Algorithm 3. The output of the algorithm, i.e., π
⋆
π ×b
π−i
,M

for any h ∈ [H + 1], ch ∈ Ch , and πi ∈ Πi , Vi,hi

b⋆

π ,M
(ch ) + (H + 1 − h)ϵe .
(ch ) ≤ Vi,h

Step 2: Bounding the value difference between G and M. Since what we care about in the end is
the equilibrium gap in the actual game G, we first bound the value difference between M and G in
terms of ϵz (M) and ϵr (M).


π,G
π,M
Lemma 3. For any given policy π′ ∈ ∆(Πdet ), π ∈ Π, and h ∈ [H +1], we have EGπ′ Vi,h
(ch )−Vi,h
(ch ) ≤
(H − h + 1)ϵr +

(H−h+1)(H−h)
ϵz .
2

⋆
b⋆ and its unilaterally deviated policy πi ×b
Note that this lemma holds for any π ∈ Π, thus also π
π−i
,
facilitating the following steps.

b⋆ under G. Now we are ready to evaluate π
b⋆ ,
Step 3: Evaluating the equilibrium gap of π
⋆
the output of Algorithm 3 in G. We define for each agent i ∈ [n] the best response as πi ∈

16

⋆
π ×b
π−i
,G

arg maxπi ∈Πi Vi,1i

(∅). Now for any π′ ∈ ∆(Πdet ):

π⋆ ×b
π⋆ ,G
b⋆ ,G
π
EGπ′ Vi,hi −i (ch ) − Vi,h
(ch )





  ⋆

π⋆ ×b
π⋆ ,G
b⋆ ,M
b ,M
b⋆ ,G
π
π
π
Vi,hi −i (ch ) − Vi,h
(ch ) + Vi,h
(ch ) − Vi,h
(ch )
 ⋆ ⋆
  ⋆

π ×b
π ,G
π⋆ ×b
π⋆ ,M
b ,M
b⋆ ,G
π
π
≤ EGπ′ Vi,hi −i (ch ) − Vi,hi −i (ch ) + Vi,h
(ch ) − Vi,h
(ch )

= EGπ′



+ (H + 1 − h)ϵe
≤ 2(H − h + 1)ϵr + (H − h)(H − h + 1)ϵz + (H − h + 1)ϵe ,
where the second step is from Lemma 9 and the third step is by Lemma 3. Letting h = 1, we conclude
that NE/CCE-gap(b
π⋆ ) ≤ 2Hϵr + H 2 ϵz + Hϵe .
Step 4: Analyzing computational complexity. Note that Algorithm 3 is of the double-loop type. (1)
bh at each h ∈ [H]. (2) For the inner-loop: the main comFor the outer-loop: it enumerates all b
ch ∈ C
⋆,M
(b
ch , · · · )}i∈[n] . Note
putation comes from computing the ϵe -NE/CE/CCE of the game defined by {Qi,h
that if we treat this game as a normal-form game with the action space being all the deterministic
prescriptions, then any normal-form game solvers can be plugged in. However, the corresponding
P
time complexity will suffer from the size of the action space, i.e., Ai i,h . Instead, we show that if we
⋆,M
regard each γi,h as a concatenation of simplexes, i.e., γi,h ∈ ∆(Ai )Pi,h , then Qi,h
is linear w.r.t. each
individual prescription under our model-belief consistency condition. Thus, an ϵe -NE/CE/CCE can be
solved with time complexity depending only polynomially on the dimension of γi,h , which is Ai Pi,h ,
P

in contrast to the previous Ai i,h . By putting the time complexity for the outer loop and inner loop
together, we obtain the final time complexity.
□

7.2

Proof of Theorem 3

We take the one-step delayed sharing case as an example, and defer the proofs for other information
structure examples to Section D.4.
Step 1: Bounding ϵr (M),ϵz (M) with the belief error. As in Definition 8, one can construct M
from some given compression functions and approximate beliefs of {PM,c
ch )}h∈[H] . Thus, we
h (sh , ph |b
can relate the model errors of M, i.e., ϵr (M) and ϵz (M), with the error of the approximate belief.
Specifically, we show the following.
Lemma 4. Given any belief {PM,c
ch )}h∈[H] and an associated consistent (in the sense of Definition 8)
h (sh , ph |b
expected approximate common information model M, it holds that for any h ∈ [H], ch ∈ Ch , γh ∈ Γh :
PGh (· | ch , γh ) − PM,z
ch , γh )
h (· |b

≤ PGh (·, · | ch ) − PM,c
ch ) ,
h (·, · |b

(7.1)

M
EG [ri,h (sh , ah ) | ch , γh ] −b
ri,h
(b
ch , γh ) ≤ PGh (·, · | ch ) − PM,c
ch ) ,
h (·, · |b

(7.2)

1

1

1

where we recall that b
ch := Compressh (ch ), with {Compressh }h∈[H+1] from M.
The proof mainly relies on our construction for M in Definition 8, and the details can be found
in Section D.4.

17

Step 2: Compressing common information using finite-memory truncation. Now it remains to
design the compression functions {Compressh }h∈[H+1] and define the associated approximate bebh → ∆(S × Ph ) for h ∈ [H]. Specifically, the information structure satisfies ch =
liefs PM,c
: C
h
{a1:h−1 , o1:h−1 }, pi,h = {oi,h }, zh+1 = {oh , ah }. More importantly, the ground-truth belief can be computed as PGh (sh , ph | ch ) = bh (a1:h−1 , o1:h−1 )(sh )Oh (oh | sh ), where bh denotes the posterior state distribution given (a1:h−1 , o1:h−1 ) (cf. Definition 11). Fix an integer L > 0, we construct the compression of ch
as b
ch = {ah−L:h−1 , oh−L+1:h−1 }. The approximate belief can be defined similarly as above as
PM,c
ch ) = b′h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh ),
h (sh , ph |b
where b′h denotes the approximate belief state, where one ignores the history before step h − L and
performs the belief update via Bayes rule along the trajectory after h − L from a prior distribution of
uniform distribution on the state (cf. Definition 11). Now we are ready to verify that Definition 7 is
satisfied.
• By definition, {b
ch }h∈[H] satisfies condition (5.1).
• For any ch ∈ Ch and the corresponding b
ch constructed above:
PGh (·, · | ch ) − PM,c
ch )
h (·, · |b
1
X
=
bh (a1:h−1 , o1:h−1 )(sh )Oh (oh | sh ) − b′h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh )
sh ,oh

= bh (a1:h−1 , o1:h−1 ) − b′h (ah−L:h−1 , oh−L+1:h−1 ) .
1

To analyze such an error of using finite-memory-based approximate belief, we rely on the result
from Golowich et al. (2022b) that belief update is a contraction under Assumption 2 so that
it forgets the misspecified prior (i.e., Unif(S) in b′h (ah−L:h−1 , oh−L+1:h−1 )) at an exponential rate.
Formally, we have the following.
Lemma 5. Suppose that the POSG satisfies Assumption 2 with parameter γ. Let ϵ ≥ 0. Fix a policy
π′ ∈ ∆(Πdet ) and indices 1 ≤ h − L < h − 1 ≤ H. If L ≥ Cγ −4 log( Sϵ ) for some large enough constant C,
then the following holds
EGa1:h−1 ,o1:h ∼π′ ∥bh (a1:h−1 , o1:h−1 ) − b′h (ah−L:h−1 , oh−L+1:h−1 )∥1 ≤ ϵ.
We defer its complete version and corresponding proof details to Theorem 10. Therefore,
combining Lemma 4 and Lemma 5, conditions (5.2), (5.3) in Definition 7 are satisfied with
ϵr = ϵz = ϵ.


SH
−4
b⋆
Finally, together with Theorem 2, by choosing L = O γ log( ϵ ) , ϵe = O(ϵ/H), we proved that π
bh ≤ (AO)L and Ph ≤ O, thus proving the quasiis an ϵ-NE/CCE. Meanwhile, it is direct to see that C
polynomial time complexity via Theorem 2.
□

7.3

Proof of Theorem 4

f 1:H ).
Step 1: Decomposing transitions of M(π

f 1:H ),z
M(π

(zh+1 |b
ch , γh ) for the model
h
π
,G
f 1:H ) is equivalent to learning P 1:h−1 (zh+1 |b
f 1:H ) in Definition 9.
M(π
ch , γh ), given the definition of M(π
h

18

Learning Ph

f 1:H ),z
M(π

(zh+1 |b
ch , γh ) by enumerating all b
ch and γh is not statistically
As highlighted before, learning Ph
efficient if naively following that of Golowich et al. (2022a). To circumvent this issue, we notice
πh

,G

Ph 1:h−1 (zh+1 |b
ch , γh ) =

πh

X
ph ,ah ,oh+1 :
χh+1 (ph ,ah ,oh+1 )=zh+1

,G

Ph 1:h−1 (ph , ah , oh+1 |b
ch , γh ),

where we recall χh+1 from Assumption 1. Now, we notice the decomposition:
πh

πh

,G

πh

,G

,G

Ph 1:h−1 (ph , ah , oh+1 |b
ch , γh ) = Ph 1:h−1 (ph |b
ch )γh (ah | ph )Ph 1:h−1 (oh+1 |b
ch , ph , ah ),
Qn
where we use the shorthand notation γh (ah | ph ) :=
and note that
i=1 γi,h (ai,h | pi,h ),
πh

,G

Ph 1:h−1 (oh+1 |b
ch , ph , ah ) does not depend on γh anymore. With such a decomposition, it suffices
πh

,G

πh

,G

to learn Ph 1:h−1 (ph |b
ch ) and Ph 1:h−1 (oh+1 |b
ch , ph , ah ).
πh

,G

πh

,G

ch , ph , ah ). The
ch ) and Ph 1:h−1 (oh+1 |b
Step 2: Bounding the statistical error for learning Ph 1:h−1 (ph |b
accuracy and sample complexity of learning those two conditional probabilities depend on the visitation probability of b
ch and ph under πh . Therefore, we first handle those b
ch and ph with large visitation
probability as follows.
Lemma 6. Fix δ1 , ζ1 , ζ2 , θ1 , θ2 > 0. Given the compression functions and correspondingly the b
L as per
Definition 10, suppose for all h ∈ [H], πh ∈ ∆(Πdet ) satisfies that πh b = Unif(A), Algorithm 5 with
h−L:h
sample complexity N0 = poly(A, O,
bh , 1 , 1 , 1 , 1 , log 1 ) ensures the following holds with probability at least 1 − δ1 for each
maxh Ph , maxh C
ζ1 ζ2 θ1 θ2
δ1
h ∈ [H]:
h

bh such that Pπ1:h−1 ,G (b
ch ) ≥ ζ1 , Algorithm 5 can learn
• For b
ch ∈ C
h
c 1:H )
c 1:H )
P
πh ,G
M(π
M(π
(ph |b
ch ) − Ph 1:h−1 (ph |b
ch ) ≤ θ1 .
Ph
(· |b
ch ) ∈ ∆(Ph ) so that ph Ph
h

bh × Ph × A such that Pπ1:h−1 ,G (b
ch , ph ) ≥ ζ2 , Algorithm 5 can learn
• For (b
ch , ph , ah ) ∈ C
h
c 1:H )
M(π

(· |b
ch , ph , ah ) ∈ ∆(O) such that
h
c 1:H )
π1:h−1
,G
M(π
(oh+1 |b
ch , ph , ah ) ≤ θ2 .
(o
|b
c
,
p
,
a
)
−
P
P
h+1
h
h
h
oh+1 h
h

Ph
P

We refer to the joint of the two bullets above as event E1 .
Proof. We prove the first item, where the second one can be proved similarly. Note that for any
πh

,G

trajectory k of Algorithm 5, the distribution of phk conditioned on b
chk is exactly Ph 1:h−1 (phk |b
chk ).
πh

,G

bh such that P 1:h−1 (b
Now consider any b
ch ∈ C
ch ) ≥ ζ1 . By the Chernoff bound, with probability at
h
ζ1 N0
ζ1 N0
least 1 − exp(− 8 ), there are at least 2 trajectories indexed by the set K1 ⊆ [N0 ], such that for
k
any k ∈ K1 , Compressh (fh (ak1:h−1 , o1:h
)) = b
ch , where we recall the definition of fh in Section 2.1. By the
folklore theorem of learning a discrete probability distribution (Canonne, 2020), with probability at
c 1:H )
P
πh ,G
M(π
least 1 − p′ , ph Ph
(ph |b
ch ) − Ph 1:h−1 (ph |b
ch ) ≤ θ1 holds as long as
1

ζ1 N0 C(Ph + log p′ )
≥
,
2
θ12
19

(7.3)

bh , the first item holds
for some constant C > 1. By a union bound over all possible h ∈ [H] and b
ch ∈ C
ζ1 N0
δ1
′
′
b
b
with probability at least 1 − H maxh Ch exp(− 8 ) − H maxh Ch p . Now set p =
b . It is direct to
4H maxh Ch

verify that Equation (D.16) holds if N0 ≥

C(maxh Ph +log
ζ1 θ12

b
4H maxh C
h)
δ1

. Furthermore, as long as C is sufficiently

bh exp(− ζ1 N0 ) ≤ δ1 . Therefore, we proved that with probability at least
large, we have that H maxh C
8
4
h

bh such that Pπ1:h−1 ,G (b
1 − δ21 , the first item holds for all h ∈ [H] and b
ch ∈ C
ch ) ≥ ζ1 .
h
c 1:H ) w.r.t. M(π
f 1:H ). To begin with, with the help
Step 3: Bounding the approximation error of M(π
h
bh such that Pπ1:h−1 ,G (b
of Lemma 6, we are able to handle those b
ch ∈ C
ch ) ≥ ζ1 as follows.
h

Lemma 7. Given policies π1:H such that πh satisfies the same condition as in Lemma 6, under the event E1
bh
in Lemma 6, then for any h ∈ [H], policy π ∈ ∆(Πdet ), and prescription γh ∈ Γh , it holds that for any b
ch ∈ C
πh

ch ) ≥ ζ1
with Ph 1:h−1 (b
X

f 1:H ),z
M(π

Ph

c 1:H ),z
M(π

(zh+1 |b
ch , γh ) − P h

zh+1

ζ
(zh+1 |b
ch , γh ) ≤ θ1 + 2APh 2 + APh θ2 .
ζ1

Proof. After some algebra, we can bound
X

PM
c h , γh ) − P M
c h , γh )
h (ph , ah , oh+1 |b
h (ph , ah , oh+1 |b
f

c

ph ,ah ,oh+1
πh

,G

≤ Ph 1:h−1 (· |b
ch ) − PM
ch )
h (· |b
1
|
{z
}
c

Term I
πh

X

+
πh

,G

,G

Ph 1:h−1 (ph |b
ch )

X

ζ

ph :Ph 1:h−1 (ph |b
ch )≥ ζ2

πh

+
πh

,G

ζ
1

ph :Ph 1:h−1 (ph |b
ch )≤ ζ2

c

Term II

1

X

,G

Ph 1:h−1 (· |b
ch , ph , ah ) − PM
ch , ph , a h )
h (· |b
1
ah |
{z
}

X πh ,G
πh
,G
c
ch )
Ph 1:h−1 (ph |b
Ph 1:h−1 (· |b
ch , ph , ah ) − PM
ch , ph , a h ) ,
h (· |b
1
ah |
{z
}
Term III

where under the event E1 , Term I can be bounded by the first item of Lemma 6. For Term II,
πh

πh

,G

,G

ch , ph ) ≥ ζ2 together with the pre-condition that
since Ph 1:h−1 (ph |b
ch ) ≥ ζζ2 , it implies that Ph 1:h−1 (b
πh

1

,G

Ph 1:h−1 (b
ch ) ≥ ζ1 . This allows us to apply the second item of Lemma 6. For Term III, we directly
bound it by 2. Combining them together, we can conclude
X

ch , γh ) − PM
ch , γh ) ≤ θ1 + 2APh
PM
h (ph , ah , oh+1 |b
h (ph , ah , oh+1 |b
f

c

ph ,ah ,oh+1

ζ2
+ APh θ2 .
ζ1

Noticing that after marginalization, the total variation distance will not increase, we proved our
lemma.
πh

Until now, we have handled those b
ch such that Ph 1:h−1 (b
ch ) ≥ ζ1 . For those less visited b
ch , we relate
h
π
,G
it to certain less-explored states at step h − b
L, specifically sh−L such that P
(sh−L ) ≤ φ as follows.
h−L

20

Lemma 8. Given compression functions {Compressh }h∈[H+1] and compute the associated b
L > 0 as in Defi′
det
nition 10. Fix any ζ > 0, φ > 0, h ∈ [H]. Consider any policies π, π ∈ ∆(Π ), such that πh b = Unif(A).
h−L:h
Then, we have
X

Pπ,G
ch ) ≤
h (b

′

b
ch :Pπh ,G (b
ch )≤ζ

A2L OL ζ
+ 1[h > b
L] ·
φ
b b

X
′

Pπ,Gb(sh−bL ).
h−L

,G
(sh−bL )≤φ
sh−bL :Pπ b
h−L

This lemma bounds the probability of less-visited b
ch with that of certain less-visited state sh−bL , for
which we can leverage existing techniques from single-agent RL to minimize by learning a certain
exploratory policy π later in Section 7.4.
c 1:H )). By a triangle inequality, we have
Finally, we are ready to evaluate ϵz (M(π


c 1:H ) ≤ ϵz (M(π
f 1:H ))
ϵz M(π
 h
 f 1:H
c 1:H ),z
π
M(π ),z
M(π
(· |b
ch , γ h ) − P h
(· |b
ch , γh )
ch ) ≥ ζ 1 P h
+
max
EGπ 1 Ph 1:h−1 (b
det
1
h,π∈Π ,γh ∈Γh
|
{z
}
Term I

 f 1:H
 h
c 1:H ),z
π
M(π ),z
M(π
ch ) ≤ ζ1 Ph
(· |b
c h , γh ) − P h
(· |b
ch , γh ) ,
+
max
EGπ 1 Ph 1:h−1 (b
det
1
h,π∈Π ,γh ∈Γh
|
{z
}
Term II

where Term I can be bounded by Lemma 7, and Term II can be bounded by 2·

P

πh

,G

b
ch :Ph 1:h−1 (b
ch )≤ζ

Pπ,G
ch ),
h (b

which can be further bounded by Lemma 8. It is direct to see that Term I and Term II together conc 1:H )) can be evaluated similarly. Now
tribute to the error ϵapx (π1:H ) defined in Theorem 4. ϵr (M(π
c 1:H ). Meanwith the help of Theorem 2, we proved the optimality in Theorem 4 for planning in M(π
while, the sample complexity is H ×N0 , thus proving the sample complexity guarantee in Theorem 4.
□

7.4

Proof of Theorem 5

Note that Theorem 4 characterizes the sample complexity for learning an equilibrium for G from
f 1:H ) with approximation errors depending on π1:H . Therefore, to obtain the fithe model M(π
nal guarantee, one needs to find certain policies π1:H to control the corresponding errors in Thef 1:H )), ϵz (M(π
f 1:H )), ϵapx (π1:H ). Note that we have evaluated ϵapx (π1:H ) above. For
orem 4, i.e., ϵr (M(π
f 1:H )), ϵz (M(π
f 1:H )), similar to the proof for Theorem 3, we take the one-step delayed sharing
ϵr (M(π
case as an example.
f 1:H )) and ϵr (M(π
f 1:H )). We also use the finite-memory truncation as the
Step 1: Evaluating ϵz (M(π
compression as before. For any π1:H , it is direct to verify that
f 1:H ),c
M(π

Ph

h

h

,G
(sh , ph |b
ch ) = P π
(sh , ph |b
ch ) = e
bπ
h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh ),
h

h

where e
bπ
h denotes the approximate belief state, where one ignores the history before step h − L and
h

,G
performs the belief update using the Bayes rule after it, from the prior distribution of Pπ
h−L (sh−L ) (cf.

21

1
Definition 11). If L ≥ Cγ −4 log( ϵφ
), it holds that

f 1:H )) = max max EG PG (· | ch , γh ) − PM,z (· |b
ch , γh )
ϵz (M(π
π
h
h
f

h

π∈Πdet ,γh

1

h

≤ max max EGπ bh (a1:h−1 , o1:h−1 ) − e
bπ
h (ah−L:h−1 , oh−L+1:h−1 ) 1
h π∈Πdet ,γh
X
Pπ,G
≤ ϵ + max max 1[h > L] · 6 ·
h−L (sh−L ),
h

π∈Πdet

h

sh−L :Pπh−L,G (sh−L )≤φ

f 1:H )) can be evaluated
where the last step can be proved similarly as Lemma 5. Moreover, ϵr (M(π
similarly.
Step 2: Minimizing the visitation probability of less-explored states with Barycentric Spanner.
f 1:H )), ϵz (M(π
f 1:H )), ϵapx (π1:H ) simultaneously, it suffices to conNow we can see that to control ϵr (M(π
P
h
trol the quantity,
Pπ,G
πh ,G
h−L (sh−L ). In other words, π should be exploratory enough in the
sh−L :Ph−L (sh−L )≤φ

sense that the actual states should be visited often enough. It turns out that finding such exploratory
policies to minimize this error term can be achieved by the Barycentric-spanner-based techniques
(Awerbuch and Kleinberg, 2008), as also adopted by Golowich et al. (2022a), using quasi-polynomial
sample and computational complexities. By choosing the parameters ζ1 , ζ2 , θ1 , θ2 , and φ properly,
we proved Theorem 5.
□

7.5

Proof Outline of Theorem 6

Correctness of the algorithmic framework. The correctness of our framework follows similarly
b⋆ is an optimal policy of M and Lemma 3,
from the proof of Theorem 2. Combining the fact that π
b⋆ is also an approximate optimal policy of G.
π
Computation analysis. As we mentioned in Section 6, the key of extending our framework to teamoptimum-finding in Dec-POMDPs is to implement Equation (6.1) in a computationally tractable
way for each h ∈ [H]. Here we briefly outline how each of the three assumptions can circumvent
the hardness in Proposition 2. Condition 1. In Proposition 11, we show that, the Q-value funcP
tion can be linearly decomposed into n functions, i.e., Qh⋆,M (b
ch , γh ) = j∈[n] Uj,h (b
ch , γj,h ), for some
b
functions {Uj,h }j∈[n] , for any b
ch ∈ Ch , γh ∈ Γh . Therefore, Equation (6.1) can be solved tractably since
each Uj,h is indeed a linear function of γj,h with the concatenation of simplexes being the constraint.
Condition 2. With such a nested structure, Equation (6.1) can indeed be solved by a dynamic prob(n) with the horizon length being
gramming over the agents. We consider the following POMDP P
the number of the agents n. The initial state x1 = (sh , ph ) ∼ PM,c
ch ). At each step j ∈ [n]
h (sh , ph |b
th
of this POMDP, the observation is yj = pj,h , the j agent takes the action aj ∈ Aj , and the next
state transitions to xj+1 = (xj , aj ). Note that the reward is non-zero only at the last step n, where
⋆,M
b
rn (xn , an ) = Esh+1 ∼Th (· | sh ,a1:n ),oh+1 ∼Oh+1 (· | sh+1 ) [rh (sh , a1:n ) + Vh+1
(b
ch+1 )]. Based on such a POMDP perspective, we can develop an efficient algorithm for Equation (6.1) (cf. Algorithm 10), where the first
b(n) constructed above to
for-loop is a standard backward procedure of value iteration for the POMDP P
⋆
⋆
compute its optimal policy u1:n . The second for-loop performs a forward procedure of translating u1:n
⋆
⋆
into γ1:n,h
, where γi,h
∈ Γi,h for each i ∈ [n] now belongs to the prescription space we hope to optimize
over in Equation (6.1). Note that throughout, we regard the number of agents n, i.e., the time horizon
b(n) as a constant. Hence, the time complexity of such a dynamic programming for finding the
of P
b(n) is indeed poly(S, A, Ph ). Condition 3. Due to the factorized structures,
exact optimal policy of P
22

Episode Rewards Episode Rewards

Spread

120

IPPO d = 0
IPPO d = 1
IPPO d = 2
IPPO d = 5
IPPO d =

140
150
160
170
180

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00
1e7

120

10

IPPO d = 0
IPPO d = 1
IPPO d = 2
IPPO d = 5
IPPO d =

15
20
25
30

MAPPO d = 0
MAPPO d = 1
MAPPO d = 2
MAPPO d = 5
MAPPO d =

140
150
160
170
0.00

0.25

0.50

Timesteps

0.75

1.00

1.25

1.50

1.75

2.00
1e7

0.0

0.5

1.0

1.5

2.0

2.5

3.0
1e6

IPPO d = 0
IPPO d = 1
IPPO d = 2
IPPO d = 5
IPPO d =

15
20
25
30
35
40

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00
1e6

5
10

10

MAPPO d = 0
MAPPO d = 1
MAPPO d = 2
MAPPO d = 5
MAPPO d =

15
20
25
30

Comm

5
10

5

130

180

Reference

5

130

0.0

0.5

1.0

Timesteps
1.5

2.0

2.5

3.0
1e6

MAPPO d = 0
MAPPO d = 1
MAPPO d = 2
MAPPO d = 5
MAPPO d =

15
20
25
30
35
40

0.00

0.25

0.50

Timesteps

0.75

1.00

1.25

1.50

1.75

2.00
1e6

Figure 1: Performance of MAPPO and IPPO in various delayed-sharing settings.

Horizon
3
4
5
6
7

Boxpushing
Ours
FM-E
RNN-E
62.78
64.22
8.40
81.44
77.80
9.10
98.73
96.40
21.78
98.76
94.61
94.36
145.35 138.44 132.70

Ours
13.06
20.89
27.95
36.03
37.72

Dectiger
FM-E RNN-E
-6.0
-6.0
-4.76 -7.00
-6.37 -10.04
-7.99 -11.90
-7.99 -13.92

Table 1: Final evaluation of the rewards using our methods, compared with using the methods of
FM-E and RNN-E in Mao et al. (2020).
in Proposition 13, we show that the Q-value can be also decoupled into n terms, such that there exist
P
n functions {Fi,h }i∈[n] such that Qh⋆,M (b
ch , {γi,h }i∈[n] ) = i∈[n] Fi,h (b
ci,h , γi,h ). Therefore, the maximization
over the joint {γi,h }i∈[n] in Equation (6.1) is equivalent to the individual maximization over each γi,h
for Fi,h , i ∈ [n], which is again a linear program as we argued before. Thus, Equation (6.1) can be also
solved with time complexity poly(S, A, Ph ).
□

8

Experimental Results

For the experiments, we will both investigate the benefits of information sharing as we considered in
various empirical MARL environments, and validate the implementability and performance of our
proposed approaches on several odest-scale examples.
Information sharing improves performance. We mainly consider three cooperative tasks, the
physical deception (Spread), the simple reference (Reference), and the cooperative communication (Comm)
in the popular deep MARL benchmarks, multi-agent particle-world environment (MPE) (Lowe et al.,
2017). We train both the popular centralized-training algorithm MAPPO (Yu et al., 2021) and the
decentralized-training algorithm IPPO (Yu et al., 2021) with different information-sharing mechanisms by varying the delay from 0 to ∞. The rewards during training are shown in Figure 1. It is
seen that in all domains (except MAPPO on Spread) with either training paradigms, smaller delays,
which correspond to the case of more information sharing, will lead to faster convergence, higher
final performance, and reduced training variance.

23

Validating implementability and performance. To further validate the tractability of our approaches, we test our learning algorithm on two popular and modest-scale partially observable
benchmarks Dectiger (Nair et al., 2003) and Boxpushing (Seuken and Zilberstein, 2012). We compare our approaches with FM-E and RNN-E, which are also common information-based approaches
developed in Mao et al. (2020). The final rewards are reported in Table 1. In both domains with
various horizons, our methods consistently outperform the baselines.

9

Concluding Remarks

In this paper, we studied provable multi-agent RL in partially observable environments, with both
statistical and computational (quasi-)efficiencies. The key to our results is to identify the value of
information sharing, a common practice in empirical MARL and a standard phenomenon in many
multi-agent control systems, in algorithm design and computation/sample efficiency analysis. We
hope our study may open up the possibilities of leveraging and even designing different information
structures, for developing both statistically and computationally efficient partially observable MARL
algorithms. One open problem and future direction is to develop a fully decentralized algorithm and
overcome the curse of multiagents, such that the sample and computation complexities do not grow
exponentially with the number of agents. Another interesting direction is to identify the combination of certain information-sharing structures and observability assumptions for more efficient (e.g.,
polynomial) sample and computation complexity results.

Acknowledgement
The authors would like to thank the anonymous reviewers from ICML 2023 for their helpful comments. The authors would also like to thank Noah Golowich and Serdar Yüksel for the valuable
feedback and discussions. X.L and K.Z. acknowledge the support from Simons-Berkeley Research
Fellowship, Northrop Grumman – Maryland Seed Grant Program, Army Research Office (ARO) grant
W911NF-24-1-0085, NSF CAREER Award 2443704, and AFOSR YIP Award FA9550-25-1-0258.

References
Altabaa, A. and Yang, Z. (2024). On the role of information structure in reinforcement learning for
partially-observable sequential teams and games. arXiv preprint arXiv:2403.00993.
Altman, E., Kambley, V. and Silva, A. (2009). Stochastic games with one step delay sharing information pattern with application to power control. In 2009 International Conference on Game Theory
for Networks. IEEE.
Aumann, R. J., Maschler, M. and Stearns, R. E. (1995). Repeated games with incomplete information.
MIT press.
Awerbuch, B. and Kleinberg, R. (2008). Online linear optimization and adaptive routing. Journal of
Computer and System Sciences, 74 97–114.
Azizzadenesheli, K., Lazaric, A. and Anandkumar, A. (2016). Reinforcement learning of POMDPs
using spectral methods. In Conference on Learning Theory. PMLR.
Bai, Y. and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning. In
International Conference on Machine Learning.

24

Bai, Y., Jin, C. and Yu, T. (2020). Near-optimal reinforcement learning with self-play. Advances in
Neural Information Processing Systems, 33.
Behn, R. and Ho, Y.-C. (1968). On a class of linear stochastic differential games. IEEE Transactions
on Automatic Control, 13 227–240.
Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q.,
Hashme, S., Hesse, C. et al. (2019). Dota 2 with large scale deep reinforcement learning. arXiv
preprint arXiv:1912.06680.
Bernstein, D. S., Givan, R., Immerman, N. and Zilberstein, S. (2002). The complexity of decentralized control of markov decision processes. Mathematics of operations research, 27 819–840.
Blum, A. and Mansour, Y. (2007). Learning, regret minimization, and equilibria. Algorithmic Game
Theory 79–102.
Busoniu, L., Babuska, R., De Schutter, B. et al. (2008). A comprehensive survey of multiagent
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C, 38 156–172.
Cai, Y., Liu, X., Oikonomou, A. and Zhang, K. (2024). Provable partially observable reinforcement
learning with privileged information. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems.
Canonne, C. L. (2020).
arXiv:2002.11457.

A short note on learning discrete distributions.

arXiv preprint

Chen, X., Deng, X. and Teng, S.-H. (2009). Settling the complexity of computing two-player Nash
equilibria. Journal of the ACM, 56 14.
Chen, Z., Zhang, K., Mazumdar, E., Ozdaglar, A. and Wierman, A. (2023). A finite-sample
analysis of payoff-based independent learning in zero-sum stochastic games. arXiv preprint
arXiv:2303.03100.
Daskalakis, C., Deckelbaum, A. and Kim, A. (2011). Near-optimal no-regret algorithms for zero-sum
games. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms.
SIAM.
Daskalakis, C., Foster, D. J. and Golowich, N. (2020). Independent policy gradient methods for
competitive reinforcement learning. In Advances in Neural Information Processing Systems.
Daskalakis, C., Goldberg, P. W. and Papadimitriou, C. H. (2009). The complexity of computing a
Nash equilibrium. SIAM Journal on Computing, 39 195–259.
Daskalakis, C., Golowich, N. and Zhang, K. (2022). The complexity of Markov equilibrium in
stochastic games. arXiv preprint arXiv:2204.03991.
Ding, D., Wei, C.-Y., Zhang, K. and Jovanovic, M. (2022). Independent policy gradient for large-scale
markov potential games: Sharper rates, function approximation, and game-agnostic convergence.
In International Conference on Machine Learning. PMLR.
Efroni, Y., Jin, C., Krishnamurthy, A. and Miryoosefi, S. (2022). Provable reinforcement learning
with a short-term memory. arXiv preprint arXiv:2202.03983.

25

Emery-Montemerlo, R., Gordon, G., Schneider, J. and Thrun, S. (2004). Approximate solutions for
partially observable stochastic games with common payoffs. In Proceedings of the Third International
Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004. IEEE.
Even-Dar, E., Kakade, S. M. and Mansour, Y. (2007). The value of observation for monitoring
dynamic systems. In IJCAI.
Farina, G., Anagnostides, I., Luo, H., Lee, C.-W., Kroer, C. and Sandholm, T. (2022). Near-optimal
no-regret learning dynamics for general convex games. Advances in Neural Information Processing
Systems, 35 39076–39089.
Filar, J. and Vrieze, K. (2012). Competitive Markov Decision Processes. Springer Science & Business
Media.
Golowich, N., Moitra, A. and Rohatgi, D. (2022a). Learning in observable POMDPs, without computationally intractable oracles. In Advances in Neural Information Processing Systems.
Golowich, N., Moitra, A. and Rohatgi, D. (2022b). Planning in observable pomdps in quasipolynomial time. arXiv preprint arXiv:2201.04735.
Golowich, N., Moitra, A. and Rohatgi, D. (2023). Planning and learning in partially observable
systems via filter stability. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing.
Gong, S., Shen, J. and Du, L. (2016). Constrained optimization and distributed computation based
car following control of a connected and autonomous vehicle platoon. Transportation Research Part
B: Methodological, 94 314–334.
Gordon, G. J., Greenwald, A. and Marks, C. (2008). No-regret learning in convex games. In Proceedings of the 25th international conference on Machine learning.
Gupta, A., Nayyar, A., Langbort, C. and Basar, T. (2014). Common information based Markov
perfect equilibria for linear-Gaussian games with asymmetric information. SIAM Journal on Control
and Optimization, 52 3228–3260.
Hansen, E. A., Bernstein, D. S. and Zilberstein, S. (2004). Dynamic programming for partially
observable stochastic games. In AAAI, vol. 4.
Hernandez-Leal, P., Kartal, B. and Taylor, M. E. (2019). A survey and critique of multiagent deep
reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33 750–797.
Horák, K., Bošanskỳ, B. and Pěchouček, M. (2017). Heuristic search value iteration for one-sided
partially observable stochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 31.
Jin, C., Kakade, S., Krishnamurthy, A. and Liu, Q. (2020). Sample-efficient reinforcement learning
of undercomplete POMDPs. Advances in Neural Information Processing Systems, 33 18530–18539.
Jin, C., Liu, Q., Wang, Y. and Yu, T. (2024). V-learning – a simple, efficient, decentralized algorithm
for multiagent reinforcement learning. Mathematics of Operations Research, 49 2295–2322.
Kao, H. and Subramanian, V. (2022). Common information based approximate state representations in multi-agent reinforcement learning. In International Conference on Artificial Intelligence
and Statistics. PMLR.
26

Kao, H., Wei, C.-Y. and Subramanian, V. (2022). Decentralized cooperative reinforcement learning
with hierarchical information structure. In International Conference on Algorithmic Learning Theory.
PMLR.
Kara, A. D. (2022). Near optimality of finite memory feedback policies in partially observed Markov
decision processes. The Journal of Machine Learning Research, 23 437–482.
Kara, A. D. and Yüksel, S. (2022). Convergence of finite memory Q learning for POMDPs and near
optimality of learned policies under filter stability. Mathematics of Operations Research.
Kozuno, T., Ménard, P., Munos, R. and Valko, M. (2021). Learning in two-player zero-sum partially
observable Markov games with perfect recall. Advances in Neural Information Processing Systems,
34 11987–11998.
Krishnamurthy, A., Agarwal, A. and Langford, J. (2016). Pac reinforcement learning with rich
observations. Advances in Neural Information Processing Systems, 29.
Kumar, A. and Zilberstein, S. (2009). Dynamic programming approximations for partially observable stochastic games.
Leonardos, S., Overman, W., Panageas, I. and Piliouras, G. (2022). Global convergence of multiagent policy gradient in Markov potential games. In International Conference on Learning Representations.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra,
D. (2016). Continuous control with deep reinforcement learning. In International Conference on
Learning Representations.
Liu, Q., Chung, A., Szepesvari, C. and Jin, C. (2022a). When is partially observable reinforcement
learning not scary? In Conference on Learning Theory.
Liu, Q., Szepesvári, C. and Jin, C. (2022b). Sample-efficient reinforcement learning of partially
observable Markov games. In Advances in Neural Information Processing Systems.
Liu, Q., Yu, T., Bai, Y. and Jin, C. (2021). A sharp analysis of model-based reinforcement learning
with self-play. In International Conference on Machine Learning. PMLR.
Liu, X. and Zhang, K. (2023). Partially observable multi-agent rl with (quasi-) efficiency: the blessing
of information sharing. In International Conference on Machine Learning. PMLR.
Long, P., Fan, T., Liao, X., Liu, W., Zhang, H. and Pan, J. (2018). Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning. In 2018 IEEE International
Conference on Robotics and Automation (ICRA). IEEE.
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P. and Mordatch, I. (2017). Multi-agent actor-critic
for mixed cooperative-competitive environments. In Advances in Neural Information Processing
Systems.
Lusena, C., Goldsmith, J. and Mundhenk, M. (2001). Nonapproximability results for partially observable markov decision processes. Journal of artificial intelligence research, 14 83–103.
Mao, W., Yang, L., Zhang, K. and Basar, T. (2022). On improving model-free algorithms for decentralized multi-agent reinforcement learning. In International Conference on Machine Learning.
PMLR.
27

Mao, W., Zhang, K., Miehling, E. and Başar, T. (2020). Information state embedding in partially observable cooperative multi-agent reinforcement learning. In 2020 59th IEEE Conference on Decision
and Control (CDC). IEEE.
Milgrom, P. and Roberts, J. (1987). Informational asymmetries, strategic behavior, and industrial
organization. The American Economic Review, 77 184–193.
Mundhenk, M., Goldsmith, J., Lusena, C. and Allender, E. (2000). Complexity of finite-horizon
Markov decision process problems. Journal of the ACM (JACM), 47 681–720.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. and Marsella, S. (2003). Taming decentralized
pomdps: Towards efficient policy computation for multiagent settings. In IJCAI, vol. 3.
Nayyar, A., Gupta, A., Langbort, C. and Başar, T. (2013a). Common information based Markov perfect equilibria for stochastic games with asymmetric information: Finite games. IEEE Transactions
on Automatic Control, 59 555–570.
Nayyar, A., Mahajan, A. and Teneketzis, D. (2010). Optimal control strategies in delayed sharing
information structures. IEEE Transactions on Automatic Control, 56 1606–1620.
Nayyar, A., Mahajan, A. and Teneketzis, D. (2013b). Decentralized stochastic control with partial
history sharing: A common information approach. IEEE Transactions on Automatic Control, 58
1644–1658.
Ouyang, Y., Tavafoghi, H. and Teneketzis, D. (2016). Dynamic games with asymmetric information: Common information based perfect Bayesian equilibria and sequential decomposition. IEEE
Transactions on Automatic Control, 62 222–237.
Papadimitriou, C. H. and Tsitsiklis, J. N. (1987). The complexity of markov decision processes.
Mathematics of operations research, 12 441–450.
Pathak, A., Pucha, H., Zhang, Y., Hu, Y. C. and Mao, Z. M. (2008). A measurement study of internet
delay asymmetry. In Passive and Active Network Measurement: 9th International Conference, PAM
2008, Cleveland, OH, USA, April 29-30, 2008. Proceedings 9. Springer.
Peralez, J., Delage, A., Buffet, O. and Dibangoye, J. S. (2024). Solving hierarchical informationsharing dec-pomdps: An extensive-form game approach. arXiv preprint arXiv:2402.02954.
Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J. and Whiteson, S. (2020).
Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal
of Machine Learning Research, 21 7234–7284.
Roughgarden, T. (2010). Algorithmic game theory. Communications of the ACM, 53 78–86.
Sallab, A. E., Abdou, M., Perot, E. and Yogamani, S. (2017). Deep reinforcement learning framework
for autonomous driving. Electronic Imaging, 2017 70–76.
Seuken, S. and Zilberstein, S. (2012). Improved memory-bounded dynamic programming for decentralized pomdps. arXiv preprint arXiv:1206.5295.
Shalev-Shwartz, S., Shammah, S. and Shashua, A. (2016). Safe, multi-agent, reinforcement learning
for autonomous driving. arXiv preprint arXiv:1610.03295.
Shapley, L. S. (1953). Stochastic games. Proceedings of the National Academy of Sciences, 39 1095–1100.
28

Shi, J., Wang, G. and Xiong, J. (2016). Leader–follower stochastic differential game with asymmetric
information and applications. Automatica, 63 60–73.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,
L., Lai, M., Bolton, A. et al. (2017). Mastering the game of Go without human knowledge. Nature,
550 354–359.
Sinha, A. and Mahajan, A. (2023). Asymmetric actor-critic with approximate information state. In
2023 62nd IEEE Conference on Decision and Control (CDC). IEEE.
Song, Z., Mei, S. and Bai, Y. (2021). When can we learn general-sum markov games with a large
number of players sample-efficiently? arXiv preprint arXiv:2110.04184.
Subramanian, J., Sinha, A., Seraj, R. and Mahajan, A. (2022). Approximate information state for
approximate planning and reinforcement learning in partially observed systems. J. Mach. Learn.
Res., 23 12–1.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M.,
Sonnerat, N., Leibo, J. Z., Tuyls, K. et al. (2018). Value-decomposition networks for cooperative
multi-agent learning based on team reward. In International Conference on Autonomous Agents and
Multi-Agent Systems.
Tang, D., Subramanian, V. and Teneketzis, D. (2024). Information compression in dynamic games.
arXiv preprint arXiv:2407.12318.
Tsitsiklis, J. and Athans, M. (1985). On the complexity of decentralized decision making and detection problems. IEEE Transactions on Automatic Control, 30 440–446.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H.,
Powell, R., Ewalds, T., Georgiev, P. et al. (2019). Grandmaster level in StarCraft II using multiagent reinforcement learning. Nature, 575 350–354.
Wei, C.-Y., Lee, C.-W., Zhang, M. and Luo, H. (2021). Last-iterate convergence of decentralized
optimistic gradient descent/ascent in infinite-horizon competitive Markov games. arXiv preprint
arXiv:2102.04540.
Witsenhausen, H. S. (1968). A counterexample in stochastic optimum control. SIAM Journal on
Control, 6 131–147.
Witsenhausen, H. S. (1971). Separation of estimation and control for discrete time systems. Proceedings of the IEEE, 59 1557–1566.
Xie, Q., Chen, Y., Wang, Z. and Yang, Z. (2020). Learning zero-sum simultaneous-move markov
games using function approximation and correlated equilibrium. In Conference on learning theory.
PMLR.
Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A. and Wu, Y. (2021). The surprising effectiveness of
ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955.
Zhang, K., Kakade, S. M., Başar, T. and Yang, L. F. (2020). Model-based multi-agent RL in zero-sum
Markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461.
Zhang, K., Yang, Z. and Başar, T. (2021a). Multi-agent reinforcement learning: A selective overview
of theories and algorithms. Handbook of Reinforcement Learning and Control 321–384.
29

Zhang, K., Zhang, X., Hu, B. and Basar, T. (2021b). Derivative-free policy optimization for linear
risk-sensitive and robust control design: Implicit regularization and sample complexity. Advances
in Neural Information Processing Systems, 34 2949–2964.
Zhang, R., Ren, Z. and Li, N. (2021c). Gradient play in stochastic games: stationary points, convergence, and sample complexity. arXiv preprint arXiv:2106.00198.
Zinkevich, M., Johanson, M., Bowling, M. and Piccione, C. (2007). Regret minimization in games
with incomplete information. In Advances in Neural Information Processing Systems.

30

Appendices

A

Additional Definitions

A.1

Belief states

In partially observable environments, each agent cannot know the underlying state but could infer
the underlying distribution of states through the observations and actions. Following the convention
in POMDPs, we call such distributions the belief states. Such posterior distributions over states can
be updated whenever the agent receives new observations and actions. Formally, we define the belief
update as:
Definition 11 (Belief state update). For each h ∈ [H + 1], the Bayes operator (with respect to the joint
observation) Bh : ∆(S) × O → ∆(S) is defined for b ∈ ∆(S), and y ∈ O by:
Bh (b; y)(x) = P

Oh (y | x)b(x)
.
z∈S Oh (y | z)b(z)

Similarly, for each h ∈ [H], i ∈ [n], we define the Bayes operator with respect to individual observations
Bi,h : ∆(S) × Oi → ∆(S) by:
Oi,h (y | x)b(x)
.
Bi,h (b; y)(x) = P
z∈S Oi,h (y | z)b(z)
For each h ∈ [H], the belief update operator Uh : ∆(S) × A × O → ∆(S), is defined by
Uh (b; a, y) = Bh+1 (Th (a) · b; y) ,
where Th (a) · b represents the matrix multiplication. We use the notation bh to denote the belief update
function, which receives a sequence of actions and observations and outputs a distribution over states at the
step h. The belief state at step h = 1 is defined as b1 (∅) = µ1 . For any 1 ≤ h ≤ H and any action-observation
sequence (a1:h−1 , o1:h ), we inductively define the belief state:
bh+1 (a1:h , o1:h ) = Th (ah ) · bh (a1:h−1 , o1:h ),
bh (a1:h−1 , o1:h ) = Bh (bh (a1:h−1 , o1:h−1 ); oh ).
Also, we slightly abuse the notation and define the belief state containing individual observations as
bh (a1:h−1 , o1:h−1 , oi,h ) = Bi,h (bh (a1:h−1 , o1:h−1 ); oi,h ).
We define the approximate belief update using the most recent L-step history. For 1 ≤ h ≤ H, we follow the
notation of Golowich et al. (2022b) and define



apx,G
µ1 if h = 1
bh
(∅; D) = 

D
otherwise ,
where D ∈ ∆(S) is the prior for the approximate belief update. Then for any 1 ≤ h − L < h ≤ H and any
action-observation sequence (ah−L:h−1 , oh−L+1:h ), we inductively define
apx,G

apx,G

bh+1 (ah−L:h , oh−L+1:h ; D) = Th (ah ) · bh
apx,G

bh

apx,G

(ah−L:h−1 , oh−L+1:h ; D) = Bh (bh

(ah−L:h−1 , oh−L+1:h ; D),

(ah−L:h−1 , oh−L+1:h−1 ; D); oh ).

For the remainder of our paper, we shall use the important initialization for the approximate belief, which
apx,G
(·; Unif(S)).
are defined as b′h (·) := bh
31

A.2

Additional definitions of value functions and policies

In Definition 1, we have defined value functions in G. Similar to the fully-observable settings (MDPs
and stochastic games), we can also extend such a definition to the prescription-value function, which
corresponds to the action-value function in the fully-observable settings.
Definition 12 (Prescription-value function with information sharing). At step h ∈ [H], given the common information ch , joint policies π = {πi }ni=1 ∈ Π, and prescriptions {γi,h }ni=1 ∈ Γh , the prescription-value
function conditioned on the common information and joint prescription of the i th agent is defined as:
h
i
π,G
π,G
Qi,h
(ch , {γj,h }j∈[n] ) := EGπ ri,h (sh , ah ) + Vi,h+1
(ch+1 ) ch , {γj,h }j∈[n] ,
where prescription γi,h ∈ Γi,h replaces the partial function πi,h (· | ωi,h , ch , ·) in the value function.
With the expected approximate common information model M given in Definition 7, we can
define the value function and policy under M accordingly as follows.
Definition 13 (Value function and policy under M). Given an expected approximate common information model M, for any policy π ∈ Π, for each i ∈ [n], h ∈ [H], we define the value function as


π,M
π,M
M
Vi,h
(ch ) = E{ωj,h }j∈[n] b
(b
ch , {πj,h (· | ωj,h , ch , ·)}j∈[n] ) + EM [Vi,h+1
(ch+1 ) |b
ch , {πj,h (· | ωj,h , ch , ·)}j∈[n] ] . (A.1)
ri,h
π,M
bh →
b whose π
bi,h : Ωh × Pi,h × C
For any cH+1 ∈ CH+1 , we define Vi,H+1
(cH+1 ) = 0. Furthermore, for a policy π
∆(Ai ) takes approximate instead of the exact common information as the input, we define


b,M
b,M
π
π
M
M
ch+1 ) |b
ch , {b
πj,h (· | ωj,h ,b
ch , ·)}j∈[n] ] , (A.2)
ch ) = E{ωj,h }j∈[n] b
ch , {b
πj,h (· | ωj,h ,b
ch , ·)}j∈[n] ) + E [Vi,h+1 (b
Vi,h (b
ri,h (b

bH+1 , we define V πb,M (b
where similarly, for each b
cH+1 ∈ C
i,H+1 cH+1 ) = 0. With a slight abuse of notation, somebi,h may also take ch ∈ Ch as input and thus π
b ∈ Π. In this case, when M and the corresponding comtimes π
bi,h (· | ·, ch , ·) := π
bi,h (· | ·, Compressh (ch ), ·).
pression function Compressh are clear from the context, it means π
b,M
b,G
π
π
Accordingly, in this case, the definitions of Vi,h (ch ) and Vi,h (ch ) follows from Definition 1 and Equation
(A.1), respectively.

B

Collection of Algorithm Pseudocodes

Here we collect both our planning and learning algorithms as in Algorithms 1, 2, 3, 4, 5, 6, 7, 8, 9.

C

Full Versions of the Results

C.1

Planning

Now we state the full version of Theorem 3 regarding the instantiations of Theorem 2.
Theorem 7. Fix ϵ > 0. Suppose there exists an (ϵr , ϵz )-expected-approximate common information model
M consistent with some given approximate belief {PM,c
ch )}h∈[H] for the POSG G under Assumptions
h (sh , ph |b
b
1 and 3 such that max{ϵz (M), ϵr (M)} ≤ O(ϵ) and maxh Ch Ph is quasi-polynomial of the problem instance
size, then there exists a quasi-polynomial time algorithm that can compute an ϵ-NE if G is zero-sum or
cooperative, and an ϵ-CE/CCE if G.
In particular, under Assumption 2, examples in Section 3 satisfy all such conditions. Therefore, there
exists a quasi-polynomial time algorithm computing ϵ-NE if G is zero-sum or cooperative and ϵ-CE/CCE if
G is general-sum, with the following information-sharing structures and time complexities, where we recall
γ is the constant in Assumption 2:
32

Common information evolution:
Brute-force search
Virtual coordinator:
Common information-based
decomposition
(Nayyar et al 2013a,b)
Proposition 9

Decision making from the perspective of the virtual coordinator

Computing equilibrium in prescription space

Figure 2: An overview of our algorithmic framework. The left part of the figure shows that there is a
virtual coordinator collecting the information shared among agents. Based on the common information ch , it will compute an equilibrium in the prescription space and assign it to all the agents. The
right part shows the computation of equilibrium. Let’s take the example of Ai = 2, Pi,h = 3, Ch = 2. If
we search over all deterministic prescriptions, the corresponding matrix game will have the size of
C P
Ai h i,h = 64. Then, Nayyar et al. (2013a,b) proposed the common information-based decomposition,
and solve Ch number of games of smaller size. However, in the Dec-POMDP setting, Nayyar et al.
(2013b) treated each deterministic prescription as an action and the size of each sub-problem will
P
be Ai i,h = 8. Furthermore, Proposition 8 shows that we can reformulate each sub-problem as a game
whose payoff is multi-linear with respect to each agent’s prescription, and whose dimensionality is
Ai Pi,h = 6.
• One-step delayed information sharing: (AO)Cγ

−4

log SH
ϵ

for some universal constant C > 0.

• State controlled by one controller with asymmetric d = poly(log H)-step delayed sharing sharSH
−4
ing: (AO)C(γ log ϵ +d) for some constant C > 0.
• Information sharing with one-directional-one-step delay:
SH
−4
(AO)Cγ log ϵ for some universal constant C > 0.
• Uncontrolled state process with d = poly(log H)-step delayed sharing: (AO)C(γ
some universal constant C > 0.
• Symmetric information game: (AO)Cγ

C.2

−4

log SH
ϵ

−4

log SH
ϵ +d)

for

for some universal constant C > 0.

Learning

Here we state the full version of Theorem 4 regarding the sample efficiency of learning and approximate common information model.
Theorem 8. Suppose the POSG G satisfies Assumptions 1 and 3. Given any compression functions of
bh for h ∈ [H + 1], we can compute b
common information, Compressh : Ch → C
L as defined in Definition
1:H
10. Then, given any H policies π , where πh ∈ ∆(Πdet ), πh b = Unif(A) for h ∈ [H], we can construct
h−L:h
f 1:H ), whose compression funca policy-dependent expected approximate common information model M(π
f 1:H )) and ϵz (π1:H ) := ϵz (M(π
f 1:H )) for short.
tions are {Compress }h∈[H+1] . We write ϵr (π1:H ) := ϵr (M(π
h

Fix some parameters δ1 , θ1 , θ2 , ζ1 , ζ2 > 0 for Algorithm 5, ϵe > 0 for Algorithm 3, and φ > 0, define the

33

Algorithm 1 Value iteration with common information
1: Input: G, ϵe
2: for each i ∈ [n] and cH+1 do

⋆,G
Vi,H+1
(cH+1 ) ← 0
4: end for
5: for h = H, · · · , 1 do
6:
for each ch do
7:
Define

3:





⋆,G
⋆,G
Qi,h
(ch , γ1,h , · · · , γn,h ) := Es ,p ∼PG (·,· | c ) E{aj,h ∼γj,h (· | pj,h )}j∈[n] Eoh+1 ∼O⊤h+1 Th (· | sh ,ah ) ri,h (sh , ah ) + Vi,h+1
(ch+1 )
h h
h
h

8:

n

9:

o
⋆,G
⋆
⋆
π1,h
(· | ·, ch , ·), · · · , πn,h
(· | ·, ch , ·) ← NE/CE/CCE({Qi,h
(ch , ·, · · · , ·)}ni=1 , ϵe )

// we refer the implementation to Section D.2
for each i ∈ [n] do


⋆,G
⋆,G
⋆
(ch+1 ) | ch , {πj,h
Vi,h
(ch ) ← E{ωj,h }j∈[n] EG ri,h (sh , ah ) + Vi,h+1
(· | ωj,h , ch , ·)}j∈[n]

end for
end for
12: end for
13: return π⋆
10:
11:

Algorithm 2 BR(G, π, i, ϵe ): ϵe -approximate Best Response for the i th agent under true model G
1: Input: G, π, i, ϵe
⋆,G

2: Vi,H+1 (cH+1 ) ← 0 for all cH+1
3: for h = H, · · · , 1 do
4:
5:

for each ch do
Define


⋆,G
⋆,G
Qi,h
(ch , γ1,h , · · · , γn,h ) := Es ,p ∼PG (·,· | c ) E{aj,h ∼γj,h (· | pj,h )}j∈[n] Eoh+1 ∼O⊤h+1 Th (· | sh ,ah ) ri,h (sh , ah ) + Vi,h+1
(ch+1 )
h

h

h

h

6:
⋆,G
⋆
πi,h
(· | ·, ch , ·) ← NE/CE/CCE-BR(Qi,h
(ch , ·, · · · , ·), {πj,h (· | ·, ch , ·)}j∈[n] , i, ϵe )

// we refer the implementation to Section D.2
7:





⋆,G
⋆,G
⋆
Vi,h
(ch ) ←E{ωj,h }j∈[n] Es ,p ∼PG (·,· | c ) E ai,h ∼πi,h
r (s , ah ) + Vi,h+1
(ch+1 )
(· | ωi,h ,ch ,pi,h ), Eoh+1 ∼O⊤
h+1 Th (· | sh ,ah ) i,h h
h h
h
h
a−i,h ∼π−i,h (· | ω−i,h ,ch ,p−i,h )

end for
9: end for
10: return πi⋆
8:

34

Algorithm 3 VIACM(M, ϵe ): Value Iteration with expected Approximate Common-information Model
1: Input: M, ϵe
2: for each i ∈ [n] and b
cH+1 do
⋆,M
Vi,H+1
(b
cH+1 ) ← 0
4: end for
5: for h = H, · · · , 1 do
6:
for each b
ch do

3:

7:



⋆,M
⋆,M
M
(b
ch , γh ) + EM Vi,h+1
(b
ch+1 ) | b
ch , {γj,h }j∈[n] for any i ∈ [n]
Define Qi,h
(b
ch , γ1,h , · · · , γn,h ) := b
ri,h

8:

if computing the equilibrium then

9:

n

10:

o
⋆,M
⋆
⋆
b1,h
bn,h
π
(· | ·,b
ch , ·), · · · , π
(· | ·,b
ch , ·) ← NE/CE/CCE({Qi,h
(b
ch , ·, · · · , ·)}ni=1 , ϵe )

// we refer the implementation to Section D.2
else if computing the team-optimum then

11:

n

12:
13:
14:

o
⋆
⋆
b1,h
bn,h
π
(· |b
ch , ·), · · · , π
(· |b
ch , ·) ← arg n

max o

γi,h ∈∆(Ai )Pi,h
i∈[n]

⋆,M
(Q1,h
(b
ch , γ1,h , · · · , γn,h ))

// we refer the implementation to Section D.7
end if
for each i ∈ [n] do

⋆,M
Vi,h
(b
ch )

⋆
b
ch , {b
πj,h
(· | ωj,h ,b
ch , ·)}j∈[n] ]

←


M
⋆
E{ωj,h }j∈[n] b
(b
ch , {b
πj,h
(· | ωj,h ,b
ch , ·)}j∈[n] )
ri,h

+

⋆,M
(b
ch+1 )
EM [Vi,h+1

|

end for
end for
17: end for
b⋆
18: return π
15:
16:

f 1:H ) using samples under the policies π1:H as:
approximation error for estimating M(π
ζ
ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ) = θ1 + 2A max Ph 2
ζ1
h
+ A max Ph θ2 +
h

(C.1)

b b


A2L OL ζ1
+ max max 1[h > b
L] · 2 · d π,G b U G b(πh ) ,
S,h−L
φ,h−L
φ
h π∈Πdet
′

′

′

π′ ,G
s∈A dS,h (s) for
π′ ,G
G
any A ⊆ S, Uφ,h
(π′ ) := {s ∈ S : dS,h
(s) < φ}, representing under-explored states under the policy π′ . Then,
c 1:H ) with the sample complexity
Algorithm 5 can learn an model M(π

π ,G
,G
π ,G
where for any policy π′ ∈ ∆(Πdet ), h ∈ [H], we define dS,h
(s) := Pπ
h (sh = s), dS,h (A) :=



bh Ph )A 
bh
4H maxh (C
4H maxh C



C(max
P
+
log
)
CA(O
+
log
)


h
h
δ1
δ1


N0 = max 
,
,



2
2


ζ1 θ1
ζ2 θ2



P

(C.2)

for some universal constant C > 0, such that with probability at least 1 − δ1 , for any policy π ∈ Π, and

35

b, i, ϵe ): ϵe -approximate Best Response for the i th agent under Approximate
Algorithm 4 ABR(M, π
common information model M
b, i, ϵe
1: Input: M, π
⋆,M
2: Vi,H+1 (b
cH+1 ) ← 0 for all b
cH+1
3: for h = H, · · · , 1 do
4:
for each b
ch do


⋆,M
⋆,M
M
(b
ch , γh ) + EM Vi,h+1
(b
ch+1 ) | b
ch , {γj,h }j∈[n] for any i ∈ [n]
Define Qi,h
(b
ch , γ1,h , · · · , γn,h ) := b
ri,h

5:
6:

⋆,M
⋆
bi,h
π
(· | ·,b
ch , ·) ← NE/CE/CCE-BR(Qi,h
(b
ch , ·, · · · , ·), {b
πj,h (· | ·,b
ch , ·)}j∈[n] , i, ϵe )

// we refer the implementation
 to Section D.2

7:

⋆,M
Vi,h
(b
ch )

⋆,M
M
⋆
b−i,h (· | ω−i,h ,b
E{ωj,h }j∈[n] b
(· | ωi,h ,b
ch , ·), π
ch , ·)}) + EM [Vi,h+1
(b
ch+1 )
ri,h
(b
ch , {b
πi,h

⋆
b−i,h (· | ω−i,h ,b
b
(· | ωi,h ,b
ch , ·), π
ch , ·)}]
ch , {b
πi,h

←

|

end for
9: end for
bi⋆
10: return π
8:

i ∈ [n]:
c 1:H )
H2
π,M(π
π,G
Vi,1
(∅) − Vi,1
(∅) ≤ H · ϵr (π1:H ) +
ϵ (π1:H ) +
2 z

!
H2
+ H ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ).
2

c 1:H ) is an ϵ-NE if G
Under such a high probability event, the policy output of Algorithm 3 on M(π
zero-sum or cooperative and ϵ-CE/CCE if G is general-sum, where
ϵ := Hϵr (π1:H ,b
r ) + H 2 ϵz (π1:H ) + (H 2 + H)ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ) + Hϵe .
We state the full version of Theorem 5 regarding the instantiation of Theorem 4 in the following.
Theorem 9. Fix ϵ, δ > 0. Suppose the POSG G satisfies Assumptions 1 and 3. If there exist some compresbh for h ∈ [H + 1], π1:H , and M(π
f 1:H ) satisfying
sion functions of common information, Compressh : Ch → C
the conditions in Theorem 4, and there exists some parameters δ1 , θ1 , θ2 , ζ1 , ζ2 > 0 for Algorithm 5, ϵe > 0
for Algorithm 3, and some φ > 0, such that
n
o
max ϵz (π1:H ), ϵr (π1:H ), ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ) ≤ O(ϵ)
bh , H, A, O, 1 , 1 , 1 , 1 ) · log 1 is quasi-polynomial of the problem
and N0 = poly(maxh∈[H] Ph , maxh∈[H] C
ζ1 ζ2 θ1 θ2
δ1
instance size, then Algorithm 5, together with Algorithm 3, can output an ϵ-NE if G is zero-sum or cooperative, and an ϵ-CE/CCE if G is general-sum, with probability at least 1 − δ, using quasi-polynomial time
and samples, where b
L is defined as in Definition 10.
In particular, under Assumption 2, examples in Section 3 satisfy such all such conditions. Then, there
exists a multi-agent RL algorithm (Algorithm 9) that, with probability at least 1 − δ, learns an ϵ-NE if
G is zero-sum or cooperative, and ϵ-CE/CCE if G is general-sum, with the following information-sharing
structures and corresponding sample and time complexities:
Cγ −4 log SHO
γϵ

• One-step delayed information sharing: (AO)

36

log 1δ for some universal constant C > 0.

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1 ):
Algorithm 5 LEE(π1:H , {C
c 1:H ) of M(π
f 1:H )
M(π

Learning Empirical Estimator

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1
1: Input: π1:H , {C
2: for 1 ≤ h ≤ H do

Define N0 as in Equation (C.2).
Draw N0 independent trajectories by executing the policy πh , and denote the k th trajectory by
k
k
), for k ∈ [N0 ], where N0 is specified in Theorem 4.
, r1:H
(ak1:H−1 , o1:H
bh do
5:
for each b
ch ∈ C
k
k
6:
Define ϕ(ph ) := |{k : Compressh (fh (ak1:h−1 , o1:h
)) = b
ch , and gh (ak1:h−1 , o1:h
) = ph }|.

3:
4:

c 1:H )
M(π

Set Ph

7:

ϕ(p )

h
(ph |b
ch ) := P ′ ϕ(p
′ for all ph ∈ Ph .
)
p

h

h

end for
bh , ph ∈ Ph , ah ∈ A do
9:
for each b
ch ∈ C
k
k
10:
Define ψ(oh+1 ) := |{k : Compressh (fh (ak1:h−1 , o1:h
)) = b
ch , gh (ak1:h−1 , o1:h
) = ph , akh =
k
ah , and oh+1
= oh+1 }|.
8:

c 1:H )
M(π

Set Ph

11:
12:

ψ(oh+1 )
′
ψ(oh+1
)
o
h+1

(oh+1 |b
ch , ph , ah ) := P ′

for all oh+1 ∈ O.

k
k
) = ph , akh =
)) = b
ch , gh (ak1:h−1 , o1:h
Define κ(b
ch , ph , ah ) := {k : Compressh (fh (ak1:h−1 , o1:h
k
= oh+1 }.
ah , and oh+1
P

c 1:H )
M(π

k∈κ(b
c ,p ,a ) r

k

h h h i,h
(b
ch , ph , ah ) := |κ(b
Set b
ri,h
for all i ∈ [n].
ch ,ph ,ah )|
14:
end for
15: end for
bh , γh ∈ Γh , oh+1 ∈ Oh+1 , zh+1 ∈ Zh+1 :
16: Define for any h ∈ [H], b
ch ∈ C

13:

c 1:H ),z
M(π

Ph

X

(zh+1 |b
ch , γh ) ←

ph ,ah ,oh+1

c 1:H )
M(π

1[χh+1 (ph , ah , oh+1 ) = zh+1 ] × Ph

c 1:H )
M(π

(ph |b
ch )γh (ah | ph )Ph

(oh+1 |b
ch , ph , ah )
(B.1)

X M(π
c 1:H )
c 1:H )
c 1:H )
M(π
M(π
b
ri,h
(b
ch , γh ) ←
Ph
(ph |b
ch )γh (ah | ph )b
ri,h
(b
ch , ph , ah ),
ph ,ah
where we recall γh (ah | ph ) :=

(B.2)

Qn

j=1 γj,h (aj,h | pj,h ).
c 1:H
c 1:H ) := ({C
bh }h∈[H+1] , {φ
bh+1 }h∈[H] , {PM(π ),z }h∈[H] , Γ ,b
r)
17: return M(π
h

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1 , ϵe ):
Algorithm 6 Plam(π1:H , {C
approximate model

Planning

in

learned

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1 , ϵe
1: Input: π1:H , {C
c 1:H ) ← Construct(π1:H , {C
bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1 )
2: M(π
c 1:H ), ϵe )
3: π⋆ ← VIACM(
M(π
n
o
c 1:H )
4: return π⋆ , M(π

// i.e., Algorithm 5

// i.e., Algorithm 3

• State controlled by one controller with asymmetric d = poly(log H)-step delayed sharing sharC(γ −4 log SHO
γϵ +d)

ing: (AO)

log 1δ for some constant C > 0.

• Information sharing with one-directional-one-step delay:
37

c 1:H,m )}m∈[K] , {π⋆,j }j∈[K] , ϵe , N2 ): Policy Selection
Algorithm 7 PoS({M(π
c 1:H,j )}j∈[K] , {π⋆,j }j∈[K] , ϵe , N2
1: Input: {M(π
2: for i ∈ [n], j ∈ [K], m ∈ [K] do
⋆,j,m
c 1:H,m ), π⋆,j , i, ϵe ) // i.e., Algorithm 4
← ABR(M(π
πi
4: end for
5: for j ∈ [K] do
j
6:
Execute π⋆,j for N2 trajectories and let the mean accumulated reward for the i th agent be Ri
7: end for
8: for i ∈ [n], j ∈ [K], m ∈ [K] do
⋆,j,m
⋆,j
9:
Execute πi
⊙ π−i for N2 trajectories and let the mean accumulated reward for the i th agent

3:

j,m

be Ri
10: end for


j,m
j
11: b
j ← arg minj∈[K] maxi∈[n] maxm∈[K] (Ri − Ri )
12: return π⋆,j

b

Algorithm 8 PoS-Dec({π⋆,j }j∈[K] , N2 ): Policy Selection for Dec-POMDP
1: Input: {π⋆,j }j∈[K] , N2
2: for j ∈ [K] do

Execute π⋆,j for N2 trajectories and let the mean accumulated reward be Rj
4: end for
5: b
j ← arg maxj∈[K] Rj
3:

6: return π⋆,j

b

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ ,b
Algorithm 9 LACI(G, {C
L, ϵ, δ2 , ζ1 , ζ2 , θ1 , θ2 , δ1 , N2 , ϵe ):
Approximate Common Information

Learning with

bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ ,b
1: Input: G, {C
L, ϵ, δ2 , ζ1 , ζ2 , θ1 , θ2 , δ1 , N2 , ϵe
b
2: {π1:H,j }K
j=1 ← BaSeCAMP(G, L, ϵ, δ2 )

// i.e., Algorithm 3 of Golowich et al. (2022a)

3: for nj ∈ [K] do

o
c 1:H,j ) ← Plam(π1:H,j , {C
bh }h∈[H+1] , {φ
bh+1 }h∈[H] , Γ , ζ1 , ζ2 , θ1 , θ2 , δ1 , ϵe )
π⋆,j , M(π
rithm 6
5: end for
6: if learning the equilibrium then
b
c 1:H,j )}K , {π⋆,j }K , ϵe , N2 ) // i.e., Algorithm 7
7:
π⋆,j ← PoS({M(π
j=1
j=1
8: else if learning the team-optimum then
b
9:
π⋆,j ← PoS-Dec({π⋆,j }K
j=1 , N2 ) // i.e., Algorithm 8
10: end if
b
11: return π⋆,j
4:

Cγ −4 log SHO
γϵ

(AO)

// i.e., Algo-

log 1δ for some universal constant C > 0.
C(γ −4 log SHO
γϵ +d)

• Uncontrolled state process with d = poly(log H)-step delayed sharing: (AO)
for some universal constant C > 0.

38

log 1δ

Algorithm 10 ADPNIS(PM,c
ch )): Agent-based Dynamic Programming under Nested Informationh (·, · |b
Sharing
M,c

1: Input: Ph

(·, · |b
ch )

2: Initialize Vn+1 (p1:n+1,h , a1:n,h ) ← Es ∼PM,c (· |b
c ,p ),s
h

h

h

h

h+1



⋆,M
E
r
(s
,
a
)
+
V
(b
c
)
h+1
h+1
∼T (· | s ,a ) oh+1 ∼Oh+1 (· | sh+1 ) h h h
h

h

h

for any ph ∈ Ph , ah ∈ A
3: for i = n, · · · , 1 do
4:
for each p1:i,h ∈ ×ij=1 Pj,h , a1:i−1,h ∈ ×i−1
j=1 Aj do
5:

ui⋆ (p1:i,h , a1:i−1,h ) ← arg max Ep

M,c
ch ,p1:i,h )
i+1,h ∼Ph (· |b

ai,h ∈Ai

Vi+1 (p1:(i+1),h , a1:i,h )

6:

Vi (p1:i,h , a1:i−1,h ) ← max Ep
ai,h ∈Ai

M,c
ch ,p1:i,h )
i+1,h ∼Ph (· |b

Vi+1 (p1:(i+1),h , a1:i,h )

end for
8: end for
9: for i = 1, · · · , n do
10:
for each pi,h ∈ Pi,h do
11:
for each j = 1, · · · , i do
ij
12:
pj,h ← Yh (pi,h )
13:
a⋆j,h ← uj⋆ (p1:j,h , a⋆1:j−1 )
14:
end for
⋆
15:
γi,h
(pi,h ) ← a⋆i,h
16:
end for
17: end for
⋆
18: return {γi,h
}i∈[n]
7:

Cγ −4 log SHO
γϵ

• Symmetric information game: (AO)

log 1δ for some universal constant C > 0.

D

Technical Details and Omitted Proofs

D.1

Missing details in Section 4.1

Before proving Proposition 1, we present some hardness results for solving the stronger solution
concepts of team-optimal policy in Dec-POMDPs to further justify the necessity of some favorable
information-sharing structures.
Proposition 3. With 1-step delayed information-sharing structure and Assumption 2, computing the team
optimal policy in Dec-POMDPs with n = 2 is NP-hard.
To prove Proposition 3, we will firstly consider Dec-POMDPs with H = 1 and then connect the
1-step Dec-POMDP with Dec-POMDPs that have 1-step delayed sharing. We will show the reduction
from Team Decision Problem (Tsitsiklis and Athans, 1985):
Problem 1 (Team decision problem). Given finite sets Y1 , Y2 , U1 , U2 , a rational probability function p :
Y1 ×Y2 → Q and an integer cost function c : Y1 ×Y2 ×U1 ×U2 → N, find decision rules γi : Yi → Ui , i = 1, 2,
39

which minimize the expected cost:
J(γ1 , γ2 ) =

X X

c(y1 , y2 , γ1 (y1 ), γ2 (y2 ))p(y1 , y2 ).

y1 ∈Y1 y2 ∈Y2

Proposition 4. Without any information sharing, computing jointly team optimal policies in Dec-POMDP
with H = 1, n = 2 is NP-hard.
Proof. We can notice that the team decision problem is quite similar to our two-agent one-step DecPOMDP. The only difference in Dec-POMDP is that the joint observations are sampled given the
initial state, which is again sampled from µ1 . Now we will show how to reduce the team decision
problem to a Dec-POMDP. To begin with, we define cmax = maxy1 ,y2 ,u1 ,u2 c(y1 , y2 , u1 , u2 ). For any team
decision problem, we can construct the following Dec-POMDP:
• Ai = Ui , i = 1, 2;
• Oi = Yi , i = 1, 2;
• S = O1 × O2 .
• O(o1,h , o2,h | sh ) = 1 if sh = (o1,h , o2,h ), else 0, for h ∈ {1, 2};
• r1 (s1 , a1 ) = 1 − c(y1 , y2 , u1 , u2 )/cmax , where s1 = (y1 , y2 );
• µ1 (s1 ) = p(y1 , y2 ), where s1 = (y1 , y2 ).
⋆
⋆
Based on the construction, computing the optimal policies {π1,1
, π2,1
} under the no-informationsharing structure in the reduced Dec-POMDP problem will give us the optimal policies {γ1⋆ , γ2⋆ } in
the original team decision problem. Concretely, we can construct the optimal policy for the team
⋆
decision problem as γi⋆ (yi ) = πi,1
(oi,1 ), where oi,1 = yi . Given the NP-hardness of the team decision
problem shown in Tsitsiklis and Athans (1985), solving this corresponding Dec-POMDP without
information sharing is also NP-hard.

This result directly implies the hardness of Dec-POMDPs with 1-step delayed sharing structure:
Proposition 5. With 1-step delayed information-sharing structure, computing jointly team optimal policies in Dec-POMDPs with n = 2 is at least NP-hard.
Proof. Since there exists 1-step delay for the common information to be shared, when the DecPOMDPs have only 1-step, there is no shared common information among agents. Therefore, based
on the proof of Proposition 4, which concerns exactly such a case, computing joint optimal policies
in Dec-POMDPs with n = 2 is also at least NP-hard.
Finally, we are ready to prove Proposition 3.
Proof of Proposition 3. Similar to the proof of Proposition 5, it suffices to show that the proposition
holds for Dec-POMDPs, with H = 1 and without information sharing. Note that in the proof of
Proposition 4, the constructed Dec-POMDPs have the state space defined as the joint observation
space (the Cartesian product of the individual observation spaces), and the observation emission
is actually a one-to-one mapping from state space to joint observation space. Correspondingly, Oh
⊤ ′
′
′
is indeed an identity matrix. Therefore, we have O⊤
h b − Oh b 1 = ∥b − b ∥1 , for any b, b ∈ ∆(S),
verifying that γ = 1.
Now, let us restate and prove our hardness results regarding NE/CE/CCE in Proposition 1 as the
following two propositions.
40

Proposition 6. For zero-sum or cooperative POSGs with any kind of information-sharing structure (including the fully-sharing structure), computing ϵ-NE/CE/CCE is PSPACE-hard.
Proof. The proof leverages the known results of the hardness of solving POMDPs. Given any instance
of POMDPs, one could add a dummy agent with only one dummy observation and one available
action, which does not affect the transition, and use any desired information-sharing strategy. Since
this dummy agent only has one action and therefore it has only one policy. And the reward could
be identical to the original agent for cooperative games or the opposite of that for zero-sum games.
Therefore, ϵ-NE/CE/CCE in this constructed POSG with the desired information-sharing strategy
gives the ϵ-optimal policy in the original POMDP. Given the known PSPACE-hardness of POMDPs
(Papadimitriou and Tsitsiklis, 1987; Lusena et al., 2001), we conclude our proof.
Proposition 7. For zero-sum or cooperative POSGs satisfying Assumption 2 without information sharing,
computing ϵ-NE/CE/CCE is PSPACE-hard.
Proof. Similar to the proof of Proposition 6, given any instance of a POMDP, we could add a dummy
agent with only one available action, and the observation of the dummy agent is exactly the underlying state. Formally, given an instance of POMDP P = (S P , AP , O P , {OPh }h∈[H+1] , {TPh }h∈[H] , r P ), we
construct the POSG G as follows:
• S = SP ;
• A1 = AP , and A2 = {∅};
• O1 = O P , and O2 = S P ;
• For any h ∈ [H + 1], o1,h ∈ O1 , o2,h ∈ O2 , sh ∈ S, it holds that

P


Oh (o1,h | sh ) if o2,h = sh
;
Oh (o1,h , o2,h | sh ) = 

0
otherwise
• For any h ∈ [H], a1,h ∈ A1 , a2,h ∈ A2 , sh , sh+1 ∈ S, it holds that Th (sh+1 | sh , a1,h , a2,h ) =
TPh (sh+1 | sh , a1,h );
• For the reward, we use the reward from the original POMDP.
Now we are ready to verify that the joint observation emission satisfies Assumption 2 with γ = 1.
Consider any b, b′ ∈ ∆(S), denote b − b′ = (δs )⊤
s∈S as the column vector. For any h ∈ [H + 1], it holds that
X X
X
X
P
′
∥O⊤
(b
−
b
)∥
=
O
(o
,
o
|
s)δ
=
|O
(o
|
s)δ
|
=
|δs | = ∥b − b′ ∥1 ,
h 1,h 2,h
s
1
s
h
h 1,h
o1,h ,o2,h s∈S

o1,h ,s

s

which verifies that γ = 1 for our constructed POSG. Computing ϵ-NE/CE/CCE in such a 1-observable
POSG immediately gives us the ϵ-optimal policy in the original POMDP. Furthermore, note that
γ ≤ 1 for any possible emission, therefore, the conclusion also holds for any γ-observable POSG,
which proves our conclusion.
Finally, we provide the proof for Lemma 1 regarding usually how large Ch Ph is.
Proof of Lemma 1. Fix any h ∈ [H + 1]. If each agent has perfect recall, then it holds that for any joint
history {o1 , a1 , o2 , · · · , ah−1 , oh } ∈ O h × Ah−1 , there exists some ch ∈ Ch and ph ∈ Ph such that {ch , ph } =
{o1 , a1 , o2 , · · · , ah−1 , oh }, which can be found by the functions fh and gh introduced after Assumption 1.
Therefore, we conclude that O h × Ah−1 ⊆ Ch × Ph , implying that Ch Ph ≥ (OA)h−1 .
41

D.2

Missing details in Section 4.2

Similar to the value iteration algorithm in Markov games (Shapley, 1953), which solves a normalform game at each step, we utilize a similar value iteration framework. Specifically, under Assumption 3, we can have the Bellman equation as follows


π,G
π,G
Vi,h (ch ) = E{ωj,h }j∈[n] Es ,p ∼PG (·,· | c ) E{aj,h ∼πj,h (· | ωj,h ,ch ,pj,h )}j∈[n] ri,h (sh , ah ) + Vi,h+1 (ch+1 ) .
h

h

h

h

oh+1 ∼O⊤
h+1 Th (· | sh ,ah )

With Assumption 3, we are ready to present our Algorithm 1 based on value iteration in the
common information space, which runs in a backward way, enumerating all possible ch at each step
h and computing the corresponding equilibrium in the prescription space.
Implementing the equilibrium subroutine at each step. Now we will discuss the three equilibrium or best response (BR) subroutines at each step h ∈ [H], where NE or NE-BR is used for zero-sum
or cooperative games, and CE/CCE (or CE/CCE-BR) is used for general-sum games for computational tractability. To find efficient implementation for these subroutines, we need the following
important properties on the prescription-value function.
⋆,G
Proposition 8. Qi,h
(ch , γ1,h , · · · , γn,h ) defined in Algorithm 1 is linear with respect to each γi,h . More
specifically, we have:
⋆,G
∂Qi,h
(ch , γ1,h , · · · , γn,h )

∂γi,h (ai,h | pi,h )

=

X X

′
′
| ch )γ−i,h (a′−i,h | p−i,h
)
PGh (sh′ , pi,h , p−i,h

(D.1)

′
sh′ ,p−i,h
a′−i,h




 X
h
i


⋆,G
′
′
′

Oh+1 (oh+1 |sh+1 )Th (sh+1 |sh , ah ) ri,h (sh , ah ) + Vi,h+1 (ch+1 )  .
× 


′
oh+1 ,sh+1

Proof. The partial derivative can be easily verified by algebraic manipulations and the definition of
⋆,G
Qi,h
. From Equation (D.1), we could notice that γi,h does not appear on the RHS, which proves
⋆,G
Qi,h
(ch , γ1,h , · · · , γn,h ) is linear with respect to γi,h .

With such kind of linear structures, we are ready to introduce how to implement those oracles
efficiently.
⋆
⋆
• The NE subroutine will give us the approximate NE {γ1,h
, · · · , γn,h
} up to some error ϵe , which
satisfies:
⋆,G
⋆,G
⋆
⋆
⋆
Qi,h
(ch , γi,h
, γ−i,h
) ≥ max Qi,h
(ch , γi,h , γ−i,h
) − ϵe ,
∀i ∈ [n].
γi,h ∈∆(Ai )Pi,h

This NE subroutine will be intractable for general-sum games even with only two agents
(Daskalakis et al., 2009; Chen et al., 2009). However, for cooperative games and zero-sum
games, this NE subroutine can be implemented efficiently. At first look, this can be done by
formulating it as a normal-form game, where each agent has the corresponding action space
P
Ai i,h . However, this could not be tractable since the action space is indeed exponentially large.
Fortunately, for cooperative games and two-agent zero-sum games, we could utilize the linear
(concave) structure, where γi,h is a vector of dimension Ai Pi,h to develop an efficient algorithm
to compute ϵe -NE using standard no-external-regret or specifically gradient-play algorithms
(Daskalakis et al., 2011; Zhang et al., 2021c; Leonardos et al., 2022; Ding et al., 2022; Mao
et al., 2022), which will run in poly(S, A, Ph , ϵ1 ) time. To further illustrate how we avoid the
e

42

P

dependence of Ai i,h , we refer to Figure 2. Similarly, the best response (BR) subroutine for NE,
denoted as the NE-BR subroutine, is defined as follows: it outputs the approximate best re⋆
for the i th agent given {γj,h }j∈[n] up to some error ϵe , which satisfies:
sponse γi,h
⋆,G
⋆
Qi,h
(ch , γi,h
, γ−i,h ) ≥

⋆,G
′
Qi,h
(ch , γi,h
, γ−i,h ) − ϵe .

max

′
γi,h
∈∆(Ai )Pi,h

⋆,G
Its implementation is straightforward by linear programming since Qi,h
is linear with respect
to each agent’s prescription.
⋆,t
⋆,t T
• The CCE subroutine will give us the approximate CCE, a uniform mixture of {γ1,h
, · · · , γn,h
}t=1
up to some error ϵe , which satisfy for any i ∈ [n]:
T

T

t=1

t=1

1 X ⋆,G
1 X ⋆,G
⋆,t ⋆,t
⋆,t
Qi,h (ch , γi,h
, γ−i,h ) ≥ max
Qi,h (ch , γi,h , γ−i,h
) − ϵe .
T
γi,h ∈∆(Ai )Pi,h T
This subroutine can be implemented using standard no-external-regret learning algorithm as
in Gordon et al. (2008); Farina et al. (2022) with poly(S, A, Ph , ϵ1 ) time.
e

⋆
Similarly, the CCE-BR subroutine can be defined as follows: it outputs the best response γi,h
of
t
t T
th
the i agent, given {γ1,h , · · · , γn,h }t=1 up to some error ϵe , which satisfies:
T

T

t=1

t=1

1 X ⋆,G
1 X ⋆,G
⋆
t
′
t
Qi,h (ch , γi,h
, γ−i,h
) ≥ max
Qi,h (ch , γi,h
, γ−i,h
) − ϵe .
′
T
γi,h
∈∆(Ai )Pi,h T
The implementation of CCE-BR is the same as CCE except that only the i th agent runs the no⋆,t T
external-regret algorithm and other agents remain fixed. Once we get the sequence {γi,h
}t=1
⋆,t
⋆,G
1 PT
⋆
from the no-external-regret algorithm, we can take γi,h = T t=1 γi,h since Qi,h is linear with
respect to each agent’s prescription.
⋆,t
⋆,t T
• The CE subroutine will give us the approximate CE {γ1,h
, · · · , γn,h
}t=1 up to some error ϵe , which
satisfy for any i ∈ [n]:
T

T

t=1

t=1

1 X ⋆,G
1 X ⋆,G
⋆,t ⋆,t
⋆,t ⋆,t
Qi,h (ch , γi,h
, γ−i,h ) ≥ max
Qi,h (ch , ui,h ⋄ γi,h
, γ−i,h ) − ϵe .
u
T
i,h T
Here ui,h = {ui,h,pi,h }pi,h is the strategy modification, where ui,h,pi,h : Ai → Ai will modify the action ai,h to ui,h,pi,h (ai,h ) given the private information pi,h . It is easy to see that the composition of
P
ui,h with any prescription γi,h is equivalent to (ui,h ⋄γi,h )(ai,h | pi,h ) := ui,h,p (a′ )=ai,h γi,h (a′i,h | pi,h ).
i,h

i,h

One can verify that ui,h ⋄ γi,h = U · γi,h , for some matrix U ∈ RAi Pi,h ×Ai Pi,h (in a block diagonal
form). Therefore, the composition of ui,h and γi,h is indeed a linear transformation. Now, as the
⋆
function Qi,h
(ch , γ1,h , · · · , γn,h ) is concave (in fact, linear) with respect to each γi,h , one can run
the no-linear-regret algorithm as in Gordon et al. (2008), such that the time-averaged policy will
give us the approximate CE. In particular, such a guarantee can be achieved by running the
swap-regret minimization algorithm in Blum and Mansour (2007) separately for each (i, pi,h )
and the corresponding time complexity will be of poly(S, A, Ph , ϵ1 ).
e

The CE-BR subroutine can be defined as follows: it will output the best strategy modification
⋆
t
t T
of the i th agent, given {γ1,h
, · · · , γn,h
}t=1 up to some error ϵe , which satisfies:
ui,h
T

T

t=1

t=1

1 X ⋆,G
1 X ⋆,G
⋆
t
t
t
t
Qi,h (ch , ui,h
⋄ γi,h
, γ−i,h
) ≥ max
Qi,h (ch , ui,h ⋄ γi,h
, γ−i,h
) − ϵe .
ui,h T
T
43

⋆,t
t
⋆
for
⋄ γi,h
For notational convenience, we shall slightly abuse the notation, writing γi,h
:= ui,h
⋆
t
⋆
any t ∈ [T ] and we assume our CE-BR subroutine returns {ui,h ⋄ γi,h }t∈[T ] instead of ui,h . Its
implementation still follows from that of CE except that only the agent i runs the no-linearregret algorithm.

D.3

Proof of Theorem 2

To prove Theorem 1, we prove our main theorem, Theorem 2, which is a generalized version. We will
first bound the sub-optimality of the planning algorithm on M at each step h through the following
two lemmas.
Lemma 9. Fix the input M and ϵe > 0 for Algorithm 3. For any h ∈ [H + 1], ch ∈ Ch , and πi ∈ Πi , for
b⋆ , satisfies that
computing approximate NE/CCE, the output of Algorithm 3, π
⋆
π ×b
π−i
,M

Vi,hi

b⋆

π ,M
(ch ) ≤ Vi,h
(ch ) + (H + 1 − h)ϵe .

Proof. Obviously, the proposition holds for h = H + 1. Note that πi does not share the randomness
⋆
′
b−i
is independent of ω−i,h . Then, we have that
. In other words, the following ωi,h
with π
⋆
π ×b
π−i
,M

Vi,hi

(ch )
π ×b
π⋆ ,M

= EM
ω′ ,{ω }

i
M
−i
+ Vi,h+1
[b
ri,h

≤ EM
ω′ ,{ω }

π ,M
M
′
⋆
b−i,h
(ch+1 ) | b
ch , {πi,h (· | ωi,h
, ch , ·), π
(· | ω−i,h ,b
ch , ·)}]
+ Vi,h+1
[b
ri,h

i,h

i,h

j,h j∈[n]

j,h j∈[n]

′
⋆
b−i,h
(ch+1 ) | b
ch , {πi,h (· | ωi,h
, ch , ·), π
(· | ω−i,h ,b
ch , ·)}]

b⋆

(D.2)

+ (H − h)ϵe
⋆
b⋆ ×b
π
π−i
,M

i
′ E
= Eωi,h
{ωj,h }j∈[n] Qi,h

⋆
b⋆ ×b
π
π−i
,M

i
′ E
≤ Eωi,h
{ωj,h }j∈[n] Qi,h

′
⋆
b−i,h
, ch , ·), π
(ch , πi,h (· | ωi,h
(· | ω−i,h ,b
ch , ·)) + (H − h)ϵe
⋆
⋆
bi,h
b−i,h
(ch , π
(· | ωi,h , ch , ·), π
(· | ω−i,h ,b
ch , ·)) + (H − h + 1)ϵe

(D.3)

b⋆

π ,M
(ch ) + (H − h + 1)ϵe ,
= Vi,h

bh⋆ (· | ·,b
where Equation (D.2) comes from the inductive hypothesis, Equation (D.3) holds since π
ch , ·)
b⋆

π ,M
⋆,M
(ch+1 ) = Vi,h+1
(b
ch+1 ) through a simple induction arguis an ϵe -NE/CCE for the stage game and Vi,h+1
ment using Definition 13.

bh → ∆(Aj ) for j ∈ [n] takes only
b whose π
bj,h : Ωh × Pj,h × C
Corollary 1. Fix the input M, i ∈ [n], π
approximate common information instead of the exact common information as the input, and ϵe > 0 for
bi⋆ satisfies that
Algorithm 4. For any h ∈ [H + 1], ch ∈ Ch , and πi ∈ Πi , the output of Algorithm 4, π
π ×b
π−i ,M

Vi,hi

b⋆ ×b
π
π−i ,M

(ch ) ≤ Vi,hi

(ch ) + (H + 1 − h)ϵe .

Proof. Since Algorithm 4 simply replaces the equilibrium oracle at each stage of Algorithm 3 by a
best-response oracle, its proof follows directly from the proof of Lemma 9.
Lemma 10. For any h ∈ [H + 1], ch ∈ Ch , and φi ∈ Φi , for computing approximate CE, the output of
b⋆ satisfies that
Algorithm 3, π
⋆
(φ ⋄b
πi⋆ )⊙b
π−i
,M

Vi,h i

b⋆

π ,M
(ch ) ≤ Vi,h
(ch ) + (H − h + 1)ϵe .

44

Proof. It is direct to see that the lemma holds for the step H + 1. For step h ∈ [H], it holds that


⋆
(φ ⋄b
π⋆ )⊙b
π⋆ ,M
(φi ⋄b
πi⋆ )⊙b
π−i
,M
M
⋆
⋆
b
b
b
b
Vi,h i i −i (ch ) = EM
+
V
(c
)
|
c
,
{φ
⋄
π
(·
|
ω
,b
c
,
·),
π
(·
|
ω
,b
c
,
·)}
r
h+1
h
i,h,ch
i,h h
−i,h h
i,h
−i,h
i,h+1
{ωj,h }j∈[n] i,h


b⋆ ,M
π
⋆
b⋆
b−i,h
b
bM
≤ EM
ch , ·), π
(· | ω−i,h ,b
ch , ·) + (H − h)ϵe
(D.4)
{ωj,h }j∈[n] ri,h + Vi,h+1 (ch+1 ) | ch , {φi,h,ch ⋄ πi,h (· | ωi,h ,b


b⋆ ,M
π
M
⋆
⋆
b−i,h
b
≤ EM
ri,h
+ Vi,h+1
(ch+1 ) | b
ch , {b
πi,h
(· | ωi,h ,b
ch , ·), π
(· | ω−i,h ,b
ch , ·)} + (H − h)ϵe
(D.5)
{ω }
j,h j∈[n]

b⋆ ,M
π
= Vi,h
(ch ) + (H − h + 1)ϵe ,
b⋆

π ,M
where Equation (D.4) comes from the inductive hypothesis, Equation (D.5) holds since Vi,h+1
(ch+1 ) =
b⋆

π ,M
bh⋆ (· | ·,b
ch , ·) is an ϵe -CE for the stage game.
Vi,h+1
(b
ch+1 ), and π

bh → ∆(Aj ) for j ∈ [n] takes only
b whose π
bj,h : Ωh × Pj,h × C
Corollary 2. Fix the input M, i ∈ [n], π
approximate common information instead of the exact common information as the input, and ϵe > 0 for
bi⋆ satisfies that
Algorithm 4. For any h ∈ [H + 1], ch ∈ Ch , and φi ∈ Φi , the output of Algorithm 4, π
(φ ⋄b
πi )⊙b
π−i ,M

Vi,h i

b⋆ ⊙b
π
π−i ,M

(ch ) ≤ Vi,hi

(ch ) + (H + 1 − h)ϵe .

Proof. Similar to the proof of Corollary 1, its proof follows directly from Lemma 10.
Now we prove Lemma 3, showing the difference between the approximate value functions and
true value functions under the same set of policies.
Proof of Lemma 3. Note that it suffices to consider any policy π′ ∈ Πdet instead of π′ ∈ ∆(Πdet ). Obviously, the proposition holds for h = H + 1. For step h ∈ [H], we have
π,M
π,G
(ch )|]
(ch ) − Vi,h
EGa1:h−1 ,o1:h ∼π′ [|Vi,h


M
ri,h
(b
ch , {πj,h (· | ωj,h , ch , ·)}nj=1 )
≤ EGa1:h−1 ,o1:h ∼π′ E{ωj,h }j∈[n] EG [ri,h (sh , ah ) | ch , {πj,h (· | ωj,h , ch , ·)}nj=1 ] − E{ωj,h }j∈[n]b

π,G
+ EGa1:h−1 ,o1:h ∼π′ E{ωj,h }j∈[n] Ez ∼PG (· | c ,{π (· | ω ,c ,·)}n ) [Vi,h+1
({ch , zh+1 })]
h+1
h
j,h
j,h h
j=1
h

π,M
− E{ωj,h }j∈[n] Ez ∼PM,z (· |b
[V
({c
,
z
})]
n
c ,{π (· | ω ,c ,·)} ) i,h+1 h h+1
h+1

h

h

j,h

≤ ϵr + (H − h)EGa1:h−1 ,o1:h ∼π′ E{ωj,h }j∈[n]
+ EGa1:h ,o1:h+1 ∼π̄



j,h h

j=1

PGh (· | ch , {πj,h (· | ωj,h , ch , ·)}nj=1 ) − PM,z
ch , {πj,h (· | ωj,h , ch , ·)}nj=1 )
h (· |b
1

π,M
π,G
Vi,h+1
(ch+1 ) − Vi,h+1
(ch+1 )



(H − h)(H − h − 1)
ϵz
2
(H − h)(H − h + 1)
≤ (H − h + 1)ϵr +
ϵz ,
2
≤ ϵr + (H − h)ϵz + (H − h)ϵr +

where π̄ ∈ ∆(Πdet ) is the policy following π′ from step 1 to h − 1 and π from step h to H, thus
completing the proof.
Finally, we are ready to prove our main theorem, Theorem 2. Before that, we need to show that the
equilibrium subroutine at each h ∈ [H] in M is also computationally tractable. Specifically, similar
⋆,M
to Proposition 8, we can show Qi,h
is also linear w.r.t. each γi,h for i ∈ [n]. Hence, the algorithms
developed to implement the equilibrium subroutine for G in Section D.2 are directly applicable for
M.
45

Proposition 9. Given M that is consistent with the approximate belief {PM,c
ch )}h∈[H] , we have
h (sh , ph |b
⋆,M
Qi,h (b
ch , γ1,h , · · · , γn,h ) defined in Algorithm 3 is linear with respect to each γi,h . More specifically, we have:
⋆,M
∂Qi,h
(b
ch , γ1,h , · · · , γn,h )

∂γi,h (ai,h | pi,h )

X X

=

′
′
′
PM,c
ch )γ−i,h (a′−i,h | p−i,h
)
h (sh , pi,h , p−i,h |b

(D.6)

′
sh′ ,p−i,h
a′−i,h



 X

h
i


⋆,M
′
′
× 
Oh+1 (oh+1 |sh+1
)Th (sh+1
|sh′ , ah ) ri,h (sh , ah ) + Vi,h+1
(b
ch+1 )  .


′
oh+1 ,sh+1

Proof. The partial derivative can be easily verified by algebraic manipulations and the definition of
⋆,M
Qi,h
. From Equation (D.6), we could notice that γi,h does not appear on the RHS, which proves
⋆,M
Qi,h
(b
ch , γ1,h , · · · , γn,h ) is linear with respect to γi,h .

Proof of Theorem 2. For computing NE/CCE, we define for each agent i ∈ [n]
⋆
π ×b
π−i
,G

πi⋆ ∈ arg max Vi,1i
πi ∈Πi

(∅).

Now note that
π⋆ ×b
π⋆ ,G

b⋆

π ,G
Ea1:h−1 ,o1:h ∼π′ [Vi,hi −i (ch ) − Vi,h
(ch )]
  ⋆
 ⋆ ⋆

π ×b
π ,G
b⋆ ,G
b ,M
b⋆ ,M
π
π
π
(ch )
(ch ) − Vi,h
(ch ) + Vi,h
= Ea1:h−1 ,o1:h ∼π′ Vi,hi −i (ch ) − Vi,h
  ⋆
 ⋆ ⋆

⋆
πi⋆ ×b
π−i
,M
πi ×b
π−i ,G
b⋆ ,G
b ,M
π
π
(ch ) + Vi,h (ch ) − Vi,h (ch ) + (H + 1 − h)ϵe
≤ Ea1:h−1 ,o1:h ∼π′ Vi,h
(ch ) − Vi,h

≤ 2(H − h + 1)ϵr + (H − h)(H − h + 1)ϵz + (H − h + 1)ϵe .
Let h = 1, and note that c1 = ∅, we get
⋆
π⋆ ×b
π−i
,G

Vi,1i

b⋆

π ,G
(∅) − Vi,1
(∅) ≤ 2Hϵr + H 2 ϵz + Hϵe .

By the definition of πi⋆ , we conclude
NE/CCE-gap(b
π⋆ ) ≤ 2Hϵr + H 2 ϵz + Hϵe .
For computing CE, define
⋆
(φ ⋄b
πi⋆ )⊙b
π−i
,G

φi⋆ ∈ arg max Vi,1 i
φi

(∅).

Now note that
(φ⋆ ⋄b
π⋆ )⊙b
π⋆ ,G

b⋆

π ,G
Ea1:h−1 ,o1:h ∼π′ [Vi,h i i −i (ch ) − Vi,h
(ch )]

  ⋆

⋆
⋆
⋆
(φi ⋄b
πi )⊙b
π−i ,G
b⋆ ,M
b ,M
b⋆ ,G
π
π
π
′
= Ea1:h−1 ,o1:h ∼π Vi,h
(ch ) − Vi,h (ch ) + Vi,h (ch ) − Vi,h (ch )


(φ⋆ ⋄b
π⋆ )⊙b
π⋆ ,G
(φ⋆ ⋄b
π⋆ )⊙b
π⋆ ,M
≤ Ea1:h−1 ,o1:h ∼π′ Vi,h i i −i (ch ) − Vi,h i i −i (ch )
 ⋆

b ,M
b⋆ ,G
π
π
+ Ea1:h−1 ,o1:h ∼π′ Vi,h (ch ) − Vi,h (ch ) + (H + 1 − h)ϵe

≤ 2(H − h + 1)ϵr + (H − h)(H − h + 1)ϵz + (H − h + 1)ϵe .
46

Let h = 1, and note that c1 = ∅, we get
⋆
(φ⋆ ⋄b
πi⋆ )⊙b
π−i
,G

Vi,1 i

b⋆

π ,G
(∅) − Vi,1
(∅) ≤ 2Hϵr + H 2 ϵz + Hϵe .

By the definition of φi⋆ , we conclude
CE-gap(b
π⋆ ) ≤ 2Hϵr + H 2 ϵz + Hϵe .
The last step is the analysis of the computational complexity. A major difference from the exact
common-information setting is that it is unclear whether there exist efficient NE/CE/CCE subroutines at each step h. However, if M is consistent with some approximate belief {PM,c
ch )}h∈[H] ,
h (sh , ph |b
by Proposition 9, we conclude the NE subroutine for zero-sum or cooperative games and CE/CCE
subroutine for general-sum games can be also implemented efficiently with the computational complexity of poly(S, A, Ph , ϵ1 ). Hence the overall computational complexity of the Algorithm 3 is
e
bh poly(S, A, Ph , 1 ), where C
bh comes from the loop at each step h.
H maxh C
ϵe

Finally, we are ready to prove Theorem 1 as a special case.
Proof of Theorem 1. we can leverage the reduction in Nayyar et al. (2013a) that reduces G to an exact
common information model M(G) such that ϵz (M(G)) = ϵr (M(G)) = 0, where in this M(G), we have
b
ch = ch for any h ∈ [H + 1], ch ∈ Ch , and M(G) is consistent with {PGh (sh , ph | ch )}h∈[H] . Therefore, by
applying Theorem 2, we conclude the proof.

D.4

Proof of Theorem 3

Theorem 2 provides a structural result for the optimality of NE/CE/CCE policy computed with approximate common information in the underlying POSG, when the approximate common information satisfies the condition in Definition 7. However, it is not clear how to construct such approximate
common information and how high the induced computational complexity is. Here we will show
when the joint observation is informative enough, specifically satisfying Assumption 2, we could
simply use finite-memory truncation to compress the common information, and indeed, the corresponding most recent L steps of history is a kind of approximate common information. Importantly,
we need the a series of following result showing that the most recent history is enough to predict the
latent state of the POSG (with information sharing).
Lemma 11 (Lemma 4.9 in Golowich et al. (2022b)). Suppose the POSG satisfies Assumption 2, b, b′ ∈
∆(S) with b ≪ b′ , and fix any h ∈ [H]. Then
s
s
! 
!

′ ; y))
′)



(B
(b
D
(b;
y)∥B
(b∥b
D


2
h
h
2
4
40
Ey∼O⊤h b  exp
− 1 ≤ 1 − γ /2 · exp
− 1,


4
4
where we recall the definition of Bh in Section A.1.
This lemma states that once the emission Oh satisfies the condition in Assumption 2, the Bayes
operator Bh is a contraction in expectation. Since the individual emission Oi,h does not necessarily
satisfy Assumption 2, the individual Bayes operator Bi,h satisfies a weaker result. We first state a
more general lemma as follows.
Lemma 12. Given two finite domains X, Y , and the conditional probability q(y | x) for x ∈ X, y ∈ Y . Define
the posterior update F q (P ; y) : ∆(X) → ∆(X) for P ∈ ∆(X), y ∈ Y as
F q (P ; y)(x) = P

P (x)q(y | x)
.
′
′
x′ ∈X P (x )q(y | x )

47

(D.7)

Then for any δ1 , δ2 ∈ ∆(X) such that δ1 ≪ δ2 , it holds that
s
s
!
!
q
q
D2 (F (δ1 ; y)||F (δ2 ; y))
D2 (δ1 ||δ2 )
− 1 ≤ exp
− 1.
Ex∼δ1 ,y∼q(· | x) exp
4
4
Proof. This is a direct consequence of the proof of Lemma 4.9 in Golowich et al. (2022b) by allowing
γ = 0 since here we do not assume any observability on q.
Corollary 3. Suppose b, b′ ∈ ∆(S) with b ≪ b′ , and fix any h ∈ [H], i ∈ [n]. Then
s
!
 !  s

′ ; y)

(b
D
B
(b;
y)∥B
D2 (b∥b′ )
2
i,h
i,h



Ey∼O⊤i,h b  exp
− 1 ≤ exp
− 1.


4
4
Lemma 13 (Lemma 4.8 in Golowich et al. (2022b)). Consider probability distributions P , Q. Then
p
∥P − Q∥1 ≤ 4 · exp (D2 (P ∥Q)/4) − 1.
Theorem 10 (Adapted from Theorem 4.7 in Golowich et al. (2022b)). There is a constant C ≥ 1 so
that the following holds. Suppose that the POSG satisfies Assumption 2 with parameter γ. Let ϵ ≥ 0. Fix
a policy π′ ∈ ∆(Πdet ) and indices 1 ≤ h − L < h − 1 ≤ H. If L ≥ Cγ −4 log( Sϵ ), then the following set of
propositions hold
EGa1:h−1 ,o1:h ∼π′ ∥bh (a1:h−1 , o1:h ) − b′h (ah−L:h−1 , oh−L+1:h )∥1 ≤ ϵ,

(D.8)

EGa1:h−1 ,o1:h ∼π′ ∥bh (a1:h−1 , o1:h−1 ) − b′h (ah−L:h−1 , oh−L+1:h−1 )∥1 ≤ ϵ,

(D.9)

EGa1:h−1 ,o1:h ∼π′ ∥bh (a1:h−1 , o1:h−1 , o1,h ) − b′h (ah−L:h−1 , oh−L+1:h−1 , o1,h )∥1 ≤ ϵ.

(D.10)

Furthermore, for any finite domain Y , conditional probability q(y | s) and the posterior update operator
F q : ∆(S) → ∆(S) as defined in Lemma 12, it holds that
EGπ′ Ey∼q·bh (a1:h−1 ,o1:h ) ∥F q (bh (a1:h−1 , o1:h ); y) − F q (b′h (ah−L:h−1 , oh−L+1:h ); y)∥1 ≤ ϵ.

(D.11)

Proof. Equation (D.8) is from Theorem 4.7 in Golowich et al. (2022b). For the remaining, it suffices
to only consider π′ ∈ Πdet . We prove Equation (D.9) first. Note that if h − L ≤ 1, then we have
bh (a1:h−1 , o1:h−1 ) = b′h (ah−L:h−1 , oh−L+1:h−1 ). The proposition holds trivially. Now let us consider h >
L + 1. Fix some history (a1:h−L−1 , o1:h−L ). We condition on this history throughout the proof. For
0 ≤ t ≤ L, define the random variables
bh−L+t = bh−L+t (a1:h−L+t−1 , o1:h−L+t−1 ) ,
′
bh−L+t
= b′h−L+t (ah−L:h−L+t−1 , oh−L+1:h−L+t−1 ) ,
v
u

 
t
′
 D2 bh−L+t ∥bh−L+t



Yt = exp 
 − 1.


4
b (x)

′
Then D2 (bh−L ||bh−L
) = log Ex∼bh bh′ (x) ≤ log(S) since bh−L = b′h−L (∅) = Unif(S), so we have
h

Y0 ≤

q
′
exp(D2 (bh−L ||bh−L
)) ≤ S.

48

Moreover, for any 0 ≤ t ≤ L − 1, by denoting the shorthand notation of the matrix A := Th−L+t (ah−L+t ),
we have :
Eah−L:h−L+t ,oh−L+1:h−L+t ∼π′ Yt+1
= Eah−L:h−L+t−1 ,oh−L+1:h−L+t ∼π′ Eah−L+t ∼π′ (· | a1:h−L+t−1 ,o1:h−L+t )
s
! 
′

D2 (A · Bh−L+t (bh−L+t ; oh−L+t )||A · Bh−L+t (bh−L+t
; oh−L+t ))


− 1
 exp


4
s
! 
′

D
(B
(b
;
o
)||B
(b
;
o
))

2
h−L+t
h−L+t
h−L+t
h−L+t
h−L+t

h−L+t
− 1
≤ E (ah−L:h−L+t−1 , Eoh−L+t ∼O⊤h−L+t bh−L+t  exp


4
oh−L+1:h−L+t−1 )∼π′
!
γ4
≤ 1 − 40 Eah−L:h−L+t−1 ,oh−L+1:h−L+t−1 ∼π′ Yt ,
2
where the second last step comes from the data processing inequality and the last step comes from
Lemma 11. By induction and the choice of L, we have that
v
u

 
t
!L
 D2 bh ∥bh′ 
γ4
ϵ


Eoh−L:h−1 ,ah−L:h−1 ∼π′ exp 
(D.12)
 − 1 ≤ 1 − 40 S ≤ .


4
4
2
It follows from Lemma 13 that
Eah−L:h−1 ,oh−L+1:h−1 ∼π′ ||bh − bh′ ||1 ≤ ϵ.
Equation (D.9) follows from Equation (D.12) and Lemma 11. Equation (D.10) follows from Equation (D.12) and Corollary 3. Equation (D.11) follows from Equation (D.12) and Lemma 12.
Before instantiating the information structure in particular cases, we prove
Lemma 4 first, which is a more sufficient condition for our Definition 7.
Proof of Lemma 4. By Definition 8, it holds that
M
(b
ch , γh ) ≤
EG [ri,h (sh , ah ) | ch , γh ] −b
ri,h

X

ch , γh ) .
PGh (sh , ah | ch , γh ) − PM,o
h (sh , ah |b

sh ,ah

Therefore, it suffices to bound the right-hand side to order to prove Equation (7.2). Now, note that
for any ch ∈ Ch , γh ∈ Γh :
X
PGh (sh , sh+1 , ph , ah , oh+1 | ch , γh ) − PM
ch , γh )
h (sh , sh+1 , ph , ah , oh+1 |b
sh ,ph ,ah ,sh+1 ,oh+1

X

=

PGh (sh , ph | ch )

sh ,ph ,ah ,sh+1 ,oh+1

− PM,c
ch )
h (sh , ph |b

n
Y

γj,h (aj,h | pj,h )Th (sh+1 | sh , ah )Oh+1 (oh+1 | sh+1 )

j=1
n
Y

γj,h (aj,h | pj,h )Th (sh+1 | sh , ah )Oh+1 (oh+1 | sh+1 )

j=1

=

X

PGh (sh , ph | ch ) − PM,c
ch ) .
h (sh , ph |b

sh ,ph

49

Finally, since after marginalization, the total variation will not increase, we conclude that
X
ch , γh )
PGh (zh+1 | ch , γh ) − PM,z
h (zh+1 |b
zh+1

X

≤

ch , γh ) ,
PGh (sh , sh+1 , ph , ah , oh+1 | ch , γh ) − PM
h (sh , sh+1 , ph , ah , oh+1 |b

sh ,ph ,ah ,sh+1 ,oh+1

X

ch , γh )
PGh (sh , ah | ch , γh ) − PM
h (sh , ah |b

sh ,ah

X

≤

PGh (sh , sh+1 , ph , ah , oh+1 | ch , γh ) − PM
ch , γh ) ,
h (sh , sh+1 , ph , ah , oh+1 |b

sh ,ph ,ah ,sh+1 ,oh+1

which proved the lemma.
Therefore, in the following discussion, we only need to define b
ch and the corresponding belief
M,z
M
ch , γh ) will follow from the consisThe definition of Ph (zh+1 |b
ch , γh ) and b
ri,h (b
tency condition (5.4) and (5.5). Now we will show when G satisfies our Assumptions 1, 2, 3, how we
can construct approximate common information with history truncation that satisfies Definition 7.

{PM,c
ch )}h∈[H] .
h (sh , ph |b

One-step delayed information-sharing. In this case, the information structure has ch =
{a1:h−1 , o1:h−1 }, pi,h = {oi,h }, zh+1 = {oh , ah }, and PGh (sh , ph | ch ) = bh (a1:h−1 , o1:h−1 )(sh )Oh (oh | sh ), which
verifies Assumption 3.
Fix L > 0, we define the approximate common information as
b
ch = {ah−L:h−1 , oh−L+1:h−1 }. Furthermore, define the common information conditioned belief as
PM,c
ch ) = b′h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh ). Now we are ready to verify that it satisfies Defih (sh , ph |b
nition 7.
• Obviously, it satisfies condition (5.1).
• Note that for any ch ∈ Ch and the corresponding b
ch constructed above:
ch )∥1
∥PGh (·, · | ch ) − PM,c
h (·, · |b
X
=
bh (a1:h−1 , o1:h−1 )(sh )Oh (oh | sh ) − b′h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh )
sh ,oh

= ∥bh (a1:h−1 , o1:h−1 ) − b′h (ah−L:h−1 , oh−L+1:h−1 )∥1 .
Therefore, by setting L ≥ Cγ −4 log( Sϵ ), according to Equation (D.8) in Theorem 10, we conclude
that for any π′ ∈ Πdet , h ∈ [H]:
EGa1:h−1 ,o1:h ∼π′ ∥PGh (·, · | ch ) − PM,c
ch )∥1
h (·, · |b
≤ Ea1:h−1 ,o1:h ∼π′ ∥bh (a1:h−1 , o1:h−1 ) − b′h (ah−L:h−1 , oh−L+1:h−1 )∥1 ≤ ϵ.
Therefore, conditions (5.2), (5.3) in Definition 7 are satisfied using Lemma 4 with ϵr = ϵz = ϵ.
Formally, we have the following theorem:
Theorem 11. Let ϵ, γ > 0. Algorithm 1 given a γ-observable POSG of one-step delayed information sharing computes an ϵ-NE if the POSG is zero-sum or cooperative, and an ϵ-CE/CCE if the POSG is general-sum
SH
−4
with time complexity H(AO)Cγ log ϵ poly(S, A, O, H, 1ϵ ) for some universal constant C > 0.
bh ≤ (AO)L and Ph ≤ O, the polynomial dependence on S, H, A, and O
Proof. It is direct to see that C
M,c
comes from computing Ph (sh , ph |b
ch ) and the equilibrium computation subroutines.
50

State controlled by one controller with asymmetric delay sharing. The information structure is
given as ch = {o1,1:h , o2,1:h−d , a1,1:h−1 }, p1,h = ∅, p2,h = {o2,h−d+1:h }, zh+1 = {o1,h+1 , o2,h−d+1 , ah }. It is a
bit less straightforward to verify Assumption 3. We do so by explicitly computing PGh (sh , ph | ch )
as follows. Denote τh−d = {a1:h−d−1 , o1:h−d }, fa = {a1,h−d:h−1 }, fo = {o1,h−d+1:h }. Now PGh (sh , ph | ch ) =
P
G
G
sh−d Ph (sh , ph | sh−d , fa , fo )Ph (sh−d | τh−d , fa , fo ). It is direct to see that

PGh (sh , ph | sh−d , fa , fo ) does not depend on the policy. For PGh (sh−d | τh−d , fa , fo ), the following holds
PGh (sh−d , fa , fo | τh−d )
G
Ph (sh−d | τh−d , fa , fo ) = P
.
′
′
PGh (sh−d
, fa , fo | τh−d )
sh−d
Now note that
PGh (sh−d , fa , fo | τh−d )

= bh−d (a1:h−d−1 , o1:h−d )(sh−d )PGh (a1,h−d | τh−d )PGh (o1,h−d+1 | sh−d , a1,h−d ) · · · PGh (o1,h | sh−d , a1,h−d:h−1 ).
Q
Now let us use the notation Ph (fo | sh−d , fa ) := dt=1 PGh (o1,h−d+t | sh−d , a1,h−d:h−d+t−1 ). Then it holds that
P
fo Ph (fo | sh−d , fa ) = 1, which suggests that the notation Ph (fo | ss−d , fa ) can be understood as a conditional probability. With such notation, we have
PGh (sh−d | τh−d , fa , fo ) = P

bh−d (a1:h−d−1 , o1:h−d )(sh−d )Ph (fo | sh−d , fa )
′
′
s′ bh−d (a1:h−d−1 , o1:h−d )(sh−d )Ph (fo | sh−d , fa )
h−d

= F Ph (· | ·,fa ) (bh−d (a1:h−d−1 , o1:h−d ); fo )(sh−d ),
where we recall the definition of F in Lemma 12. Finally, we compute:
X
PGh (sh , ph | sh−d , fa , fo )F Ph (· | ·,fa ) (bh−d (a1:h−d−1 , o1:h−d ); fo )(sh−d ).
PGh (sh , ph | ch ) =
sh−d

It is easy to see that this expression does not depend on the policy executed, thus verifying Assumption 3. Now for some fixed L > 0, we construct the approximate common information
b
ch := {o1,h−d−L+1:h , o2,h−d−L+1:h−d , a1,h−d−L:h−1 } and correspondingly:
X
PM,c
ch ) =
PGh (sh , ph | sh−d , fa , fo )F Ph (· | ·,fa ) (b′h−d (ah−d−L:h−d−1 , oh−d−L+1:h−d ); fo )(sh−d ).
h (sh , ph |b
sh−d

To verify Definition 7:
• Obviously, it satisfies the condition (5.1).
• For any ch ∈ Ch and the corresponding b
ch constructed above:
∥PGh (·, · | ch ) − PM,c
ch )∥1
h (·, · |b
≤ F P (· | ·,fa ) (bh−d (a1:h−d−1 , o1:h−d ); fo ) − F P (· | ·,fa ) (b′h−d (ah−d−L:h−d−1 , oh−d−L+1:h−d ); fo ) .
1

Finally, for any policy π′ ∈ Πdet taking expectations over τh−d , fa , fo , we conclude that as long as
L ≥ Cγ −4 log Sϵ using Equation D.11 of Theorem 10, we have
EGa1:h−1 ,o1:h ∼π′ ∥PGh (·, · | ch ) − PM,c
ch )∥1 ≤ ϵ.
h (·, · |b
Therefore, conditions (5.2), (5.3) in Definition 7 are satisfied using Lemma 4 with ϵr = ϵz = ϵ.
51

Formally, we have the following theorem:
Theorem 12. Let ϵ, γ > 0. Algorithm 1 given a γ-observable POSG of state controlled by one controller
with asymmetric delay sharing computes an ϵ-NE if the POSG is zero-sum or cooperative, and an ϵ-CE/CCE
SH
−4
if the POSG is general-sum with time complexity H(AO)C(γ log ϵ +d) poly(S, A, O, H, 1ϵ ) for some universal
constant C > 0.
bh ≤ (AO)L+d and Ph ≤ Od . The polynomial dependence on S, H,
Proof. It follows from the fact that C
2
A, and O comes from computing PM,c
(s
,
p
|b
c
)
and
the
equilibrium
computation subroutines.
h h h
h
Information sharing with one-directional-one-step delay. For this case, we have
ch = {a1:h−1 , o1:h−1 , o1,h }, p1,h = ∅, p2,h = {o2,h }, zh+1 = {o1,h+1 , o2,h , ah }, and PGh (sh , ph | ch ) =
bh (a1:h−1 , o1:h−1 , o1,h )(sh )Ph (o2,h | sh , o1,h ),

where Ph (o2,h | sh , o1,h )

=

Oh (o1,h ,o2,h | sh )
′
o′ Oh (o1,h ,o2,h | sh )
2,h

P

,

thus veri-

fying Assumption 3.
Fix L > 0, we construct the approximate common information as
b
ch = {ah−L:h−1 , oh−L+1:h−1 , o1,h }. Furthermore, we define the belief as
PM,c
ch ) = b′h (ah−L:h−1 , oh−L+1:h−1 , o1,h )(sh )Ph (o2,h | sh , o1,h ).
h (sh , ph |b
Now we are ready to verify that Definition 7 is satisfied.
• Obviously, the condition (5.1) is satisfied.
• Note that for any ch ∈ Ch and the corresponding b
ch constructed above:
ch )
PGh (·, · | ch ) − PM,c
h (·, · |b
1
X
=
bh (a1:h−1 , o1:h−1 , o1,h )(sh )Ph (o2,h | sh , o1,h ) − b′h (ah−L:h−1 , oh−L+1:h−1 , o1,h )(sh )Ph (o2,h | sh , o1,h )
sh ,o2,h

= ∥bh (a1:h−1 , o1:h−1 , o1,h ) − b′h (ah−L:h−1 , oh−L+1:h−1 , o1,h )∥1 .
Therefore, by setting L ≥ Cγ −4 log( Sϵ ), according to (D.10) in Theorem 10, we conclude that for
any π′ ∈ Πdet :
EGa1:h−1 ,o1:h ∼π′ ∥PGh (·, · | ch ) − PM,c
ch )∥1
h (·, · |b
≤ EGa1:h−1 ,o1:h ∼π′ bh (a1:h−1 , o1:h−1 , o1,h ) − b′h (ah−L:h−1 , oh−L+1:h−1 , o1,h )

1

≤ ϵ.

Therefore, conditions (5.2), (5.3) in Definition 7 are satisfied using Lemma 4 with ϵr = ϵz = ϵ.
Formally, we have the following theorem:
Theorem 13. Let ϵ, γ > 0. Algorithm 1 given a γ-observable POSG of information sharing with onedirectional-one-step delay computes an ϵ-NE if the POSG is zero-sum or cooperative, and an ϵ-CE/CCE
SH
−4
if the POSG is general-sum with time complexity H(AO)Cγ log ϵ poly(S, A, O, H, 1ϵ ) for some universal
constant C > 0.
bh ≤ (AO)L and Ph ≤ O2 . The polynomial dependence on S, H, A, and O
Proof. It is direct to see that C
comes from computing PM,c
ch ) and the equilibrium computation subroutines.
h (sh , ph |b

52

Uncontrolled state process with delayed sharing. As long as the state transition does not depend
on the actions, Assumption 3 is satisfied. To be more concrete, we have
X
PGh (sh , ph | ch ) =
bh−d (oh−d−L+1:h−d )(sh−d )PGh (sh , oh−d+1:h | sh−d ),
sh−d

which verifies Assumption 3, where in the notation for bh−d , we omit the actions since they do
not affect transitions. For generality, we consider the d-step delayed sharing information structure, where d ≥ 0 and not necessarily d = 1, as in the one-step delayed information sharing structure. The information structure satisfies ch = {o1:h−d }, pi,h = {oi,h−d+1:h }, and zh+1 = {oh−d+1 }. Fix
a L > 0, the approximate common information is b
ch = {oh−d−L+1:h−d }, the corresponding belief is
P
G
′
PM,c
(s
,
p
|b
c
)
=
b
(o
)(s
)P
(s
,
o
h h h
h−d h h h−d+1:h | sh−d ). Now we are ready to verify Defish−d h−d h−d−L+1:h−d
h
nition 7.
• Obviously, the condition (5.1) is satisfied.
• Note that for any ch and the corresponding b
ch constructed above:
ch )
PGh (·, · | ch ) − PM,c
h (·, · |b
1
X
X X
bh−d (o1:h−d )(sh−d )PGh (sh , oh−d+1:h | sh−d ) −
b′h−d (oh−d−L+1:h−d )(sh−d )PGh (sh , oh−d+1:h | sh−d )
=
sh ,oh−d+1:h sh−d

=

X

sh−d

X
(bh−d (o1:h−d )(sh−d ) − b′h−d (oh−d−L+1:h−d )(sh−d ))PGh (sh , oh−d+1:h | sh−d )

sh ,oh−d+1:h sh−d

≤ ∥bh−d (o1:h−d ) − b′h−d (oh−d−L+1:h−d )∥1 ,
where for the last step, we use Lemma 14 (proved later). Therefore, by setting L ≥ Cγ −4 log( Sϵ ),
according to Equation (D.9) in Theorem 10, we conclude that for any π′ ∈ Πdet :
ch ) 1 ≤ EGπ′ bh−d (o1:h−d ) − b′h−d (oh−d−L+1:h−d ) 1 ≤ ϵ.
EGπ′ PGh (·, · | ch ) − PM,c
h (·, · |b
This verifies the conditions (5.2), (5.3) in Definition 7 using Lemma 4 with ϵr = ϵz = ϵ.
b⋆ is an ϵ-NE/CE/CCE, according to our Theorem 2, one needs L ≥
Finally, to guarantee that π
SH
Cγ −4 log( ϵ ). Formally, we have the following theorem:
Theorem 14. Let ϵ, γ > 0. Algorithm 1 given a γ-observable POSG of uncontrolled state process computes
an ϵ-NE if the POSG is zero-sum or cooperative, and an ϵ-CE/CCE if the POSG is general-sum with time
SH
−4
complexity H(O)Cγ log ϵ poly(S, A, Od , H, 1ϵ ) for some universal constant C > 0.
bh ≤ OL and Ph = Od . The polynomial dependence on S, A, H, and Od
Proof. It is direct to see that C
M,c
comes from computing Ph (sh , ph |b
ch ) and the equilibrium computation subroutines.
Symmetric information game. For symmetric information games, it has the following information structure: ch = {a1:h−1 , o1:h }, pi,h = ∅, zh+1 = {ah , oh+1 }, and PGh (sh , ph | ch ) = bh (a1:h−1 , o1:h )(sh ),
verifying Assumption 3. Fix L > 0, we construct the approximate common information as b
ch =
′
{ah−L:h−1 , oh−L+1:h }. Furthermore, we define the belief PM,c
(s
,
p
|b
c
)
=
b
(a
,
o
)(s
).
Now
h h h
h
h h−L:h−1 h−L+1:h h
we are ready to verify Definition 7.
• Obviously, it satisfies the condition (5.1).
53

• Note that for any ch ∈ Ch and the corresponding b
ch constructed above:
ch ) 1 = ∥bh (a1:h−1 , o1:h ) − b′h (ah−L,h−1 , oh−L+1:h )∥1 .
PGh (·, · | ch ) − PM,c
h (·, · |b
Therefore, by setting L ≥ Cγ −4 log( Sϵ ), according to (D.9) in Theorem 10, we conclude that for
any π′ ∈ Πdet :
ch ) 1 = ∥bh (a1:h−1 , o1:h ) − b′h (ah−L,h−1 , oh−L+1:h )∥1 ≤ ϵ.
EGa1:h−1 ,o1:h ∼π′ PGh (·, · | ch ) − PM,c
h (·, · |b
Therefore, the conditions (5.2) and (5.3) in Definition 7 are satisfied with ϵr = ϵz = ϵ using
Lemma 4.
b⋆ is an ϵ-NE/CE/CCE, according to Theorem 2, one needs L ≥
Finally, to guarantee π

Cγ −4 log( SH
ϵ ). Formally, we have the following theorem:

Theorem 15. Let ϵ, γ > 0. Algorithm 1 given a γ-observable POSG of symmetric information computes
an ϵ-NE if the POSG is zero-sum or cooperative, and an ϵ-CE/CCE if the POSG is general-sum with time
SH
−4
complexity H(AO)Cγ log ϵ poly(S, A, H, O, 1ϵ ) for some universal constant C > 0.
bh = (AO)L and Ph = 1, the polynomial dependence on S, H, A, and O
Proof. It is direct to see that C
M,c
comes from computing Ph (sh , ph |b
ch ) and the equilibrium computation subroutines.
We conclude the section by proving the following lemma.
m n
Lemma 14. For any given sequence {xi }m
i=1 and {{yi,j }i=1 }j=1 such that
following holds
n X
m
m
X
X
xi yi,j ≤
|xi |.
j=1 i=1

Pn

j=1 |yi,j | = 1 , ∀i ∈ [m].

The

i=1

Proof. Let x = (x1 , · · · , xm )⊤ , yj = (y1,j , · · · , ym,j )⊤ , and Y = (y1 , · · · , yn ). Therefore, we have
n X
m
X

xi yi,j =

D.5

|x⊤ yj | = ||Y ⊤ x||1 ≤ ||Y ⊤ ||1 ||x||1 .

j=1

j=1 i=1

Note that ||Y ⊤ ||1 = ||Y ||∞ = maxi
conclude the proof.

n
X

Pn

j=1 |yi,j | = 1.

Therefore, we have

Pn

j=1 |

Pm

i=1 xi yi,j | ≤

Pm

i=1 |xi |, and

Proof of Theorem 4

Note that our previous planning algorithms require the knowledge of the true model (transition
dynamics and rewards) of the POSG G, which avoids the issue of strategic explorations. For learning
NE/CE/CCE in G, one could potentially treat G as a (fully-observable) Markov game on the state
space of ch , and use black-box algorithms for learning Markov games. However, this formulation
could be neither computationally nor sample efficient because of the typical large space of common
information. Therefore, we have to learn NE/CE/CCE in the approximate model M with the state
space of b
ch in Definition 7. However, the key problem is that we can only sample according to the
model of G instead of M. As we highlighted in Section 5.2 of our main paper, to circumvent this
f 1:H ) using a
issue, inspired by the idea of Golowich et al. (2022a), one solution is to construct M(π
sequence of H policies π1:H according to Definition 9, where each πh ∈ ∆(Πdet ). Formally, Proposition
f 1:H ) constructed according to Definition 9 can be simulated by executing policies
10 verifies that M(π
h
π at each step h in the underlying true model G.
54

f 1:H ) as in Definition 9, it holds that for any i ∈ [n], h ∈ [H], b
bh , γh ∈ Γh ,
Proposition 10. Given M(π
ch ∈ C
oh+1 ∈ O, zh+1 ∈ Zh+1 :
f 1:H ),z
M(π

Ph

f 1:H )
M(π

b
ri,h
πh

πh

,G

(zh+1 |b
ch , γh ) = Ph 1:h−1 (zh+1 |b
ch , γh ),
(b
ch , γh ) = EG h

π1:h−1

[ri,h (sh , ah ) |b
ch , γh ].

,G

Proof. Note for Ph 1:h−1 (zh+1 |b
ch , γh ), it holds that
πh

,G

Ph 1:h−1 (zh+1 |b
ch , γh )
X
πh ,G
=
ch , γh )
Ph 1:h−1 (ph , ah , oh+1 |b
ph ,ah ,oh+1 :
χh+1 (ph ,ah ,oh+1 )=zh+1

=

X
sh ,ph ,ah ,oh+1 :
χh+1 (ph ,ah ,oh+1 )=zh+1



πh

,G

Ph 1:h−1 (sh , ph , |b
ch )γh (ah | ph ) ×

X


Th (sh+1 | sh , ah )Oh+1 (oh+1 | sh+1 ) ,

sh+1

where we recall the shorthand notation γh (ah | ph ) :=

Q

j∈[n] γj,h (aj,h | pj,h ). Now by Definition 9, we
h
f 1:H ),c
π1:h−1
,G
M(π
(sh , ph , |b
ch ). Combined with Equation (5.4) of Definition 8, we con(sh , ph , |b
ch ) = Ph
have Ph
h
f 1:H )
f 1:H ),z
π1:h−1
,G
M(π
M(π
(b
ch , γh ) =
(zh+1 |b
ch , γh ). At the same time, we can prove b
ri,h
(zh+1 |b
ch , γh ) = Ph
clude Ph
G
E h [ri,h (sh , ah ) |b
ch , γh ] holds by the same derivation.
π1:h−1

Therefore, different from a generic M in Definition 7, to which we do not have algorithmic access,
such a delicately designed transition dynamic and reward function allow us to actually simulate
f 1:H ) by executing policies π1:H in G.
M(π
bh }h∈[H+1] . It turns out that when such a state
The next question is how to explore the state space {C
b
ch comes from a sequence of observations and actions, a uniform policy can be used to explore the
state space (Efroni et al., 2022; Golowich et al., 2022a). Formally, define the under-explored set of b
ch
and b
ch ∪ ph under some policy π as follows.
Definition 14. Fix b
L > 0 as given in Definition 10. For each h ∈ [H], ζ > 0, and a joint policy π ∈ ∆(Πdet ),
low
bh as
define the set Ch,ζ (π) ⊆ C
n
o
bh : d π,G (b
C low (π) := b
ch ∈ C
ch ) < ζ ,
h,ζ

C,h

low
bh × Ph as
and the set Vh,ζ
(π) ⊆ Vh := C

n
o
low
Vh,ζ
(π) := vh ∈ Vh : dVπ,G
(v
)
<
ζ
,
h
,h
low
and the set Xh,ζ
(π) ⊆ Xh := Amin{h,L} × O min{h,L} as
n
o
low
Xh,ζ
(π) := xh ∈ Xh : dXπ,G
(x
)
<
ζ
,
h
,h
b

b

π,G
π,G
π,G
π,G
where dC,h
(b
ch ) := Pπ,G
ch ), dVπ,G
h (b
,h (vh ) := Ph (vh ), and dX ,h (xh ) := Ph (xh ).

Now we shall relate the under-explored set of b
ch with the under-explored set of sh′ for some
′
h ∈ [H]. Firstly, for any φ > 0, define the under-explored states under some policy π ∈ ∆(Πdet ) as
π,G
G
Uφ,h
(π) := {s ∈ S : dS,h
(s) < φ}.

Then the following lemma holds.
55

Lemma 15. Fix b
L > 0 as given in Definition 10. Fix any ζ > 0, φ > 0, h ∈ [H]. Consider any policies π,
′
det
π ∈ ∆(Π ), such that π′ takes uniformly random actions at each step from max{h − b
L, 1} to h, each chosen
independently of all previous states, actions, and observations. Then, we have
π,G low ′
dC,h
(Ch,ζ (π )) ≤

A2L OL ζ
+ 1[h > b
L] · d π,G b(U G b(π′ )).
S,h−L φ,h−L
φ
b b

bh
Proof. Note that we have for each b
ch ∈ C
X

π,G
dC,h
(b
ch ) =

dXπ,G
,h (xh )

xh :fbh (xh )=b
ch

where we recall the definition of fbh and xh from Definition 10. Therefore, we have
X
X
π,G
dC,h
(b
ch ) =
dXπ,G
,h (xh )
low ′
b
ch <Ch,ζ
(π )

low ′
b
ch <Ch,ζ
(π )
b
xh :fh (xh )=b
ch

X

=

dXπ,G
,h (xh )

low ′
xh :fbh (xh )<Ch,ζ
(π )

X

≥

dXπ,G
,h (xh ),

low ′
xh <Xh,ζ
(π )

low ′
low ′
where the last step comes from the fact that xh < Xh,ζ
(π ) implies fbh (xh ) < Ch,ζ
(π ). This leads to that
b b
A2L OL ζ
π,G low ′
π,G
low ′
+ 1[h > b
L] · d π,G b(U G b(π′ )),
dC,h (Ch,ζ (π )) ≤ dX ,h (Xh,ζ (π )) ≤
S,h−L φ,h−L
φ

where in the second inequality, we use Lemma 10.4 of Golowich et al. (2022a).
f 1:H ),z
M(π

The next step is to learn Ph
h
π1:h−1
,G

defined as Ph

f 1:H )
M(π

(zh+1 |b
ch , γh ), b
ri,h

(zh+1 |b
ch , γh ) and EG h

π1:h−1

f 1:H ), which are
(b
ch , γh ) of the model M(π

[ri,h (sh , ah ) |b
ch , γh ], respectively. The challenge here compared

with the single-agent learning problem (Golowich et al., 2022a) is that although γh serves as the
f 1:H ), it is not possible to enumerate all possible actions, since
actions for the approximate game M(π
γh in general lies in continuous spaces, and even if we only consider deterministic γh , the number of
all possible mappings from the private information to the real actions in G is still of the order APh .
f 1:H ),z
M(π

Therefore, learning Ph

(zh+1 |b
ch , γh ) by enumerating all possible b
ch and γh is not statistically
f 1:H ),z
M(π

efficient. To circumvent this issue, we observe the fact that for Ph
πh

,G

ch , γh ) =
Ph 1:h−1 (zh+1 |b

where

we recall χh+1
h
π1:h−1
,G
Ph
(ph , ah , oh+1 |b
ch , γh ):

in

πh

X
ph ,ah ,oh+1 :
χh+1 (ph ,ah ,oh+1 )=zh+1

Assumption

1.

πh ,G
πh ,G
Ph 1:h−1 (ph , ah , oh+1 |b
ch , γh ) = Ph 1:h−1 (ph |b
ch )

i=1

56

,G

Ph 1:h−1 (ph , ah , oh+1 |b
ch , γh ),

Further,

n
Y

(zh+1 |b
ch , γh ), it holds that

notice

πh

the

,G

decomposition

γi,h (ai,h | pi,h )Ph 1:h−1 (oh+1 |b
ch , ph , ah ).

for

πh

πh

,G

,G

Therefore, it suffices to learn Ph 1:h−1 (ph |b
ch ) and Ph 1:h−1 (oh+1 |b
ch , ph , ah ). Similarly for b
r M(π
that
n
X πh ,G
Y
f 1:H )
πh
M(π
1:h−1
b
(b
ch , γh ) =
ri,h
Ph
(ph |b
ch )
ch , ph , ah ),
γj,h (aj,h | pj,h )ri,h1:h−1 (b
ph ,ah

f

1:H

) , it holds

j=1

πh

where we define ri,h1:h−1 (b
ch , ph , ah ) := EG h

π1:h−1

[ri,h (sh , ah ) |b
ch , ph , ah ]. Formally, the following algorithm

c 1:H ) of M(π
f 1:H ). The algorithm for constructing the approximation
learns an approximation M(π
enjoys the following guarantee.
Lemma 16. Fix δ1 , ζ1 , ζ2 , θ1 , θ2 > 0. For Algorithm 5, suppose for all h ∈ [H], πh ∈ ∆(Πdet ) satisfies the
conditions for π′ of Lemma 15, then as long as N0 in Algorithm 5 satisfies


bh Ph )A 
bh
4H maxh (C
4H maxh C



CA(O
+
log
C(max
P
+
log
)
)


h h
δ1
δ1


N0 ≥ max 
,



2
2


ζ1 θ1
ζ2 θ2


for some sufficiently large constant C, then with probability at least 1 − δ1 , the following holds:
low
(πh ), we have that
• For all h ∈ [H], b
ch < Ch,ζ
1

X

c 1:H )
M(π

Ph

πh

,G

ch ) ≤ θ1 .
(ph |b
ch ) − Ph 1:h−1 (ph |b

(D.13)

ph
low
• For all h ∈ [H], (b
ch , ph ) < Vh,ζ
(πh ), ah ∈ A, we have that
2

X

c 1:H )
M(π

Ph

πh

,G

(oh+1 |b
ch , ph , ah ) − Ph 1:h−1 (oh+1 |b
ch , ph , ah ) ≤ θ2 ,

(D.14)

oh+1
c 1:H )
M(π

b
ri,h

1:H

π
(b
ch , ph , ah ) ≤ θ2 .
(b
ch , ph , ah ) − ri,h

(D.15)

We refer to the two bullets above as event E1 .
Proof. We will prove Equation (D.13) first. Note that for any trajectory k of Algorithm 5, the distriπh

,G

bution of phk conditioned on b
chk is exactly Ph 1:h−1 (· |b
chk ).
low
Now consider any b
ch < Ch,ζ
(πh ). By the Chernoff bound, with probability at least 1 − exp(− ζ18N0 ),
1

there are at least ζ12N0 trajectories indexed by the set K1 ⊆ [N0 ], such that for any k ∈ K1 ,
k
Compressh (fh (ak1:h−1 , o1:h
)) = b
ch . By the folklore theorem of learning a discrete probability distribution
(Canonne, 2020), with probability at least 1 − p′ , (D.13) holds as long as
1
ζ1 N0 C(Ph + log p′ )
≥
,
2
θ12

(D.16)

bh , (D.13) holds with
for some constant C > 1. By a union bound over all possible h ∈ [H] and b
ch ∈ C
probability at least
bh exp(− ζ1 N0 ) − H max C
bh p′ .
1 − H max C
8
h
h

57

b
4H max C

h h)
C(maxh Ph +log
δ1
δ1
. Furand it is easy to verify that (D.16) holds since N0 ≥
2
b
ζ1 θ1
4H maxh Ch
bh exp(− ζ1 N0 ) ≤ δ1 . Therefore, we
thermore, as long as C is sufficiently large, we have that H maxh C
8
4
low
ch < Ch,ζ
proved that with probability at least 1 − δ21 , Equation (D.13) holds for all h ∈ [H], and b
(πh ).
1

Now set p′ =

Similarly, consider any trajectory k, the distribution of oh+1 conditioned on any (b
ch , ph , ah ) is exπh

,G

low h
actly Ph 1:h−1 (· |b
ch , ph , ah ). Now consider any (b
ch , ph ) < Vh,ζ
(π ) and ah ∈ A. Note that due to the
2

h

,G
assumption on πh that takes uniform random actions after step h − L, it holds that Pπ
(b
ch , ph , ah ) =
h
h

h

,G
2 N0
),
Pπ
(b
ch , ph )Phπ ,G (ah |b
ch , ph ) ≥ ζA2 . By the Chernoff bound, with probability at least 1 − exp(− ζ8A
h
ζ2 N0
2
2
there are at least 2A trajectories indexed by the set K ⊆ [N0 ], such that for any k ∈ K ,
k
k
Compressh (fh (ak1:h−1 , o1:h
)) = b
ch , gh (ak1:h−1 , o1:h
) = ph , akh = ah . Again, with probability at least 1 − p′ ,
(D.14) and (D.15) hold as long as
1
ζ2 N0 C(O + log p′ )
≥
,
2A
θ22

for some constant C ≥ 1. By a union bound over all possible h ∈ [H], b
ch , ph , ah , (D.14) and (D.15) hold
with probability at least
bh Ph )A exp(− ζ2 N0 ) − H max(C
bh Ph )Ap′ .
1 − H max(C
8A
h
h
b P )A
4H max (C

h h h )
CA(O+log
δ1
δ1
.
Then
since
N
>
Now we set
=
, it holds that
2
0
bh Ph )A
ζ 2 θ2
4H maxh (C
ζ
N
δ
δ
bh Ph )A exp(− 2 0 ) ≤ 1 and H maxh (C
bh Ph )Ap′ ≤ 1 as long as the constant C is sufficiently
H maxh (C
4
4
8A
large. Therefore, we conclude that with probability at least 1 − δ21 , Equation (D.14) holds for all

p′

bh , ph ∈ Ph , ah ∈ A. Finally, by a union bound, we conclude the proof.
h ∈ [H], b
ch ∈ C
With the previous lemma, the next step is to bound the two important quantities in Definition 7.
f for M(π
f 1:H ), and M
c for M(π
c 1:H ).
In the following discussion, we will use the shorthand notation M
Lemma 17. Under the event E1 in Lemma 16, for any h ∈ [H], policy π ∈ ∆(Πdet ), and prescription γh ∈ Γh ,
it holds that
X f
c
EGa1:h−1 ,o1:h ∼π
PM,z
ch , γh ) − PM,z
ch , γh )
h (zh+1 |b
h (zh+1 |b
zh+1

ζ
A2L OL ζ1
≤ θ1 + 2APh 2 + APh θ2 +
+ 1[h > b
L] · 2 · d π,G b(U G b(πh )),
S,h−L φ,h−L
ζ1
φ
b b

(D.17)

M
M
EGa1:h−1 ,o1:h ∼π b
ri,h
(b
ch , γh ) −b
ri,h
(b
ch , γh )
f

c

A2L OL ζ1
ζ
+ 1[h > b
L] · 2 · d π,G b(U G b(πh )).
≤ θ1 + 2APh 2 + APh θ2 +
S,h−L φ,h−L
ζ1
φ
b b

(D.18)

Proof. It suffices to only consider π ∈ Πdet , since if the statement holds for any π ∈ Πdet , it will hold

58

low
for any π ∈ ∆(Πdet ) also. Under the event E1 , consider any b
ch < Ch,ζ
(πh ) and γh ∈ Γh :
1

X

ch , γh )
ch , γh ) − PM
PM
h (ph , ah , oh+1 |b
h (ph , ah , oh+1 |b
c

f

ph ,ah ,oh+1
h
,G
Pπ
(ph |b
ch )
h

X

=

ph ,ah ,oh+1
n
X Y

≤

n
Y

h
c
,G
ch )
γi,h (ai,h | pi,h )Pπ
(oh+1 |b
ch , ph , ah ) − PM
h (ph |b
h

i=1

n
Y

γi,h (ai,h | pi,h )PM,o
ch , ph , ah )
h (oh+1 |b
c

i=1
h

,G
ch ) +
(ph |b
ch ) − PM
γi,h (ai,h | pi,h ) Pπ
h (ph |b
h

ph ,ah ,oh+1 i=1
n
Y

c

h

h

,G
,G
γi,h (ai,h | pi,h )Pπ
(ph |b
ch ) Pπ
(oh+1 |b
ch , ph , ah ) − PM,o
ch , ph , ah )
h
h
h (oh+1 |b
c

i=1
X h
h
c
c
πh ,G
,G
,G
(·
|b
c
)∥
+
ch , ph , ah )∥1
≤ ∥Ph (· |b
ch ) − PM
Pπ
(ph |b
ch )∥Pπ
(· |b
ch , ph , ah ) − PM
h
1
h
h (· |b
h
h
ph ,ah





≤ 



X
h

X

+
ζ
1

ph :Phπ ,G (ph |b
ch )≤ ζ2

h



 X h
h
c

,G
,G
ch , ph , ah ) + Oθ1
Pπ
(ph |b
ch ) Pπ
(· |b
ch , ph , ah ) − PM

h (· |b
h
h

1
ζ2  ah

ph :Pπh ,G (ph |b
ch )> ζ

1

ζ
≤ θ1 + 2APh 2 + APh θ2 ,
ζ1
h

,G
low
ch , ph ) <
where the last inequality comes from the fact that if b
ch < Ch,ζ
(πh ) and Pπ
(ph |b
ch ) > ζζ2 , then (b
h
1

1

low h
(π ). Finally, for any policy π ∈ Πdet , by taking expectations over b
ch , we conclude that
Vh,ζ
2

EGa1:h−1 ,o1:h ∼π

X

ch , γh )
ch , γh ) − PM
PM
h (ph , ah , oh+1 |b
h (ph , ah , oh+1 |b
c

f

ph ,ah ,oh+1

ζ
π,G low
≤ θ1 + 2APh 2 + APh θ2 + 2 · dC,h
(Ch,ζ1 (πh ))
ζ1
A2L OL ζ1
ζ
≤ θ1 + 2APh 2 + APh θ2 +
+ 1[h > b
L] · 2 · d π,G b(U G b(πh )),
S,h−L φ,h−L
ζ1
φ
b b

where the last step comes from Lemma 15. By noticing that after marginalization, the total variation
will not increase, we proved the first inequality.

59

Similarly, for the approximate reward, it holds that
M
M
b
(b
ch , γh )
(b
ch , γh ) −b
ri,h
ri,h
c

f

X

=

h
Phπ ,G (ph |b
ch )

n
Y

ph ,ah

≤

πh
c
ch )
γi,h (ai,h | pi,h )ri,h1:h−1 (b
ch , ph , ah ) − PM
h (ph |b

n
Y

i=1

n
XY

M
(b
ch , ph , ah )
γi,h (ai,h | pi,h )b
ri,h
c

i=1
h

,G
ch ) +
(ph |b
ch ) − PM
γi,h (ai,h | pi,h ) Pπ
h (ph |b
h

ph ,ah i=1
n
Y

c

πh

h

,G
M
(b
ch , ph , ah )
ch , ph , ah ) −b
ri,h
γi,h (ai,h | pi,h )Pπ
(ph |b
ch ) ri,h1:h−1 (b
h
c

i=1
h

X

,G
ch )∥1 +
≤ ∥Pπ
(· |b
ch ) − PM
h (· |b
h
c

h

πh

,G
M
(b
ch , ph , ah )
ch , ph , ah ) −b
ri,h
Pπ
(ph |b
ch ) ri,h1:h−1 (b
h
c

ph ,ah





≤ 



X
h

X

+
ζ
1

ph :Pπh ,G (ph |b
ch )≤ ζ2

h



 X h
h
π1:h−1
c

,G
M
Pπ
(p
|b
c
)
r
(b
ch , ph , ah ) −b
ri,h
(b
ch , ph , ah ) + Oθ1

h
h
h
i,h

ζ2  ah

ph :Pπh ,G (ph |b
ch )> ζ

1

ζ
≤ θ1 + 2APh 2 + APh θ2 .
ζ1
Again, by taking expectations over b
ch , we proved the second inequality.
c 1:H ) through the intermediate
Finally, we are ready to prove Theorem 4 by relating G and M(π
1:H
f
M(π
).
f for M(π
f 1:H ) and M
c for M(π
c 1:H ). Note that
Proof of Theorem 4. In the following proof, we will use M
c it holds that
for ϵr (M),
c
M
G
c = max max EG
(b
ch , γh )|
ri,h
ϵr (M)
a1:h−1 ,o1:h ∼π |E [ri,h (sh , ah ) | ch , γh ] −b
i,h π∈Πdet ,γh

M
ri,h
(b
ch , γh )|
≤ max max EGa1:h−1 ,o1:h ∼π |EG [ri,h (sh , ah ) | ch , γh ] −b
f

i,h π∈Πdet ,γh

M
M
(b
ch , γh ) −b
ri,h
(b
ch , γh )|
+ max max EGa1:h−1 ,o1:h ∼π |b
ri,h
f

c

i,h π∈Πdet ,γh

≤ ϵr (π1:H ) + ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ),
c it holds that
where the last step comes from Lemma 17. Similarly, for ϵz (M),
c
M,z
G
c = max max EG
ϵz (M)
ch , γh )||1
a1:h−1 ,o1:h ∼π ||Ph (· | ch , γh ) − Ph (· |b
h

π∈Πdet ,γh

≤ max max EGa1:h−1 ,o1:h ∼π ||PGh (· | ch , γh ) − PM,z
ch , γh )||1
h (· |b
f

h

π∈Πdet ,γh

M,z
+ max max EGa1:h−1 ,o1:h ∼π ||PM,z
ch , γh )||1
h (· | ch , γh ) − Ph (· |b
f

h

c

π∈Πdet ,γh

≤ ϵz (π1:H ) + ϵapx (π1:H ,b
L, ζ1 , ζ2 , θ1 , θ2 , φ),
where the last step again comes from Lemma 17. Therefore, with Lemma 3 and Theorem 2, we
proved Theorem 8.
60

D.6

Proof of Theorem 5

f 1:H ) and G, which will necessarily
Until now, we have not considered the relationship between M(π
depend on the choice of approximate common information b
ch and π1:H . For planning, we have seen
how to construct an approximate common information b
ch using finite memory. Similarly, here we
f 1:H ) is a good approximation of G.
will also show how to construct b
ch with finite memory so that M(π
In the following discussions, we shall use another important policy-dependent approximate belief
apx,G
π,G
e
bπ
(·; dS,h−L
). We first introduce the following important lemmas.
h (·) := bh
Lemma 18. There is a constant C ≥ 1 so that the following holds. If Assumption 2 holds, then for any
1
ϵ, φ > 0, L ∈ N so that L ≥ Cγ −4 log( ϵφ
), it holds that for any policies π, π′ ∈ ∆(Πdet ),
 G

π′ ,G
EGπ′ bh (a1:h−1 , o1:h ) − e
bπ
h (ah−L:h−1 , oh−L+1:h ) 1 ≤ ϵ + 1[h > L] · 6 · dS,h−L Uφ,h−L (π) ,
 G

π′ ,G
(a
)
(π)
EGπ′ bh (a1:h−1 , o1:h−1 ) − e
bπ
≤
ϵ
+
1[h
>
L]
·
6
·
d
U
,
o
,
h−L:h−1
h−L+1:h−1
h
S,h−L
φ,h−L
1





π′ ,G
G
(π)
≤
ϵ
+
1[h
>
L]
·
6
·
d
U
bπ
a
,
o
,
o
.
EGπ′ bh a1:h−1 , o1:h−1 , o1,h − e
h−L:h−1
h−L+1:h−1
1,h
h
S,h−L
φ,h−L
1

Furthermore, for any finite domain Y , conditional probability q(y | s), and the posterior update operator
F q : ∆(S) → ∆(S) as defined in Lemma 12, it holds that
EGπ′ Ey∼q·bh (a1:h−1 ,o1:h ) ||F q (bh (a1:h−1 , o1:h ); y) − F q (b′h (ah−L:h−1 , oh−L+1:h ); y)||1 ≤ ϵ.
Proof. It directly follows from our Theorem 10, and Lemma 12.2 of Golowich et al. (2022a).
The lemma shows that if we use the d π,G b instead of a Unif(S) as the prior, the approximate belief
S,h−L

 G
π′ ,G
will suffer from an additional error term dS,h−L
Uφ,h−L (π) . The following lemma shows that there

 G
π′ ,G
already exists an efficient algorithm for finding π to minimize dS,h−L
Uφ,h−L (π) .
2

log(HSO/(αγ))
αγ
Lemma 19. Given α, β > 0, b
L≥C
, and φ = C 3 H 10 S 5 O4 for some constant C > 0. There
γ4
exists an algorithm BaSeCAMP (Algorithm 3 of Golowich et al. (2022a)) with both computation and sample
b
h,j ∈
complexity bounded by (OA)L log( β1 ), outputting K = 2HS groups of policies {π1:H,j }K
j=1 , where π
h,j
∆(Πdet ) and πh′ = Unif(A) for h′ ≥ h − b
L, j ∈ [K]. It holds that with probability at least 1 − β, there is at
⋆
b
least one j ∈ [K] such that for any h > L, policy π ∈ Πdet :

d π,G b(U G
S,h−L

(π
φ,h−b
L

h,j ⋆

)) ≤

α
.
CH 2

Proof. It follows from Theorem 3.1 in Golowich et al. (2022a).
By combining two previous lemmas, we can show the following corollary:
ϵγ 2

log(HSO/(ϵγ))

Corollary 4. Given ϵ, δ2 > 0, L ≥ C
, and φ = C 2 H 8 S 5 O4 for some constant C > 0. There
γ4
exists an algorithm BaSeCAMP (Algorithm 3 of Golowich et al. (2022a)) with both computation and sample
complexity bounded by
h,j ∈ ∆(Πdet ) and πh,j =
N1 = (OA)L log( δ1 ), outputting K = 2HS groups of policies {π1:H,j }K
j=1 , where π
h′
2
Unif(A) for h ∈ [H], h′ ≥ h − L, j ∈ [K]. The following event E2 holds with probability at least 1 − δ2 : there

61

is at least one j ⋆ ∈ [K] such that for any h > L, policy π′ ∈ ∆(Πdet ):
h,j ⋆


 G
⋆ 
π′ ,G
≤ ϵ + 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j ,
1
 G


h,j ⋆
π′ ,G
G
h,j ⋆
(a
)
Eπ′ bh (a1:h−1 , o1:h−1 ) − e
bπ
≤
ϵ
+
1[h
>
L]
·
6
·
d
U
π
,
o
,
h−L:h−1
h−L+1:h−1
h
S,h−L
φ,h−L
1






⋆
h,j ⋆
π′ ,G
G
πh,j ,
≤ ϵ + 1[h > L] · 6 · dS,h−L
Uφ,h−L
bπ
ah−L:h−1 , oh−L+1:h−1 , oi,h
EGπ′ bh a1:h−1 , o1:h−1 , oi,h − e
h
bπ
EGπ′ bh (a1:h−1 , o1:h ) − e
h

(ah−L:h−1 , oh−L+1:h )

1
π′ ,G
G
h,j ⋆
dS,h−L (Uφ,h−L (π )) ≤ ϵ.

2

Proof. Let α = CH2 ϵ , δ2 = β, and L ≥ max{C
leads to the conclusion.

1
log( ϵφ
)

γ4

log(HSO/(αγ))
}.
γ4

,C

Combining Lemmas 18 and 19
⋆

j
f for M(π
f 1:H,j ⋆ ) and M
c for M(π
c 1:H,j ⋆ ), and b
In the discussion thereafter, we will use M
ri,h for b
ri,h
interchangeably. There is still one issue unsolved, which is that BaSeCAMP does not tell us which
j ∈ [K] is the j ⋆ we want. Therefore, we have to evaluate the policies {π⋆,j }K
j=1 , which are generated by
1:H,j
c
running Algorithm 3 on the candidate models {M(π
)}j∈[K] . The policy evaluation and selection

algorithm is described in Algorithm 7.
Lemma 20. For Algorithm 7, suppose that the K groups of policies {π1:H,j }K
j=1 and K reward functions
j

⋆
{(b
ri )ni=1 }K
j=1 satisfy that there exists some j ∈ [K] such that for any policy π ∈ Π, i ∈ [n], we have
⋆

c 1:H,j )
π,M(π

π,G
Vi,1
(∅) − Vi,1

(∅) ≤ ϵ.

2

If N2 ≥ C
holds

H 2 log Kδ n
ϵ2

3

for some constant C > 0, then with probability at least 1 − δ3 , the following event E3
⋆

NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) + 6ϵ + Hϵe .
b

π

⋆,j,m

⋆,j

⋆,j

c 1:H,m )
×π ,M(π

c 1:H,m )
π ×π ,M(π

−i
Proof. For NE/CCE, note that Vi,1i
(∅) ≥ maxπi Vi,1i −i
(∅) − Hϵe according to
Corollary 1 for m ∈ [K]. By the concentration bound on the relationship between the accumulated
⋆,j,m
⋆,j
rewards and the value function for all policies π⋆,j , πi
× π−i , and further a union bound over all
i ∈ [n], j ∈ [K], and m ∈ [K], with probability at least 1 − δ3 , the following event E3 holds for any
i ∈ [n], j ∈ [K], m ∈ [K]:

⋆,j

j

j,m

π ,G
Ri − Vi,1
(∅) ≤ ϵ,

Ri

π

⋆,j,m

− Vi,1i

⋆,j

×π−i ,G

(∅) ≤ ϵ.
j,m

In the following proof, we will assume the previous event holds. Define m⋆i,j ∈ arg maxm Ri . Now we
j,m

will firstly show that maxm Ri
[K]:

⋆,j

approximates the best response of π−i . Note that for any i ∈ [n], j ∈

⋆,j

⋆,j

⋆,j,m⋆
i,j

π ×π ,G
π ×π ,G
π
j,m
max Vi,1i −i (∅) − max Ri ≥ max Vi,1i −i (∅) − Vi,1i
π
m
π
i

i

62

⋆,j

×π−i ,G

(∅) − ϵ ≥ −ϵ.

On the other hand,
⋆,j

π ×π−i ,G

max Vi,1i
πi

j,m

(∅) − max Ri

⋆,j

π ×π−i ,G

≤ max Vi,1i
πi

m

πi

⋆,j,m

⋆,j

×π−i ,G

m

⋆,j

π ×π−i ,G

≤ max Vi,1i

π

(∅) − max Vi,1i
π

⋆,j,m

(∅) − max Vi,1i

⋆

⋆,j

c 1:H,j )
×π−i ,M(π

m

⋆,j,j ⋆

⋆,j

π ×π ,G
π
≤ max Vi,1i −i (∅) − Vi,1i
π

(∅) + ϵ

⋆,j c 1:H,j ⋆
×π−i ,M(π
)

(∅) + 2ϵ

(∅) + 2ϵ

i

⋆,j

π ×π−i ,G

≤ max Vi,1i
πi

⋆

⋆,j

c 1:H,j )
π ×π−i ,M(π

(∅) − max Vi,1i
πi

(∅) + 2ϵ + Hϵe

≤ 3ϵ + Hϵe ,
where the second last step comes from Corollary 1 and the last step comes from the fact that the
b
max-operator is non-expansive. Now we are ready to evaluate π⋆,j :
!
⋆,b
j
b
πi ×π−i ,G
π⋆,j ,G
⋆,b
j
NE/CCE-gap(π ) = max max Vi,1
(∅) − Vi,1 (∅)
πi

i

!

⋆,j
b
b
b
π ×π ,G
j
j,m
j
Vi,1i −i (∅) − Ri + ϵ ≤ max max Ri − Ri + 4ϵ + Hϵe .
m
i
b

≤ max max
πi

i





⋆

Meanwhile for π⋆,j , we have that
NE/CCE-gap(π

⋆,j ⋆

⋆,j ⋆

) = max max
πi

i

⋆
π ×π ,G
π⋆,j ,G
Vi,1i −i (∅) − Vi,1
(∅)
⋆,j ⋆

π ×π ,G
j⋆
Vi,1i −i (∅) − Ri

≥ max max
πi
i


j ⋆ ,m
j⋆
≥ max max Ri − Ri − 2ϵ.

!

!
−ϵ

m

i




j,m
j
b
Recall the definition of b
j ∈ arg minj maxi maxm (Ri − Ri ) , we conclude that NE/CCE-gap(π⋆,j ) ≤
⋆

NE-gap(π⋆,j ) + 6ϵ + Hϵe .
For CE, note that
π

⋆,j,m

Vi,1i

⋆,j c 1:H,m
⊙π−i ,M(π
)

⋆,j
⋆,j c 1:H,m
(φ ⋄πi )⊙π−i ,M(π
)

(∅) ≥ max Vi,1 i
φi

(∅) − Hϵe .

Similarly, by a concentration bound and then a union bound, with probability at least 1 − δ3 , the
following event E3 holds for any i ∈ [n], j ∈ [K], m ∈ [K]:
⋆,j

j

j,m

π ,G
Ri − Vi,1
(∅) ≤ ϵ,

Ri

π

⋆,j,m

− Vi,1i

⋆,j

⊙π−i ,G

(∅) ≤ ϵ.
j,m

In the following proof, we will assume the previous event holds. Define m⋆i,j = arg maxm Ri . Now
j,m

⋆,j

we will firstly show that maxm Ri
Note that for any i ∈ [n], j ∈ [K]:

approximates the best strategy modification with respect to π−i .
⋆,j

⋆,j

(φ ⋄πi )⊙π−i ,G

max Vi,1 i

j,m

(∅) − max Ri
m

φi

⋆,j

⋆,j

⋆,j,m⋆
i,j

(φ ⋄π )⊙π−i ,G
π
≥ max Vi,1 i i
(∅) − Vi,1i
φ
i

≥ −ϵ.
63

⋆,j

⊙π−i ,G

(∅) − ϵ

On the other hand,
⋆,j

⋆,j

(φ ⋄πi )⊙π−i ,G

maxVi,1 i

j,m

(∅) − max Ri
m

φi

⋆,j

⋆,j

(φ ⋄πi )⊙π−i ,G

≤ max Vi,1 i

π

⋆,j,m

(∅) − max Vi,1i

⋆,j

⊙π−i ,G

m

φi

⋆,j

⋆,j

(φ ⋄πi )⊙π−i ,G

≤ max Vi,1 i

π

⋆,j,m

(∅) − max Vi,1i

(∅) + ϵ
⋆

⋆,j

c 1:H,j )
⊙π−i ,M(π

m

φi

⋆,j

⋆,j

(φ ⋄πi )⊙π−i ,G

≤ max Vi,1 i
φi

⋆,j

π

⋆,j,j ⋆

(∅) − Vi,1i

φi

⋆

⋆,j

c 1:H,j )
⊙π−i ,M(π

(∅) + 2ϵ

⋆,j
⋆,j c 1:H,j ⋆
(φ ⋄πi )⊙π−i ,M(π
)

⋆,j

(φ ⋄πi )⊙π−i ,G

≤ max Vi,1 i

(∅) + 2ϵ

(∅) − max Vi,1 i
φi

(∅) + 2ϵ + Hϵe

≤ 3ϵ + Hϵe ,
where the second last step comes from Corollary 2 and the last step comes from the fact that the
b
max-operator is non-expansive. Now we are ready to evaluate π⋆,j :
!
⋆,b
j
⋆,b
j
b
(φi ⋄πi )⊙π−i ,G
π⋆,j ,G
⋆,b
j
CE-gap(π ) = max max Vi,1
(∅) − Vi,1 (∅)
i

φi

!

⋆,j
⋆,j
b
(φ ⋄π )⊙π−i ,G
j
Vi,1 i i
(∅) − Ri + ϵ
b

b

≤ max max
i
φi


b
b
j,m
j
≤ max max Ri − Ri + 4ϵ + Hϵe .
m

i

Meanwhile for π⋆,j , we have that
b

CE-gap(π

⋆,j ⋆

⋆,j ⋆

) = max max
i

φi

(φ ⋄π
Vi,1 i i

⋆,j ⋆

(φ ⋄π
Vi,1 i i

⋆,j ⋆

)⊙π−i ,G
⋆,j ⋆

)⊙π−i ,G

⋆

π⋆,j ,G
(∅) − Vi,1
(∅)
j⋆
(∅) − Ri

≥ max max
i
φi


j ⋆ ,m
j⋆
≥ max max Ri − Ri − 2ϵ.
i

!

!
−ϵ

m



j,m

Recall the definition of b
j = arg minj maxi maxm (Ri


j
b
− Ri ) , we conclude that CE-gap(π⋆,j ) ≤

⋆

CE-gap(π⋆,j ) + 6ϵ + Hϵe .
We put together the entire learning procedure in Algorithm 9. Before diving into the examples
in Section 3, the proof for the first part of Theorem 8 follows from the fact that both the computation and sample complexities depend on maxh Ch and maxh Ph . Therefore, if we can find π1:H and
Compressh for h ∈ [H] such that the relevant errors are minimized while maxh Ch and maxh Ph are
of quasi-polynomial size, then there exists a quasi-polynomial sample and time algorithm learning
ϵ-NE if G is zero-sum or cooperative and ϵ-CE/CCE if G is general-sum. In the following discussion,
we will see the sample complexity of our algorithm instantiated with specific information structures.
One-step delayed information sharing. In this case, the information structure gives ch =
{a1:h−1 , o1:h−1 }, pi,h = {oi,h }, zh+1 = {oh , ah }. Fix L > 0, we define the approximate common information as b
ch = {ah−L:h−1 , oh−L+1:h−1 }. For any π1:H , where πh ∈ ∆(Πdet ) for h ∈ [H], it is direct to verify
that
f 1:H ),c
h
h
M(π
,G
Ph
(sh , ph |b
ch ) = Pπ
(sh , ph |b
ch ) = e
bπ
h (ah−L:h−1 , oh−L+1:h−1 )(sh )Oh (oh | sh ),
h
64

h
where we recall the definition of e
bπ
h in Section D.6. Meanwhile, according to Definition 10, it is direct
f to denote M(π
f 1:H,j ⋆ ) for short. Therefore, we
to verify that b
L = L. Hereafter in the proof, we use M
log(HSO/(ϵγ))
conclude that if L ≥ C
, by a union bound of the high probability event E1 in Lemma 16,
γ4
E2 in Corollary 4, and E3 in Lemma 20, with probability at least 1 − δ1 − δ2 − δ3 , it holds that for any
i ∈ [n]
⋆

ϵr (π1:H,j )
M
= max max EGa1:h−1 ,o1:h ∼π EG [ri,h (sh , ah ) | ch , γh ] −b
(b
ch , γh )
ri,h
f

i,h π∈Πdet ,γh

⋆

h,j
≤ max max EGa1:h−1 ,o1:h ∼π ∥bh (a1:h−1 , o1:h−1 ) − e
bhπ (ah−L:h−1 , oh−L+1:h−1 )∥1
h π∈Πdet

 G
⋆ 
π,G
≤ ϵ + max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j ,

h

π∈Πdet

and moreover
⋆

ch , γh )
ϵz (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π PGh (· | ch , γh ) − PM,z
h (· |b
f

π∈Πdet ,γh

h

≤ max max EGa1:h−1 ,o1:h ∼π bh (a1:h−1 , o1:h−1 ) − e
bπ
h
h

h,j ⋆

π∈Πdet ,γh

1

(ah−L:h−1 , oh−L+1:h−1 )

1

 G

⋆ 
π,G
≤ ϵ + max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j .
h

π∈Πdet

⋆

According to the choice π1:H,j , it holds that by Corollary 4
 G

⋆ 
π,G
max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j ≤ 6ϵ.
h

π

α
α
α
2
Therefore, for any α, δ > 0, setting ϵ = 200(H+1)
2 , θ1 = 200(H+1)2 O , ζ2 = ζ1 , θ2 = 200(H+1)2 A max P ,
h h


αφ
ϵγ 2
α
δ f 1:H,j ⋆
α
ζ1 = min 200(H+1)2 A2L OL , 400(H+1)2 A max P , φ = C 2 H 8 S 5 O4 , ϵe = 200H , δ1 = δ2 = δ3 = 3 , M(π
) is an
h h

14α
(ϵr , ϵz )-expected-approximate common information model of G, where ϵr , ϵz ≤ 200(H+1)
2 . This leads
⋆

c 1:H,j )
π,M(π

⋆

π,G
to that π⋆,j is a 15α
200 -NE/CE/CCE, and |Vi,1 (∅)−Vi,1

(∅)| ≤ 15α
200 for any policy π ∈ Π by Lemma
⋆

3. By Lemma 20, NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) + 91α
200 ≤ α. Finally, we are ready to
analyze the computation and sample complexities of our algorithm.
b

Theorem 16. Let α, δ, γ > 0. Algorithm 9 given a γ-observable POSG of one-step delayed information sharing structure outputs an α-NE if the POSG is zero-sum or cooperative, or α-CE/CCE if the
POSG is general-sum, with probability at least 1 − δ, with time and sample complexities bounded by
Cγ −4 log SHO
γα

log 1δ for some universal constant C > 0.


b
b
4H maxh C

h ) CA(O+log 4H maxh (Ch Ph A) ) 


C(max
P
+log
h h

δ1
δ1
bh ≤ (OA)L , Ph ≤ O, N0 = max 
Proof. Recall that C
,
, N =


2
2


ζ1 θ1
ζ2 θ2

 1
(AO)

2

H 2 log K n

δ3
(OA)L log( δ1 ), and N2 = C
for some constant C > 0, and we have set δ1 = δ2 = δ3 = 3δ . The
ϵ2
2
total number of samples used is KN0 + N1 + (K + nK 2 )N2 . Substituting the choices of parameters
into N0 , N1 , and N2 , we proved the sample complexity. Furthermore, for time complexity, since our
algorithm only calls the BaSeCAMP and our planning algorithm a polynomial number of times, the

Cγ −4 log SHO
γα

time complexity is also bounded by (OA)

65

log 1δ .

State controlled by one controller with asymmetric delay sharing. The information structure is
given as ch = {o1,1:h , o2,1:h−d , a1,1:h−1 }, p1,h = ∅, p2,h = {o2,h−d+1:h }. Fix some L > 0, the approximate
common information is constructed as b
ch := {o1,h−d−L+1:h , o2,h−d−L+1:h−d , a1,h−d−L:h−1 }. Then for any
1:H
h
det
given policy π , where π ∈ ∆(Π ), following exactly the same derivation as in Section D.4, it
holds that
f 1:H ),c
M(π

Ph

X

=

h

,G
(sh , ph |b
ch ) = Pπ
(sh , ph |b
ch )
h
h

PG (sh , ph | sh−d , fa , fo )F P (· | ·,fa ) (e
bπ
h−d (ah−d−L:h−d−1 , oh−d−L+1:h−d ); fo )(sh−d ).

sh−d

Meanwhile, it is direct to verify that b
L = L + d by Definition 10. Therefore, we conclude that if
log(HSO/(ϵγ))
, by a union bound of the high probability event E1 in Lemma 16, E2 in Corollary 4,
L≥C
γ4
and E3 in Lemma 20, with probability at least 1 − δ1 − δ2 − δ3 , it holds that for any i ∈ [n]:
⋆

ϵr (π1:H,j )
M
(b
ch , γh )
ri,h
= max max EGa1:h−1 ,o1:h ∼π EG [ri,h (sh , ah ) | ch , γh ] −b
f

i,h π∈Πdet ,γh

≤ + max max EGa1:h−1 ,o1:h ∼π F P (· | ·,fa ) (bh−d (a1:h−d−1 , o1:h−d ); fo )
π∈Πdet

h

h,j ⋆

− F P (· | ·,fa ) (e
bπ
h−d (ah−d−L:h−d−1 , oh−d−L+1:h−d ); fo ) 1



π,G
G
h,j ⋆
b
≤ ϵ + max max 1[h > L] · 6 · d
,
b U
b π
h

S,h−L

π∈Πdet

φ,h−L

and moreover
⋆

ϵz (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π PGh (· | ch , γh ) − PM,z
h (· | ch , γh )
f

π∈Πdet ,γh

h

1

≤ max max EGa1:h−1 ,o1:h ∼π′ F P (· | ·,fa ) (bh−d (a1:h−d−1 , o1:h−d ); fo )
h

π∈Πdet ,γh

h,j ⋆

− F P (· | ·,fa ) (e
bπ
h−d (ah−d−L:h−d−1 , oh−d−L+1:h−d ); fo ) 1



π,G
G
h,j ⋆
b
.
≤ ϵ + max max 1[h > L] · 6 · d
b U
b π
h

S,h−L

π∈Πdet

φ,h−L

⋆

According to the choice π1:H,j , it holds that by Corollary 4



⋆
max max 1[h > b
L] · 6 · d π,G b U G b πh,j ≤ 6ϵ.
h

S,h−L

π∈Πdet

φ,h−L

α
α
α
2
Therefore, for any α, δ > 0, setting ϵ = 200(H+1)
2 , θ1 = 200(H+1)2 O , ζ2 = ζ1 , θ2 = 200(H+1)2 A max P ,
h h


αφ
ϵγ 2
δ f 1:H,j ⋆
α
α
ζ1 = min 200(H+1)2 A2(L+d) OL+d , 400(H+1)2 A max P , φ = C 2 H 8 S 5 O4 , ϵe = 200H , δ1 = δ2 = δ3 = 3 , M(π
)
h h

14α
is an (ϵr , ϵz )-expected-approximate common information model of G, where ϵr , ϵz ≤ 200(H+1)
2 . This
⋆

c 1:H,j )
π,M(π

⋆

π,G
leads to that π⋆,j is a 15α
200 -NE/CE/CCE, and |Vi,1 (∅) − Vi,1

(∅)| ≤ 15α
200 for any policy π ∈ Π by
⋆

Lemma 3. By Lemma 20, NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) + 91α
200 ≤ α. Finally, we are
ready to analyze the computation and sample complexities of our algorithm.
b

Theorem 17. Let α, δ, γ > 0. Algorithm 9 given a γ-observable POSG of state controlled by one controller
with asymmetric delay sharing outputs an α-NE if the POSG is zero-sum or cooperative, or α-CE/CCE if
the POSG is general-sum, with probability at least 1 − δ, with time and sample complexities bounded by
C(γ −4 log SHO
γα +d)

(OA)

log 1δ for some universal constant C > 0.
66



b
b
4H maxh C

h ) CA(O+log 4H maxh (Ch Ph )A ) 


C(maxh Ph +log


δ
δ
1
1
bh ≤ (AO)L , Ph ≤ (AO)d , N0 = max 
,
Proof. Recall that C
,N =

2
2


ζ1 θ1
ζ 2 θ2
 1

K2n

2

H log δ
b
3
(OA)L log( δ1 ), and N2 = C
ϵ2
2

for some constant C > 0, and we have set δ1 = δ2 = δ3 = 3δ . The
total number of samples used is KN0 + N1 + (K + nK 2 )N2 . Substituting the choices of parameters into
N0 , N1 , and N2 , we proved the sample complexity. Furthermore, for time complexity analysis, since
our algorithm only calls the BaSeCAMP and our planning algorithm polynomial number of times, the
C(γ −4 log SHO
γα +d)

time complexity is also bounded by (OA)

log 1δ .

Information sharing with one-directional-one-step delay. For this case, we have ch =
{o1,1:h , o2,1:h−1 , a1:h−1 }, p1,h = ∅, p2,h = {o2,h }, and zh+1 = {o1,h+1 , o2,h , ah }. Fix L > 0, we construct
the approximate common information as b
ch = {o1,h−L+1:h , o2,h−L+1:h−1 , ah−L:h−1 }. For any π1:H , where
h
det
π ∈ ∆(Π ) for h ∈ [H], it is easy to verify that
h
h
,G
Pπ
(sh , ph |b
ch ) = e
bπ
h (o1,h−L+1:h , o2,h−L+1:h−1 , ah−L:h−1 )(sh )Ph (o2,h | sh , o1,h )
h

O (o

,o

|s )

h

1,h

2,h

where Ph (o2,h | sh , o1,h ) = P ′ hO 1,h(o 2,h,o′ h | s ) . Furthermore, it is direct to verify that b
L = L. Therefore, we
o
2,h

h

log(HSO/(ϵγ))
, by a union bound of the high probability event E1 in Lemma 16,
conclude that if L ≥ C
γ4

E2 in Corollary 4, and E3 in Lemma 20, with probability at least 1 − δ1 − δ2 − δ3 , it holds that for any
i ∈ [n]:
⋆

ϵr (π1:H,j )
M
(b
ch , γh )
= max max EGπ EG [ri,h (sh , ah ) | ch , γh ] −b
ri,h
f

i,h

π∈Πdet ,γ

h

⋆

h,j
≤ max max EGπ bh (a1:h−1 , o1:h−1 , o1,h ) − e
bhπ (ah−L:h−1 , oh−L+1:h−1 , o1,h )
det
1
h π∈Π
 G


π,G
h,j ⋆
≤ ϵ + max max 1[h > L] · 6 · dS,h−L Uφ,h−L π
.

h

π∈Πdet

Moreover, we have
⋆

ϵz (π1:H,j )
= max max EGπ PGh (· | ch , γh ) − PM,z
h (· | ch , γh )
f

h

π∈Πdet ,γh

≤ max max EGπ′ bh (a1:h−1 , o1:h−1 , o1,h ) − e
bπ
h
h

1

h,j ⋆

π∈Πdet ,γh

(ah−L:h−1 , oh−L+1:h−1 , o1,h )

1

 G

⋆ 
π,G
≤ ϵ + max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j .
h

π∈Πdet

⋆

According to the choice π1:H,j , it holds that by Corollary 4:
 G

⋆ 
π,G
max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j ≤ 6ϵ.
h

π∈Πdet

α
α
α
2
Therefore, for any α, δ > 0, setting ϵ = 200(H+1)
2 , θ1 = 200(H+1)2 O , ζ2 = ζ1 , θ2 = 200(H+1)2 A max P ,
h h


αφ
ϵγ 2
α
α
δ f 1:H,j ⋆
ζ1 = min 200(H+1)2 A2L OL , 400(H+1)2 A max P , φ = C 2 H 8 S 5 O4 , ϵe = 200H , δ1 = δ2 = δ3 = 3 , M(π
) is an
h h

14α
(ϵr , ϵz )-expected-approximate common information model of G, where ϵr , ϵz ≤ 200(H+1)
2 . This leads
⋆

⋆

c 1:H,j )
π,M(π

π,G
to that π⋆,j is a 15α
200 -NE/CE/CCE, and |Vi,1 (∅)−Vi,1

67

(∅)| ≤ 15α
200 for any policy π ∈ Π by Lemma

⋆

3. By Lemma 20, NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) + 91α
200 ≤ α. Finally, we are ready to
analyze the computation and sample complexities of our algorithm.
b

Theorem 18. Let α, δ, γ > 0. Algorithm 9 given a γ-observable POSG of one-directional-one-step delayed
information sharing structure outputs an α-NE if the POSG is zero-sum or cooperative, or α-CE/CCE if
the POSG is general-sum, with probability at least 1 − δ, with time and sample complexities bounded by
Cγ −4 log SHO
γα

log 1δ for some universal constant C > 0.


b
b
4H maxh C

h ) CA(O+log 4H maxh (Ch Ph A) ) 


C(max
P
+log
h
h

δ1
δ1
bh ≤ (OA)L , Ph ≤ O, N0 = max 
Proof. Recall that C
,
, N =


2
2


ζ
θ
ζ
θ
1 1
2 2

 1
(AO)

2

H 2 log K n

δ3
for some constant C > 0, and we have set δ1 = δ2 = δ3 = 3δ . The
(OA)L log( δ1 ), and N2 = C
ϵ2
2
total number of samples used is KN0 + N1 + (K + nK 2 )N2 . Substituting the choices of parameters into
N0 , N1 , and N2 , we proved the sample complexity. Furthermore, for time complexity analysis, since
our algorithm only calls the BaSeCAMP and our planning algorithm polynomial number of times, the

Cγ −4 log SHO
γα

time complexity is also bounded by (OA)

log 1δ .

Uncontrolled state process with delayed sharing. The information structure gives that ch =
{o1:h−d }, pi,h = {oi,h−d+1:h }, and zh+1 = {oh−d+1 }. Fix a L > 0, the approximate common information
is b
ch = {oh−d−L+1:h−d }. For any policy π1:H , where πh ∈ ∆(Πdet ) for h ∈ [H], it is easy to verify that
X h
f 1:H ),c
h
M(π
,G
e
bπ
Ph
(sh , ph |b
ch ) = Pπ
(s
,
p
|b
c
)
=
h
h
h
h−d (oh−d−L+1:h−d )(sh−d )P(sh , oh−d+1:h | sh−d ).
h
sh−d

Furthermore, it is direct to verify that b
L = L + d by Definition 10. Therefore, we conclude that if
log(HSO/(ϵγ))
L≥C
,
by
a
union
bound
of
the
high probability event E1 in Lemma 16, E2 in Corollary 4,
γ4
and E3 in Lemma 20, with probability at least 1 − δ1 − δ2 − δ3 , it holds that for any i ∈ [n]:
⋆

M
(b
ch , γh )
ϵr (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π EG [ri,h (sh , ah ) | ch , γh ] −b
ri,h
f

i,h

π∈Πdet ,γ

h

⋆

h,j
≤ max max EGa1:h−1 ,o1:h ∼π bh−d (o1:h−d ) − e
bπ
(oh−d−L+1:h−d )
h−d
det
1
h π∈Π



⋆
π,G
G
h,j
≤ ϵ + max max 1[h > b
L] · 6 · d
.
b π
b U

h

φ,h−L

S,h−L

π∈Πdet

Moreover, we also have
⋆

ϵz (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π PGh (· | ch , γh ) − PM,z
h (· | ch , γh )
f

π∈Πdet ,γh

h

1

h,j ⋆

≤ max max EGa1:h−1 ,o1:h ∼π′ bh−d (o1:h−d ) − e
bπ
h−d (oh−d−L+1:h−d )
π∈Πdet ,γh

h



≤ ϵ + max max 1[h > b
L] · 6 · d π,G b U G
h

S,h−L

π∈Πdet

1



h,j ⋆
.
π
b

φ,h−L

⋆

According to the choice π1:H,j , it holds that by Corollary 4:



⋆
max max 1[h > b
L] · 6 · d π,G b U G b πh,j ≤ 6ϵ.
h

S,h−L

π∈Πdet

φ,h−L

α
α
α
2
Therefore, for any α, δ > 0, setting ϵ = 200(H+1)
2 , θ1 = 200(H+1)2 O , ζ2 = ζ1 , θ2 = 200(H+1)2 A max P ,
h h


αφ
ϵγ 2
α
α
δ f 1:H,j ⋆
ζ1 = min 200(H+1)2 A2(L+d) OL+d , 400(H+1)2 A max P , φ = C 2 H 8 S 5 O4 , ϵe = 200H , δ1 = δ2 = δ3 = 3 , M(π
)
h h

68

14α
is an (ϵr , ϵz )-expected-approximate common information model of G, where ϵr , ϵz ≤ 200(H+1)
2 . This
⋆

c 1:H,j )
π,M(π

⋆

π,G
leads to that π⋆,j is a 15α
200 -NE/CE/CCE, and |Vi,1 (∅) − Vi,1
Lemma 3. By Lemma 20,

⋆

(∅)| ≤ 15α
200 for any policy π by

NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) +
b

91α
≤ α.
200

Finally, we are ready to analyze the computational and sample complexities of our algorithm.
Theorem 19. Let α, δ, γ > 0. Algorithm 9 given a γ-observable POSG of uncontrolled state process and
delayed information sharing structure outputs an α-NE if the POSG is zero-sum or cooperative, or αCE/CCE if the POSG is general-sum, with probability at least 1 − δ, with time and sample complexities
C(γ −4 log SHO
γα +d)

log 1δ for some universal constant C > 0.


b
b
4H maxh C

h ) CA(O+log 4H maxh (Ch Ph )A ) 


C(maxh Ph +log


δ1
δ1
bh ≤ OL , Ph ≤ Od , N0 = max 
,
Proof. Recall that C
, N1 =

2
2


ζ
θ
ζ
θ
1 1
2 2



bounded by (OA)

2

H 2 log K n

δ3
for some constant C > 0, and we have set δ1 = δ2 = δ3 = 3δ . The
(OA)L log( δ1 ), and N2 = C
ϵ2
2
total number of samples used is KN0 + N1 + (K + nK 2 )N2 . Substituting the choices of parameters into
N0 , N1 , and N2 , we proved the sample complexity. Furthermore, for time complexity analysis, since
our algorithm only calls the BaSeCAMP and our planning algorithm polynomial number of times, the

b

C(γ −4 log SHO
γα )

time complexity is also bounded by (OA)

log 1δ .

Symmetric information game. For symmetric information game, ch = {o1:h , a1:h−1 }, pi,h = ∅,
and zh+1 = {ah , oh+1 }. Fix L > 0, we construct the approximate common information as b
ch =
{oh−L+1:h , ah−L:h−1 }. For any π1:H , where πh ∈ ∆(Πdet ) for h ∈ [H], it is easy to verify that
f 1:H ),c
M(π

Ph

h

h

,G
(sh , ph |b
ch ) = e
bπ
(sh , ph |b
ch ) = Pπ
h (ah−L:h−1 , oh−L+1:h )(sh ).
h

Meanwhile, it is direct to verify that b
L = L by Definition 10. Therefore, we conclude that if L ≥
log(HSO/(ϵγ))
C
, by a union bound of the high probability event E1 in Lemma 16, E2 in Corollary 4, and
γ4
E3 in Lemma 20, with probability at least 1 − δ1 − δ2 − δ3 , it holds that for any i ∈ [n]:
⋆

M
ri,h
(b
ch , γh )
ϵr (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π EG [ri,h (sh , ah ) | ch , γh ] −b
f

i,h π∈Πdet ,γh

h,j ⋆

≤ ϵ + max max EGa1:h−1 ,o1:h ∼π bh (a1:h−1 , o1:h ) − e
bhπ (ah−L:h−1 , oh−L+1:h )
1
h π∈Πdet
 G

⋆ 
π,G
≤ 2ϵ + max max 1[h > L] · 6 · dS,h−L Uφ,h−L πh,j .
h

π∈Πdet

Moreover, we have
⋆

ϵz (π1:H,j ) = max max EGa1:h−1 ,o1:h ∼π PGh (· | ch , γh ) − PM,z
h (· | ch , γh )
f

π∈Πdet ,γh

h

≤ max max EGa1:h−1 ,o1:h ∼π′ bh (a1:h−1 , o1:h ) − e
bπ
h
h

π∈Πdet ,γh

h,j ⋆

(ah−L:h−1 , oh−L+1:h )

 G

⋆ 
π,G
≤ ϵ + max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j .
h

π∈Πdet
⋆

According to the choice π1:H,j , it holds that by Corollary 4
 G

⋆ 
π,G
max max 1[h > L] · 6 · dS,h−L
Uφ,h−L πh,j ≤ 6ϵ.
h

π∈Πdet

69

1

1

α
α
α
2
Therefore, for any α, δ > 0, setting ϵ = 200(H+1)
2 , θ1 = 200(H+1)2 O , ζ2 = ζ1 , θ2 = 200(H+1)2 A max P ,
h h


αφ
ϵγ 2
α
δ f 1:H,j ⋆
α
) is an
ζ1 = min 200(H+1)2 A2L OL , 400(H+1)2 A max P , φ = C 2 H 8 S 5 O4 , ϵe = 200H , δ1 = δ2 = δ3 = 3 , M(π
h h

14α
(ϵr , ϵz )-expected-approximate common information model of G, where ϵr , ϵz ≤ 200(H+1)
2 . This leads
⋆

c 1:H,j )
π,M(π

⋆

π,G
to that π⋆,j is a 15α
200 -NE/CE/CCE, and |Vi,1 (∅)−Vi,1

(∅)| ≤ 15α
200 for any policy π ∈ Π by Lemma
⋆

3. By Lemma 20, NE/CE/CCE-gap(π⋆,j ) ≤ NE/CE/CCE-gap(π⋆,j ) + 91α
200 ≤ α. Finally, we are ready to
analyze the computation and sample complexities of our algorithm.
b

Theorem 20. Let α, δ, γ > 0. Algorithm 9 given a γ-observable POSG of symmetric information sharing
structure outputs an α-NE if the POSG is zero-sum or cooperative, or α-CE/CCE if the POSG is generalCγ −4 log SHO

γα log 1
sum, with probability at least 1 − δ, with time and sample complexities bounded by (AO)
δ
for some universal constant C > 0.


b
b
4H maxh C

h ) CA(O+log 4H maxh (Ch Ph )A ) 


C(maxh Ph +log


δ1
δ1
bh ≤ (OA)L , Ph = 1, N0 = max 
,
Proof. Recall that C
, N =

2
2


ζ
θ
ζ
θ
1 1
2 2
 1

2

H 2 log K n

δ3
for some constant C > 0, and we have set δ1 = δ2 = δ3 = 3δ . The
(OA)L log( δ1 ), and N2 = C
ϵ2
2
total number of samples used is KN0 + N1 + (K + nK 2 )N2 . Substituting the choices of parameters into
N0 , N1 , and N2 , we proved the sample complexity. Furthermore, for time complexity analysis, since
our algorithm only calls the BaSeCAMP and our planning algorithm polynomial number of times, the

C(γ −4 log SHO
γα )

time complexity is also bounded by (OA)

D.7

log 1δ .

Missing details in Section 6

Now we prove Proposition 2, where the hardness follows from the hardness of the one-step DecPOMDP in Proposition 4.
Proof of Proposition 2. Note that for Equation (6.1), if we take the underlying Dec-POMDP G to be
H = 1, n = 2 without any information-sharing, and the approximate belief is constructed to be
the ground-truth belief of the underlying Dec-POMDP G, the optimal prescription solved by Equation (6.1) is then exactly the optimal policy of the underlying G. By the hardness from Proposition 4,
we conclude that solving Equation (6.1) is also NP-hard.
Proposition 11. Given any approximate common information model M that is consistent with a belief
bh , γh ∈ Γh
ch′ )}h′ ∈[H] , if Condition 1 holds, we have for any h ∈ [H], b
ch ∈ C
{PM,c
h′ (sh′ , ph′ |b
X
Qh⋆,M (b
ch , γh ) =
Uj,h (b
ch , γj,h ),
(D.19)
j∈[n]

for some functions {Uj,h }j∈[n] . Correspondingly, Equation (6.1) can be solved exactly in time complexity
poly(S, A, Ph ).

70

Proof of Proposition 11. By the definition of Qh⋆,M (b
ch , γh ) and Definition 8, it holds that
Qh⋆,M (b
ch , γh )
X
=

PM,c
ch )
h (sh , ph |b

sh ,ph ,ah ,sh+1 ,oh+1

n
Y

γi,h (ai,h | pi,h )Th (sh+1 | sh , actt(h),h )

i=1

h
i
⋆,M
Oh+1 (oh+1 | sh+1 ) rh (sh , ah ) + Vh+1
(b
ch+1 )
X X
h
i
=
PM,c
(s
,
p
|b
c
)γ
(a
|
p
)
r
(s
,
a
)
h h h j,h j,h j,h
j,h h j,h +
h
j,ctt(h) sh ,ph ,aj,h

X

PM,c
ch )γctt(h),h (actt(h),h | pctt(h),h )Th (sh+1 | sh , actt(h),h )Oh+1 (oh+1 | sh+1 )
h (sh , ph |b

sh ,ph ,actt(h),h ,sh+1 ,oh+1

h
i
⋆,M
× rctt(h),h (sh , actt(h),h ) + Vh+1
(b
ch+1 )
:=

X

Uj,h (b
ch , γj,h )

j∈[n]

bh+1 (b
where the last step is due to the assumption that that b
ch+1 = φ
ch , zh+1 ) and zh+1 =
χh+1 (ph , actt(h),h , oh+1 ). Now, to solve Equation (6.1), we only need to optimize w.r.t. each γj,h for
j ∈ [n] individually, which is a linear program with the constraint set of γj,h to be a concatenation
of simplex by Proposition 8. Hence, Equation (6.1) can be solved even exactly in time complexity
poly(S, A, Ph ).
⋆
Proposition 12. Suppose Condition 2 holds, Algorithm 10 returns γ1:n,h
such that
⋆
γ1:n,h
∈ arg max Qh⋆,M (b
ch , γ1,h , · · · , γn,h ),
γ1,h ,··· ,γn,h

with time complexity poly(Ph , A, S).
Proof of Proposition 12. We slightly abuse our notation for the Qh⋆,M as below to define for any ui ∈
Ui := {(×ij=1 Pj,h ) × (×i−1
j=1 Aj ) → ∆(Ai )} and i ∈ [n] that
ch , u1 , · · · , un )
Qh⋆,M (b
n
X
Y
h
i
⋆,M
M,c
:=
Ph (sh , ph |b
ch )
ui (ai,h | p1:i,h , a1:i−1,h )Th (sh+1 | sh , ah )Oh+1 (oh+1 | sh+1 ) rh (sh , ah ) + Vh+1
(b
ch+1 ) .
sh ,ph ,ah ,sh+1 ,oh+1

i=1

⋆
By the standard result of value iteration for POMDPs, we have that u1:n
is an optimal policy for the
b
POMDP P (n) in the sense that

Qh⋆,M (b
ch , u1⋆ , · · · , un⋆ ) =

max Qh⋆,M (b
ch , u1 , · · · , un ) ≥

{ui ∈Ui }i∈[n]

max

{γi,h ∈Γi,h }i∈[n]

Qh⋆,M (b
ch , γi,h , · · · , γn,h ),

where the inequality comes from the fact that any γi,h ∈ Γi,h can be realized by an equivalent ui ∈ Ui
such that the value is the same. Meanwhile, due to the nested information-sharing structure, for any
⋆
⋆
ph ∈ Ph , it holds that u1:n
and γ1:n,h
outputs the same action deterministically according to the second
for-loop of Algorithm 10. Hence, we conclude that
⋆
⋆
ch , γ1,h
, · · · , γn,h
) = Qh⋆,M (b
ch , u1⋆ , · · · , un⋆ ),
Qh⋆,M (b

71

⋆
returned by Algorithm 10 is an exact solution of Equation (6.1).
which further concludes that γ1:n,h
b(n), which is Qn Pi,h Ai,h =
Finally, the time complexity scales with the size of the history space of P
i=1
APh . The additional polynomial dependency on S comes from computing the posterior distribution
for the initialization step in Algorithm 10.

Proposition 13. Suppose Condition 3 holds. For each h ∈ [H], there exist n functions {Fi,h }i∈[n] such that
Qh⋆,M (b
ch , γ1,h , · · · , γn,h ) =

n
X

Fi,h (b
ci,h , γi,h ).

(D.20)

i=1

Correspondingly, Equation (6.1) can be solved in time

P

i∈[n] poly(Si , Ai , Pi,h ).

Proof of Proposition 13. We prove our result by backward induction on h. Obviously, it holds for
h = H + 1. Now suppose the proposition holds for h + 1. For step h, it holds that
Qh⋆,M (b
ch , γ1,h , · · · , γn,h )
n
X
Y
=
PM,c
(s
,
p
|b
c
)
γj,h (aj,h | pj,h )Th (sh+1 | sh , ah )
h h h
h
sh ,ph ,ah ,sh+1 ,oh+1

j=1


 n

X
⋆
ri,h (si,h , ai,h ) + Fi,h+1 (b
ci,h+1 , γi,h+1 (b
ci,h+1 ))
Oh+1 (oh+1 | sh+1 ) 
i=1

=

n
X

X

PM,c
ci,h )γi,h (ai,h | pi,h )Th (si,h+1 | si,h , ai,h )
i,h (si,h , pi,h |b

i=1 si,h ,pi,h ,ai,h ,si,h+1 ,oi,h+1

h
i
⋆
Oi,h+1 (oi,h+1 | si,h+1 ) ri,h (si,h , ai,h ) + Fi,h+1 (b
ci,h+1 , γi,h+1
(b
ci,h+1 ))
:=

n
X

Fi,h (b
ci,h , γi,h ),

i=1
⋆
where for the first equality, we defined γi,h+1
(b
ci,h+1 ) ∈ arg maxγi,h+1 ∈Γi,h+1 Fi,h+1 (b
ci,h+1 , γi,h+1 ), thus proving the decomposition. Therefore, to solve Equation (6.1), it suffices to optimize each Fi,h (b
ci,h , γi,h )
individually w.r.t. γi,h , which is a linear program with the concatenation of simplex as the constraint
P
by Proposition 8. Thus, the time complexity is ni=1 poly(Si , Ai , Pi,h ).

Remark 3. In fact, under Condition 3, Algorithm 3 and its time complexity can be further improved,
where for each h ∈ [H], we do not necessarily need to enumerate all possible joint approximate common
information b
ch , but only the individual approximate common information b
ci,h for each i ∈ [n]. This allows
P
bh Ph , thus not
b
the final time complexity to depend only on maxh∈[H] i∈[n] Ci,h Pi,h instead of maxh∈[H] C
suffering from the exponential dependency on the number of agents anymore.
b⋆ , i.e., the return of Algorithm 3 with the
Proof of Theorem 6. The first step is to show that π
equilibrium-computation subroutine replaced as Equation (6.1) is a near-optimal policy for the underlying Dec-POMDP G. To begin with, for any policy π ∈ Π, we shall prove inductively that for any
h ∈ [H], ch ∈ Ch that
b⋆
Vhπ,M (ch ) ≤ Vhπ ,M (b
ch ).
It is direct to verify that the inequality holds for h = H + 1. Now suppose it holds for step h + 1. For

72

step h, note that
π,M
Vhπ,M (ch ) = E{ωj,h }j∈[n] EM [b
rhM + Vh+1
(ch+1 ) |b
ch , {πj,h (· | ωj,h , ch , ·)}j∈[n] ]
b⋆

π ,M
(b
ch+1 ) |b
ch , {πj,h (· | ωj,h , ch , ·)}j∈[n] ]
≤ E{ωj,h }j∈[n] EM [b
rhM + Vh+1
b⋆

π ,M
⋆
≤ EM [b
rhM + Vh+1
(b
ch+1 ) |b
ch , {b
πj,h
(· |b
ch , ·)}j∈[n] ]
⋆

= Vhπ ,M (b
ch ),
where the first inequality is by inductive hypothesis, and the second inequality is due to
b⋆ ,M
π
⋆,M
⋆
(· |b
ch , ·)}j∈[n] is a solution of Equation (6.1). Now under the groundVh+1
(b
ch+1 ) = Vh+1
(b
ch+1 ) and {b
πj,h
truth model G, for any π ∈ Π, h ∈ [H], ch ∈ Ch , by Lemma 3, it holds that
b⋆

b⋆

V1π,G (∅) − V1π ,G (∅) ≤ V1π,M (∅) − V1π ,M (∅) + 2(Hϵr + H 2 ϵz ) ≤ 2(Hϵr + H 2 ϵz ).
bh
To analyze the time complexity, we observe that Algorithm 3 needs to solve Equation (6.1) for C
times for each h ∈ [H]. Therefore, if Equation (6.1) can be solved with time complexity poly(S, A, Ph )
bh × poly(S, A, Ph ).
for each h ∈ [H], the total time complexity of Algorithm 3 is H maxh∈[H] C
Now we are ready to instantiate the guarantees for the examples in Section 3. Specifically, it is
direct to verify that Example 2 and Example 5 together with the approximate belief constructed in
Section D.4 satisfy Condition 1 (turned-based structures), while Example 3 and Example 5 together
with the approximate belief constructed in Section D.4 satisfy Condition 2 (the nested informationsharing structure). Therefore, by Proposition 11 and Proposition 12, Equation (6.1) can be solved
with time complexity poly(S, A, Ph ) for each h ∈ [H], and the total time complexity of planning
bh · poly(S, A, Ph , H).
such a 2(Hϵr + H 2 ϵz )−team-optimal solution for the Dec-POMDP is maxh∈[H] C
Finally, by Theorem 7, for all examples in Section 3, there exists an approximate model M such
bh Ph is only quasi-polynomial of the problem instance size.
that max{ϵr , ϵz } ≤ O( Hϵ2 ), while maxh C
Hence, the time complexity for planning the ϵ-team-optimal solution for those examples is also
quasi-polynomial.
For Example 1, i.e., the one-step delayed sharing case, if we additionally assume the Part (1) of
Condition 3 (factorized structures) holds, the approximate belief we constructed in Section D.4 also
satisfies the Part (2) of Condition 2. Thus, by the improved algorithm and guarantees in Remark 3
bi,h × poly(Si , Ai , Pi,h , H). Meanwhile,
and Proposition 13, the total time complexity is n maxi∈[n],h∈[H] C
bi,h Pi,h ≤
by our construction of the approximate belief, we can ensure max{ϵr , ϵz } ≤ O( Hϵ2 ), while C
4

(Ai Oi )O(log(SH/ϵ)/γ ) . Therefore, the total time complexity of planning the ϵ-team-optimal solution is
4
n(Ai Oi )O(log(SH/ϵ)/γ ) , without suffering from the exponential dependency on n.

(Quasi-)Efficient learning in Dec-POMDPs without model knowledge. Based on such planning
algorithms, we are ready to extend our MARL algorithm to the Dec-POMDP setting for finding the
team optimum. Specifically, we only need to replace line 4 of Algorithm 9, i.e., planning for equilibria of the POSG with the planning algorithm for the team-optimal solution of the Dec-POMDP
discussed above. Meanwhile, the line 7 of Algorithm 9 for policy selection (Algorithm 7) can be
greatly simplified, where we can directly choose
b
j ← arg max Rj ,
j∈[K]

i.e., the policy with the highest empirical rewards. For completeness, we provided the modified
policy selection algorithm in Algorithm 8. Meanwhile, Algorithm 5 of learning the approximate

73

c 1:H ) also needs to ensure that planning in the learned M(π
c 1:H ) is computationally quasimodel M(π
c 1:H ) by
efficient. Specifically, Equation (6.1) needs to be solved computationally efficiently for M(π
enforcing that Condition 1, 2, or 3 holds. This can be done by slightly adjusting Algorithm 5 as in
the proof of the following theorem.
Theorem 21. Fix ϵ, δ > 0. Under Assumption 2, for the one-step delayed sharing example under the
assumption of Part (1) in Condition 3 and all the other information-sharing structure examples in Section 3, there exists a multi-agent RL algorithm that learns an ϵ-team optimal solution with probability at
least 1 − δ, with both quasi-polynomial time and sample complexities.
To prove Theorem 21, the major step is to prove the correctness of the simplified policy selection
procedure, i.e., the counterpart of Lemma 20 for the Dec-POMDP setting.
Lemma 21. Fix ϵ, δ3 > 0. For Algorithm 7, suppose that the K groups of policies {π1:H,j }K
j=1 satisfy that
there exists some j ⋆ ∈ [K] such that for any policy π ∈ Π, we have
⋆

c 1:H,j )
π,M(π

V1π,G (∅) − V1

(∅) ≤ ϵ.

2

If N2 ≥ C

H 2 log Kδ n
ϵ2

3

for some constant C > 0, then with probability at least 1 − δ3 , it holds that
V1π

⋆,b
j

,G

(∅) ≥ max V1π,G (∅) − 4ϵ.
π∈Π

Proof. By the concentration bound on the accumulated rewards of policies π⋆,j , and further a union
bound over all j ∈ [n], with probability at least 1 − δ3 , the following event E3 holds for any j ∈ [K]:
Rj − V1π

⋆,j

,G

(∅) ≤ ϵ.

Therefore, it holds that
V1π

⋆,b
j

,G

⋆

(∅) ≥ Rj − ϵ ≥ Rj − ϵ ≥ V1π
b

⋆,j ⋆

,G

(∅) − 2ϵ.

Meanwhile, by denoting π⋆ ∈ arg maxπ∈Π V1π,G (∅), we have
V1π

⋆,j ⋆

,G

⋆

⋆

c 1:H,j )
π⋆,j ,M(π

⋆

(∅) − V1π ,G (∅) ≥ V1

⋆

c 1:H,j )
π⋆ ,M(π

(∅) − V1

(∅) − 2ϵ ≥ −2ϵ,

⋆
c 1:H,j ⋆ ). Therefore, we
where the last step is due to the fact that π⋆,j is the optimal policy of M(π

conclude that that V1π

⋆,b
j

,G

(∅) ≥ V1π

⋆,j ⋆

,G

(∅) − 2ϵ ≥ maxπ∈Π V1π,G (∅) − 4ϵ.

Finally, we are ready to prove Theorem 21.
Proof of Theorem 21. The correctness of the extended learning algorithm follows similarly as the
proof of Theorem 8, where for any α, δ > 0, under the exactly the same choices of all parameters
(cf. Section D.6) as for learning the equilibrium, with probability 1 − δ, there exists j ⋆ ∈ [K] such
⋆

c 1:H,j )
π,M(π

that |V1π,G (∅) − V1

π
(∅)| ≤ 15α
200 for any π ∈ Π. Now by Lemma 21, it holds that V1

⋆,b
j

,G

(∅) ≥

π,G
60α
⋆,b
j is an α-team-optimal solumaxπ∈Π V1π,G (∅) − 60α
200 ≥ maxπ∈Π V1 (∅) − 200 , thus concluding that π

tion. For the sample complexity, since the choice of all parameters remains the same as that for
learning the equilibrium, the sample complexity remains the same as for learning the equilibrium,
i.e., quasi-polynomial.
For the time complexity, as we mentioned above, we need to adjust Algorithm 5 to ensure Equation (6.1) can be solved computationally efficiently for the learned model. Specifically,
74

• For Example 2 and Example 5 that satisfy Condition 1, it suffices to estimate
c 1:H )
M(π

c 1:H )
M(π

(b
ch , ph , ai,h ) for each i ∈ [n] instead of the original
1:H
1:H
c
c
M(π )
M(π )
(b
ch , ph , ah ). Then Equation (B.1) and Equation (B.2) in Algo(oh+1 |b
ch , ph , ah ), b
ri,h
Ph
Ph

(oh+1 |b
ch , ph , actt(h) ), b
ri,h

rithm 5 can be replaced as follows:
c 1:H ),z
M(π

Ph

X

(zh+1 |b
ch , γctt(h),h ) ←

1[χh+1 (ph , actt(h),h , oh+1 ) = zh+1 ]

ph ,actt(h),h ,oh+1
c 1:H )
M(π

c 1:H )
M(π

b
ri,h

(b
ch , γi,h ) ←

× Ph
X

c 1:H )
M(π

(ph |b
ch )γctt(h),h (actt(h),h | pctt(h),h )Ph

c 1:H )
M(π

Ph

c 1:H )
M(π

(ph |b
ch )γi,h (ai,h | pi,h )b
ri,h

(oh+1 |b
ch , ph , actt(h),h )

(b
ch , ph , ai,h ).

ph ,ai,h

With the modified construction, it is direct to verify that Equation (D.19) of Proposition 11 still
c 1:H ). Thus, one can solve Equation (6.1) computationally efficiently.
holds for M(π
• For Example 3 and Example 4 that satisfy Condition 2, Algorithm 5 requires no modification
since the learned model automatically satisfies Condition 2.
• For Example 1 under the assumption of Part (1) in Condition 3, it suffices to estimate
c 1:H )
M(π

c 1:H )
M(π

c 1:H )
M(π

(b
ci,h , pi,h , ai,h ) separately for each i ∈ [n]
1:H
1:H
c 1:H )
c
c
M(π
M(π )
M(π )
(b
ch , ph , ah ). Then Equa(oh+1 |b
ch , ph , ah ), b
ri,h
(ph |b
ch ), Ph
instead of the original Ph
Ph

(pi,h |b
ci,h ), Ph

(oi,h+1 |b
ci,h , pi,h , ai,h ), b
ri,h

tion (B.1) and Equation (B.2) can be replaced as follows for each i ∈ [n]
c 1:H ),z
M(π

Ph

X

(zi,h+1 |b
ci,h , γi,h ) ←

1[χi,h+1 (pi,h , ai,h , oi,h+1 ) = zi,h+1 ]

pi,h ,ai,h ,oi,h+1
c 1:H )
M(π

c 1:H )
M(π

(oi,h+1 |b
ci,h , pi,h , ai,h )
(pi,h |b
ci,h )γi,h (ai,h | pi,h )Ph
× Ph
X
c 1:H )
c 1:H )
c 1:H )
M(π
M(π
M(π
b
ri,h
(b
ci,h , γi,h ) ←
Ph
(pi,h |b
ci,h )γi,h (ai,h | pi,h )b
ri,h
(b
ci,h , pi,h , ai,h ).
pi,h ,ai,h

With the modified construction, it is direct to verify that Equation (D.20) of Proposition 13 still
c 1:H ). Thus, one can solve Equation (6.1) computationally efficiently.
holds for M(π
Now, since we have called the planning algorithm (i.e., Algorithm 3) only polynomial times, the total
time complexity is also quasi-polynomial by Theorem 6.

75

