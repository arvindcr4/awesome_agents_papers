Here is the extracted information in the requested format:

PAPER: Rational_Policy_Gradient.pdf
TITLE: Robust and Diverse Multi-Agent Learning via Rational Policy Gradient
RESEARCH_METHOD: 02_rlhf_alignment
METHOD_DESCRIPTION: The paper introduces Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational. To solve RPO, the authors develop Rational Policy Gradient (RPG), a gradient-based method that trains agents to maximize their own reward in a modified version of the original game.
KEY_CONTRIBUTIONS:
* Introduced Rationality-preserving Policy Optimization (RPO) to avoid self-sabotage in adversarial optimization
* Developed Rational Policy Gradient (RPG) to solve RPO
* Applied RPG to various adversarial optimization algorithms, including Adversarial Policy, Adversarial Training, PAIRED, and Adversarial Diversity
* Demonstrated the effectiveness of RPG in finding rational adversarial examples, training robust agents, and learning diverse policies in multiple environments.
