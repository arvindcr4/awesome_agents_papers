Multi-Agent Training for Pommerman: Curriculum
Learning and Population-based Self-Play Approach

arXiv:2407.00662v2 [cs.MA] 8 Jan 2025

Nhat-Minh Huynh1,∗ Hoang-Giang Cao2,3,∗ I-Chen Wu4,†
1
Agoda (Booking Holdings Inc.), Thailand
2
Ming Chi University of Technology, Taiwan
3
Can Tho University, Vietnam
4
National Yang Ming Chiao Tung University, Taiwan

Abstract: Pommerman is a multi-agent environment that has received considerable attention from researchers in recent years. This environment is an ideal
benchmark for multi-agent training, providing a battleground for two teams with
communication capabilities among allied agents. Pommerman presents significant challenges for model-free reinforcement learning due to delayed action effects, sparse rewards, and false positives, where opponent players can lose due
to their own mistakes. This study introduces a system designed to train multiagent systems to play Pommerman using a combination of curriculum learning
and population-based self-play. We also tackle two challenging problems when
deploying the multi-agent training system for competitive games: sparse reward
and suitable matchmaking mechanism. Specifically, we propose an adaptive annealing factor based on agents’ performance to adjust the dense exploration reward during training dynamically. Additionally, we implement a matchmaking
mechanism utilizing the Elo rating system to pair agents effectively. Our experimental results demonstrate that our trained agent can outperform top learning
agents without requiring communication among allied agents.
Keywords: Multi-agent, Reinforcement Learning, Curriculum Learning, Selfplay, Population-based

1

Introduction

In 1983, Bomberman was released for the Nintendo Entertainment System (NES) platform. In 2018,
Pommerman [1] was introduced; it is a well-organized open-source environment that is implemented
in Python. This version enhances the original Bomberman with a battle scenario where players are
allowed to communicate in a team.
Pommerman is an appropriate multi-agent benchmark because the environment offers various challenges, including multiple agent interaction, sparse rewards, false-positive rewards, delayed action
effects, and complex exploration [2]. There are four agents in a game, either playing in 2vs2 team
mode or individually competing against each other in Free-For-All mode. The sparse reward is a
problem related to the time that an agent receives a reward. In Pommerman, a game reward of win
or loss is given to each agent at the end of an episode, which can take up to 800 timesteps. The
false-positive reward occurs when rewards are not derived from appropriate actions by an agent but
from the opponent’s mistake of committing suicide. Another challenging problem in Pommerman
is the delay action effect. In the game, the only way to explore the board and eliminate an enemy is
by placing bombs, but the effect of bomb placement does not produce a direct result but delays 10
timesteps. In addition, as the vision of an agent is limited to a 9x9 grid around it, the environment
∗

Equal contribution.

†

Corresponding.

1st Workshop on Game AI Algorithms and Multi-Agent Learning (GAMMAL - IJCAI 2024), South Korea.

turns into a partially observable environment, which makes it hard for an agent to explore the board
or find enemies.
In this study, we introduce a multi-agent training system to play the 2vs2 team mode of Pommerman.
Our system includes two stages: curriculum learning and population-based self-play. The curriculum learning, consisting of three incremental difficulty phases, assists the agent in learning essential
skills to handle the games, such as exploring the map, picking up items, hiding from an explosion,
and using defensive strategies to stay safe and survive while simultaneously fighting to eliminate
enemies. Once agents acquire those necessary skills, we design a population-based self-play system
wherein a population of agents compete against each other, naturally evolving their own strategies
to improve performance. Figure 1 shows the overview of our system.
Deploying a multi-agent self-play training system for a competitive game poses two challenges [3]:
(1) addressing exploration problems in a competitive game with sparse reward; and (2) designing a
suitable matchmaking mechanism for effective agent pairing in training, enabling progressive learning. To address the exploration problem with sparse reward, we introduce an adaptive annealing
factor, which dynamically anneals the dense exploration reward during training based on the agent’s
performance. To design a suitable matchmaking, we implement the matchmaking probability based
on the Elo rating system, ensuring incremental learning.
The main contributions of this paper are summarized as follows: 1) We present a multi-agent training
system to play Pommerman, which includes two stages: curriculum learning and population-based
self-play. 2) We propose an adaptive annealing factor based on agents’ performance to dynamically
adjust the dense exploration reward during training. 3) We implement a matchmaking mechanism
utilizing the Elo rating system to pair agents effectively 4) In our experiments, we demonstrate that
our trained agent can outperform learning agents, even without requiring communication among
allied agents.

Figure 1: Overview of our multi-agent training system with two stages: curriculum learning and populationbased self-play.

2

2

Approaches

2.1

Curriculum Learning Stage

2.1.1

Curriculum Learning with Incremental Difficulty Agents

Pommerman is a complex environment that requires multiple skills to handle the game. To encourage the agent to acquire different skills, we designed a curriculum learning with three phases:
exploring the map and locating opponents, eliminating opponents, and surviving encounters while
fighting opponents. Three phases correspond to three rule-based agents (inspired by Chao Gao et al.
[4]), denoted as static agent, simple moving agent and simple bomb agent.
In the first phase, the training agent plays with the static agent, who idle and are waiting to be
eliminated by our agent. In this phase, the agent learns how to explode wooden walls to open up the
passages, pick up items, hide from an explosion of its own bombs, and finally, eliminate the static
opponents.
In the second phase, the training agent plays with simple moving agent who move randomly
on the board but are not allowed to place bombs. Furthermore, the opponents are programmed to
dodge the bomb explosion safely. As the difficulty increases, our agent is required to find strategies
to place bombs more effectively.
In the third phase, the simple bomb agent are able to move and place bombs randomly. In this
scenario, approaching the opponents becomes more challenging for our agent. Therefore, our agent
is required to learn defensive strategies to stay safe and survive while simultaneously fighting to
eliminate enemies.
The next phase of the curriculum learning stage is activated when the training agent achieves a 55%
win rate over the current rule-based agent.
2.1.2

Adaptive Exploration Reward by Performance

The main objective of Pommerman is to eliminate the opponents. However, players are initially located in the corners of the map and are blocked by wooden walls and stones. Thus, the gameplay can
be divided into two subtasks: (1) explore the map to navigate to the opponent; and (2) strategically
place bombs to engage in fighting and eliminate them. These two sub-tasks correspond to two types
of reward: exploration reward and game reward, denoted as e and R, respectively. The details of
reward setting are detailed in C.
There is a conflict between using exploration rewards and game rewards. The dense exploration
reward is necessary at the beginning of training to encourage the agent to explore the map and open
up the passages. However, using dense exploration rewards distracts the purpose of the self-evolving
strategy, which benefits from the sparse game reward. Conversely, directly using the sparse game
reward is excessively challenging for agents to learn gameplay effectively due to the complexity of
exploration in the game environment.
To solve this problem, a work [3] calculated the reward function with an annealing factor, denoted
as α, to linearly reduce the dense exploration reward to zero during the training. In the work [3], the
reward function at timestep t is defined as follows:
rt = αt et + (1 − αt )R

(1)

where rt is the total reward agent received, et is the exploration reward, R is a game reward, and αt
is the annealing factor, controlling the impact of et and R. Generally, deciding when the annealing
factor should equal zero is a manual fine-tuning process.
To dynamically adjust the annealing factor α for adapting to strategy changes during the training,
we propose an adaptive annealing factor based on the agent’s performance as follows:
α = 1 − tanh(k ∗ x)
3

(2)

where k is the tuning parameter, and x is the current performance of an agent. In our work, an
agent’s performance is measured by the average number of deaths of the enemy so that x is in the
range of [0, 2] and k is set to 1.2 to obtain a suitable curve of tanh function (shown in Figure 2).
At the beginning of training, the agent is not able to approach and eliminate the opponent; the
current performance of an agent x = 0. Thus, the exploration reward fully impacts the reward,
motivating the agent to place bombs to explore the map. Subsequently, the agent eventually locates
the opponents to eliminate them, increasing the performance. As performance increases, the impact
of the dense exploration rewards and sparse game rewards shifts. This encourages the agent to
gradually prioritize self-evolving strategies aligned with the primary game objective.
After the curriculum learning stage, the dense exploration reward annealed to zero, entirely replaced
by the sparse game reward. So, in the self-play stage, the agent is trained only on game rewards.

Figure 2: Annealing factor α function with k = 1.2. Noted annealing factor only calculated in the range of [0,
2], which is the part in red square

2.2
2.2.1

Population-based Self-play Stage
Population-based Self-play System

After training in the curriculum learning stage, the agent knows the properties of the Pommerman
environment and learns the essential skills to handle the game. To improve the agent’s strength, we
designed a population-based self-play system where a population of agents plays against each other
for self-improvement.
We designed a population-based self-play system with eight agents. Three agents are rule-based
agents used in the curriculum learning stage, to prevent from forgetting learned skills The remaining
five learning-based agents are initialized by the trained agent from the curriculum learning stage.
During the self-play training, if an agent has a win rate below 45% over the population, it is considered to be replaced by a stronger agent. A stronger agent is selected randomly from the four
remaining agents. Figure 1 shows the design of our population-based self-play stage.
To ensure effective progressive learning during self-play, selecting an appropriate opponent for pairing is crucial. In the next subsection, we will introduce a matchmaking mechanism based on Elo
rating to tackle this problem.
2.2.2

Match Making Probability

During the self-play stage, each agent gains experience by interacting with other agents. Therefore,
implementing a suitable matchmaking system is necessary to effectively pair agents for training,
ensuring they facilitate progressive learning, adapt to new strategies introduced by other agents, and
avoid getting stuck [3].
4

In this research, we implement the matchmaking probability based on the Elo rating system. The
Elo rating system is commonly used in board games like Chess and Go [5]. In competitive games,
the Elo rating is adopted to measure players’ strength and calculate the win rate of a match between
two players.
Let the Elo ratings of agent A and agent B be denoted as RA and RB , respectively. The formula to
calculate the expected win rate of Agent A against Agent B is defined as follows:
1
(3)
EA =
1 + 10(RB −RA )/400
Then, after a match between agent A and agent B, the Elo rating of agent A is updated as follows:
′
RA
= RA + K(SA − EA )

(4)

where K is the maximum adjustment per game, and SA is the actual score of agent A after a match:
1, 0.5, or 0 if the match results in a win, tie, or loss, respectively.
In a multi-agent training system, the matchmaking probability of other agents to a selected agent
is calculated by applying the softmax function to the expected win rate between those agents and
the selected one. As a result, a higher Elo rating agent has a higher chance of being selected as an
opponent of the current training agent.
For example, a population-based self-play system with 4 agents, A1, A2, A3, and A4, with 1010,
1020, 920, and 986 Elo points corresponding. If agent A4 is the current training agent to play a
match, the expected win rate of A1, A2, and A3 are 0.53, 0.54, and 0.4 (Equation 3). We then apply
the soft-max function to these expected win rates to obtain the matchmaking probabilities of agents
A1, A2, and A3 as follows: 0.346, 0.35, and 0.304.

3

Experiment Results

All training agents are implemented using the actor-critic algorithm with Proximal Policy Optimization (PPO) [6]. The network architecture is detailed in Appendix D.
3.1

Curriculum Learning Stage

As described in Section 2.1, in this stage, the agent will learn essential skills by training with three
different rule-based agents in three phases. The adaptive exploration reward by performanc is also
applied in this stage.
In the beginning, the agent does not know how to eliminate the opponents, so the annealing factor α
in Equation 1 is equal to 0. In this way, the agent prefers to do actions to explore the area as well as
pick up items while it is not punished by the negative reward of accidental suicide.
After about 5 million timesteps, the agent is able to locate and eliminate static agent, reaching
the win rate of 55%. Then, the second phase with simple moving agent of curriculum learning
is activated. Due to the increase in difficulty in the second phase, the performance of the agent
quickly drops. Following the decrease in performance is the increase in the annealing factor, which
forces the agent to pick up items and place more bombs to find better strategies to eliminate the
enemies.
Finally, in the third phase, the simple bombs agent starts to place bombs randomly, making
it harder for the training agent to approach the opponent. As a result, the training agent learned
defensive strategies to effectively dodge the bomb’s explosion while simultaneously threatening and
eliminating opponents.
The two Figure 3 and Figure 4 illustrate the contrast between the annealing factor and the performance of the training agent.
In comparison with the performance annealing factor, we train another model with a linear annealing
factor to show the difference between the two methods. Figure 5 illustrates the average number of
5

Figure 3: Average of Enemy Deaths.

Figure 4: Exploration reward annealing during the training process.

Figure 5: Average number of Enemy deaths using linear annealing factor.

6

enemy deaths together with the linear annealing factor. After around 5 million timesteps, the training
agent can also finish the first phase and activate the second phase, the same as in the performance
of the adaptive exploration reward method. In the next phase, the performance of the training drops
immediately. The problem is that the annealing factor cannot adjust to encourage the training agent
to keep exploring. Ultimately, the training agent takes a long time to play against the second rulebased agent and cannot pass the second phase. Note that adjusting the value of the linear annealing
factor can lead to improved results. However, it will introduce another problem of manually finetuning the hyperparameters.
After the curriculum learning stage, the training learned useful behaviors such as exploding wood
walls, picking up revealed items, dodging, hiding from the explosion, and finding enemies. Also,
the dense exploration reward is annealed to zero. The reward function in the self-play stage now
contains only sparse game rewards, allowing the training agent to self-develop their strategy aligned
with the main game objective.
3.2

Self-play Stage

As described in Section 2.2, the self-play stage utilizes a population-based training system with eight agents: three rule-based agents (static agent, simple move agent,
simple bombs agent), and five training agents (initialized from the trained agent in the curriculum stage). We evaluate the agent using the Elo rating. Initially, every agent is given 1000 Elo
rating points and continuously updates after every match. This Elo rating point is also used to obtain
matchmaking probability between two agents, using the mechanism detailed in Subsection 2.2.2,
which ensures the selection of suitable opponents for facilitating progressive learning and avoids
getting stuck.

Figure 6: Elo rating of five training agents during the self-play training stage.

Figure 6 demonstrates the Elo rating of our training agent during the self-play training stage. After
the curriculum stage, our agents start with 1040 Elo rating points. The Elo rating keeps increasing
gradually to 1160 after 200 million training timesteps.
After finishing self-play stage training, we arranged 100 round-robin matches to evaluate all methods, including a baseline agent developed by Pommerman, nine agents from the 2018 and 2019
competitions, and our agent. As shown in Figure 7, our method has 982 Elo rating points, which is
greater than the top two learning agents in 2018 and a robust rule-based agent (Neoteric in 2019).
Besides, our Elo rating almost equals to dypm, a tree search-based agent.
We also compare the win rate of our agent against other agents. As shown in Table 1, our agent
has a win rate of 98.85% against the baseline of Pommerman, which is a heuristic agent using
the Dijkstra algorithm to find the opponents and place bombs next to enemies. Also, our agent
7

Figure 7: Evaluating Elo rating of the agents with 100 round-robins matches.

Our win rate

simple (baseline)
98.85%

skynet
96.23%

navocado
88.33%

neoteric
87.69%

Table 1: Win rate of our method against other learning agents

Our win rate

dypm
30.00%

eisenach
8.89%

hakozaki
5.17%

gorogm
4.35%

inspir
4.00%

thing1
1.75%

Table 2: Win rate of our method against tree-search-based agents and communicated agents.

outperforms the top learning agents in 2018, Skynet955 and Navocado, with 96.23% and 88.33%
win rate, respectively. Furthermore, we defeated Neoteric - a ranked 4th agent in 2019, using robust
rule-based strategies. Our agent is defeated when facing tree-search-based agents or methods that
have communication in their team (shown in Table 2), which is out of scope in this study.

4

Conclusions

This research introduces a training system for multi-agent learning in Pommerman with two stages:
curriculum learning and population-based self-play. After the curriculum learning stage, the agent
learned essential skills such as placing bombs to explode wooden walls and picking up revealed
items. After the self-play stage, the agents’ strength is improved, and they autonomously learn a
more effective and secure strategy, which includes strategic behaviors such as kicking bombs toward enemies or trapping enemies with bombs. We also address two challenges when deploying
a multi-agent self-play training system for competitive games: sparse reward and suitable matchmaking. Specifically, we propose an adaptive annealing factor based on agents’ performance to dynamically adjust the dense exploration reward, gradually prioritizing the game reward. Furthermore,
we implement a matchmaking mechanism utilizing the Elo rating system to pair agents effectively,
ensuring incremental learning during the self-play training. Finally, without communication, our
trained agent is able to defeat the top learning agent and the top four rule-based agents in the 2019
competition.

8

References
[1] C. Resnick, W. Eldridge, D. Ha, D. Britz, J. Foerster, J. Togelius, K. Cho, and J. Bruna. Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124, 2018.
[2] C. Gao, B. Kartal, P. Hernandez-Leal, and M. E. Taylor. On hard exploration for reinforcement
learning: A case study in pommerman. CoRR, 2019.
[3] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch. Emergent complexity via multiagent competition. CoRR, abs/1710.03748, 2017. URL http://arxiv.org/abs/1710.03748.
[4] C. Gao, P. Hernandez-Leal, B. Kartal, and M. E. Taylor. Skynet: A top deep rl agent in the
inaugural pommerman team competition. CoRR, 2019. URL http://arxiv.org/abs/1905.01360.
[5] A. E. Elo. The Rating of Chessplayers, Past and Present. Arco Publishing, New York, 1978.
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–
1780, Nov 1997. URL https://doi.org/10.1162/neco.1997.9.8.1735.

9

A

The Pommerman

A.1

Game Description

In Pommerman, there are four agents, and each agent is placed in four corners. The board size is
11 x 11. It is symmetric along the main diagonal. The board is scattered with three types of cells:
wooden walls, rigid walls, and passages (see Figure 8).

Figure 8: Initial game board, where four agents are located at four corners.

In general, each agent can place a bomb that will explode after 10 timesteps later. Flames are created
after a bomb explodes, which is in a cross shape with a size of 1 cell around the exploded bomb.
The flames eliminate any agents, wooden walls, and items if they hit the objects and explode other
bombs on collision. Furthermore, agents cannot go through bombs and obstacles. Wooden walls,
after destruction, may reveal items for agents to pick up. There are three types of pick-up items,
which are the extra bomb (increases the agent’s ammunition by one), increased range (increases the
size of bomb explosion of an agent by one), and kick-ability (allows the agent to kick bombs) (see
Figure 9).

Figure 9: Game board with bombs, flames, pick-up items.

A.2

Game Modes

There are two unique modes in Pommerman: Free-For-All mode (FFA) and team mode.
In the first mode, all agents in a battle fight against each other, and the last survival wins the game.
Team mode is a 2-versus-2 setting where one team needs to eliminate the other team. Also, the
board is partially observable because the view of the agent is limited in a 9x9 grid around the agent.
The allied agents are allowed to communicate with each other by sending a list of two numbers in
the range of 1 to 8. If at least one member of each team survives at the end of the battle, the result
will be a tie.
10

B

State Representation

Pommerman internal states are represented into 2D features and scalar features. The game is partial
observation, with a limit observation size of 9x9 around the player position, so the 2D features are
spatially related features size 9x9 with the center being the player position.
Scalar features present some information about the game, like ammunition, bomb blast straight, and
equipped item ability.
Figure 10 is an example of a map of a 4x4 grid. The figure represents passages and rigid walls in
binary features for any available cell. Besides, the bomb blast strength is 4, and the bomb life is 2,
which illustrates the strength and time-to-explode of the bomb on the board. In particular, a state in
every step is represented by a 9x9 grid around the agent which is the same as the limited vision of
the agent. The cells outside of the board are filled with rigid walls. The state represented in this way
is more compact and helps an agent focus on surrounding objects and speed up the training process.

Figure 10: Example of 2D features.

Table 3 and Table 4 are 2D features and scalar features used in our research.
Features
Passage
Rigid walls
Wooden walls
Bombs
Flames
Enemy position
Current position
Teammate position
Item – Increase range
Item – Extra bomb
Item – Kick
Bomb blast strength
Bomb life

Describe
Indicate the passege path
Indicate the position of rigid walls
Indicate the position of wooden walls
Indicate the position of bombs are placed
Indicate the position of flames
Indicate the position of enemy
Indicate the position of current player
Indicate the position of teammate
Indicate the position of Item - Increase range
Indicate the position of Item - Extra bomb
Indicate the position of Item-Kick
The blast strength of the placed bombs.
The remaining time before the bombs explode.

Table 3: 2D features (size 9x9) of Pommerman

Features
Position
Ammunition
Bomb blast strength
Kick ability
Teammate alive
Two opponents

Shape
2
1
1
1
1
1

Describe
2D coordinate of the current player
The available number of bombs to place.
The blast strength of the bomb when exploding.
Indicate that the player has kick-ability or not
Indicate that teammate is alive or not
Indicate the number of opponents.

Table 4: Scalar features of Pommerman

11

C

Reward System

The main objective of Pommerman is to eliminate the opponents. However, players are initially
located in the corners of the map and are blocked by wooden walls and stones. Thus, the gameplay
can be divided into two subtasks: exploring the map to navigate to opponents and engaging in
combat to eliminate them.
Corresponding to the two subtasks, we define two reward types: exploitation reward et and game
reward R. The exploration reward et is received at timestep t to encourage the agent to place bombs
to explore the map and uncover the items hidden behind the wooden walls. The game reward R is
received at the end of the game based on the game results (win, tie or loss), motivating the agent to
engage in combat and eliminate opponents. The reward system for training is defined as follows:
Exploration reward et
Game reward R

Pick up items (blast strength, ammo, kick)
Placing bomb
At the end of episode
Death
Tie

+0.1
+0.005
+1/death enemy
-1
-1

Table 5: Reward system for training

D

Network Architecture and Training Parameters

All training agents are implemented using the actor-critic algorithm with Proximal Policy Optimization (PPO). Our network architecture is conducted from 4 convolutional layers, which have kernel
sizes of 3x3, stride 1, padding 0, and are activated by a ReLU function. At the end of the final convolutional layer, the output is flattened into 256 units and concatenated with scalar features. After that,
the extracted features were put into the Long Short Term Memory (LSTM) [7] layer to remember
the information in a sequence of 10 steps. The output of the LSTM layer is a fully connected layer
of 128 units, which splits into two heads: value head and policy head. A value head estimates the
value of the current state, and a policy head outputs the probabilities of 6 actions (stay, up, down,
left, right, bomb) of the agent.
Figure 11 visualizes our network structure used in this research.

Figure 11: Network structure.

12

