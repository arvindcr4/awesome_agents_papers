Here is the extracted information:

**PAPER**: R-MADDPG for Partially Observable Environments and Limited Communication
**TITLE**: R-MADDPG for Partially Observable Environments and Limited Communication
**ARXIV_ID**: 2002.06684v2
**RESEARCH_METHOD**: 03_multi_agent_rl
**METHOD_DESCRIPTION**: This paper proposes a recurrent multi-agent actor-critic model (R-MADDPG) for handling multi-agent coordination under partially observable environments and limited communication. The model learns two policies in parallel, one for physical navigation and another for communication, and uses recurrency to gather partial observations and minimize differences in system behavior.
**KEY_CONTRIBUTIONS**:
* Introduction of R-MADDPG, a deep recurrent multi-agent actor-critic framework for handling multi-agent coordination under partial observability and limited communication.
* Demonstration of the importance of recurrency in learning representations capable of estimating the true state of the environment from partial observations.
* Empirical comparison between R-MADDPG and other architectures, highlighting the importance of a recurrent critic in partially observable settings.
* Open-sourced implementation of R-MADDPG.
* Investigation of the effects of recurrency on performance and communication use in multi-agent environments.
