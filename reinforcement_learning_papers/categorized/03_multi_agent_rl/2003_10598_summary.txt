Here is the extracted information in the requested format:

**PAPER:** "Multi-Agent Reinforcement Learning for Problems with Combined Individual and Team Reward"
**TITLE:** Multi-Agent Reinforcement Learning for Problems with Combined Individual and Team Reward
**ARXIV_ID:** 2003.10598v1
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** The authors propose a novel cooperative multi-agent reinforcement learning framework called Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG). This framework learns to maximize both the global and local rewards simultaneously without creating an entangled multi-objective reward function. DE-MADDPG uses a dual-critic approach, where a global critic estimates the cumulative global reward and a local critic estimates the cumulative local reward for each agent.
**KEY_CONTRIBUTIONS:**
* The authors develop a dual-critic framework for multi-agent reinforcement learning that learns to simultaneously maximize the decomposed global and local rewards.
* They apply performance enhancement techniques such as Prioritized Experience Replay (PER) and Twin Delayed Deep Deterministic Policy Gradients (TD3) to tackle the overestimation bias problem in Q-functions.
* They evaluate their proposed solution on the defensive escort team problem and show that it achieves significantly better and more stable performance than the direct adaptation of the MADDPG algorithm.
