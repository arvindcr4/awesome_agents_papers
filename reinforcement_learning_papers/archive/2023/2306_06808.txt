Multi-Agent Reinforcement Learning Guided by
Signal Temporal Logic Specifications

arXiv:2306.06808v2 [cs.AI] 22 Oct 2023

Jiangwei Wang1 , Shuo Yang2 , Ziyan An3 , Songyang Han1 , Zhili Zhang1 ,
Rahul Mangharam2 , Meiyi Ma3 , Fei Miao1
Abstractâ€” Reward design is a key component of deep reinforcement learning (DRL), yet some tasks and designerâ€™s
objectives may be unnatural to define as a scalar cost function.
Among the various techniques, formal methods integrated
with DRL have garnered considerable attention due to their
expressiveness and flexibility to define the reward and requirements for different states and actions of the agent. However,
how to leverage Signal Temporal Logic (STL) to guide multiagent reinforcement learning (MARL) reward design remains
unexplored. Complex interactions, heterogeneous goals and
critical safety requirements in multi-agent systems make this
problem even more challenging. In this paper, we propose a
novel STL-guided multi-agent reinforcement learning framework. The STL requirements are designed to include both
task specifications according to the objective of each agent
and safety specifications, and the robustness values of the STL
specifications are leveraged to generate rewards. We validate the
advantages of our method through empirical studies. The experimental results demonstrate significant reward performance
improvements compared to MARL without STL guidance,
along with a remarkable increase in the overall safety rate
of the multi-agent systems.

blue autonomous vehicles want to drive forward and pass
through the only open lane (right lane in the figure) as soon
as possible while keeping a safety distance between each
other and the broken-down vehicles. Additionally, the time
length which autonomous vehicles remain blocked after the
broken-down vehicles should be constrained below a certain
threshold. To make the vehicles learn to fulfill all these
requirements, designing a reward function may take lots of
trials and can be computationally expensive. Regarding the
safety requirements in reinforcement learning [4], [5], [6],
using penalty in reward to discourage the unsafe actions, or
framing the problem into a constrained optimization problem
may not provide sufficient safety assurances for the selected
actions. Furthermore, it is even more challenging to encode
a reward function considering temporal requirements for the
agents [7], [8].

I. INTRODUCTION
Multi-agent reinforcement learning (MARL) have gained
significant research interests in solving various sequential
decision-making problems for multi-agent systems, especially with the rapid development of deep reinforcement
learning (DRL). It is a key component of an MARL algorithm to define a reward function that maps each state
and action to some real-valued reward [1]. However, define
or encode a scalar reward function according to the desired
behavior and objectives considering the dynamic interactions
and typically heterogeneous goals of a multi-agent system
remains challenging. Moreover, for MARL that involves both
the interactions of the agents and the physical dynamic
process of each individual agent, poorly designed reward
functions can lead to undesired policies that are unable to
accomplish the tasks, or worse, execute unsafe actions in
safety-critical systems [2], [3].
A motivating example of decision-making for multi-agent
system named Traffic-jam is shown in Fig. 1. In this case,
red broken-down vehicles stopped on streets and blocked
three lanes (due to an accident or other reasons), the three
1 Jiangwei Wang, Songyang Han, Zhili Zhang and Fei Miao are with
University of Connecticut {jiangwei.wang, songyang.han,

zhili.zhang, fei.miao}@uconn.edu
2 Shuo Yang and Rahul Mangharam are with University of Pennsylvania

{yangs1, rahulm}@seas.upenn.edu
3 Ziyan An and Meiyi Ma are with Vanderbilt University {ziyan.an,

meiyi.ma}@vanderbilt.edu

(a)

(b)

Fig. 1: Traffic-jam in CARLA simulator. a: Scenario initialization. b: Agents
trained based on MARL with hand-engineered reward fail to cross the open
lane in a timely manner and collide with other agents.

To address this challenge, we propose a safe MARL
algorithm that taking full advantages of signal temporal
logic (STL), which, as a formal language provides a more
principled and expressive way to describe the requirements.
We adopt the robustness values based on the STL requirements as rewards in our proposed STL-guided MARL
algorithm. We also guarantee the satisfaction of hard safety
requirements through an STL safety shield. While there are
extensive works exploring the use of temporal logic in singleagent RL, very little attention has been drawn on how to
leverage signal temporal logic (STL) to guide MARL [9],
[10]. Our proposed algorithm shows promising results in
learning a better policy for each agent to reach its objective
and ensure the safety of the system. Our contributions are
summarized as follows:
â€¢ We design a multi-agent reinforcement learning algorithm that is guided by STL specifications, where the
STL specifications include both safety requirements and
the task that each agent aims to finish. We incorporate

the STL safety shield to fulfill the safety specifications
and provide safe actions for the agents.
â€¢ The proposed algorithm utilizes STL to check partial
trajectories and provide robustness values as a corresponding reward during the training process.
â€¢ We validate the proposed methodology in multiagent particle-world environment (MPE) and CARLA
testbeds. We demonstrate that compared with the baseline MARL algorithms and commonly used rewards,
our proposed algorithm can learn better policies with
both larger rewards and higher safety rates.
II. RELATED WORKS
A. Multi-Agent Reinforcement Learning
There has been growing interest in the study of MARL
since many real-world problems involve the interactions of
multiple agents [11]. MARL approaches have been proposed
for various multi-agent systems, such as unmanned aerial vehicles [12], [13], complex traffic networks [14], autonomous
driving [15], and so on. Despite these successful applications,
one remaining challenge in MARL is how to design good
reward functions for complex tasks. Poorly-designed reward
functions might lead to undesired behavior and be detrimental to safety-critical systems [16]. The complex interactions
among agents and their diverse objectives make the reward
design hard.
B. Temporal Logic for Reinforcement Learning
RL reward function usually relies on hand-engineered design or approaches like reward shaping [17]. In recent years,
temporal logic specifications have been used extensively as
training guidance in the context of single agent reinforcement
learning for its power of expressiveness. In one direction,
finite state automaton (FSA) is constructed to reward the
agent (e.g., [18], [19], [20]) with the benefit of easy rewardgenerating automaton and high interpretability. In another
direction, the quantitative semantics of temporal logic formulas are captured to guide the policy training [9], [21]. Simply
applying the STL-guided single agent reinforcement learning
in multi-agent setting is not a good solution because they
donâ€™t consider the complex interactions between the agents
and their safety requirements, which is usually the case in
real world systems.
In the context of MARL, very few works have been done
to satisfy temporal logic specifications [10], [22], [23]. For
example, [10] proposes the first MARL algorithm for temporal logic specifications with correctness and convergence
guarantees. However, it uses the LTL specifically designed
to satisfy the non-Markovian, infinite-horizon specifications,
which may not be applicable in the real world. Also, it
has not been empirically verified in MARL environments.
[23] proposes an extended Markov Games as a general
mathematical model that allows multiple RL agents to concurrently learn various LTL specifications. In our work, we
consider STL specifications and use their robustness values
as the rewards instead. Compared with LTL, STL preserves
quantitative semantics that can be used to establish a robust

satisfaction value to quantify how well a trajectory fulfills
a specification. This can further allow us to quantify the
rewards more precisely and less sparser compared with LTL
based approaches.
III. P RELIMINARY AND P ROBLEM F ORMULATION
A. Signal Temporal Logic
In this section, we introduce the syntax, semantics, and
robustness metric of STL, which is a powerful formal symbolism for specifying temporal logical requirements [24]. We
first define a signal Ï‰ = s0 s1 Â· Â· Â· st as the state trajectory
from starting state to time point t.
Definition 3.1: The syntax of STL is defined by:
Ï† ::= Âµ | Â¬Ï† | Ï†1 âˆ§ Ï†2 | Ï†1 âˆ¨ Ï†2 | â™¢I Ï† | â–¡I Ï† | Ï†1 UI Ï†2
where I = [a, b], a, b âˆˆ Râ‰¥0 denotes a bounded time
interval. The atomic predicate Âµ represents the underlying
function Âµ(Ï‰ t ) â‰¥ 0, where Ï‰ t is the signal value at time t.
We use the symbols â–¡, â™¢, and U to denote temporal operators always, eventually, and until. The satisfaction relation
(Ï‰, t) |= Ï† evaluates to true (âŠ¤) if the specification Ï† is satisfied by Ï‰ starting from t and false (âŠ¥) otherwise. In addition
to its Boolean semantics, STL also owns the quantitative
semantics (i.e. robust satisfaction values), which quantify
the degree of satisfaction [24], [25], [26]. The quantitative
semantics assign real-valued measurements to the satisfaction
(positive values) or violation (negative values) of the STL
formula. In the evaluation section of this work, we utilize
the robustness values to measure specification satisfactions.
Definition 3.2: STL quantitative semantics are defined in
Table I.
TABLE I: STL quantitative semantics
Formula
Ï(x âˆ¼ c, Ï†, t)
Ï(Â¬Ï†, Ï‰, t)
Ï(Ï†1 âˆ§ Ï†2 , Ï‰, t)
Ï(â–¡I Ï†, Ï‰, t)
Ï(â™¢I Ï†, Ï‰, t)
Ï(Ï†1 UI Ï†2 , Ï‰, t)

Semantics
f (Ï‰[t]) âˆ’ c
âˆ’Ï(Ï†, Ï‰, t)
min{Ï(Ï†1 , Ï‰, t), Ï(Ï†2 , Ï‰, t)}
min Ï(Ï†, Ï‰, tâ€² )
tâ€² âˆˆ(t,t+I)

max

Ï(Ï†, Ï‰, tâ€² )

tâ€² âˆˆ(t,t+I)

suptâ€² âˆˆ(t+I)âˆ©T (min{Ï(Ï†2 , Ï‰, tâ€² ),
inf tâ€²â€² âˆˆ[t,tâ€² ] (Ï(Ï†1 , Ï‰, tâ€²â€² ))})

B. Problem Formulation of STL-guided MARL
STL-guided MARL: In this work, we define an STLguided MARL for multi-agent decision-making problem
such as the example shown in Fig. 1, to address the
challenge of designing a reward function that utilizes the
strength of STL. In particular, we define a tuple G =
(S, A, O, T , {ri }, Î³, {Ï†i }, Ï‰ tâˆ’L+1:t ), where S is the joint
state space, A is the joint action space, oi = O(s; i) is the
local observation for agent i at global state s. T corresponds
to the state transition function as defined for Markov games
in the literature [27]. The key new components of the
tuple definition include: Ï‰ tâˆ’L+1:t = stâˆ’L+1 Â· Â· Â· st1 being
the partial state trajectory of length L, and Ï†i being the
1 To increase readability, we will omit truncation time indices (t âˆ’ L + 1 :
t), i.e., we will use Ï‰ instead of Ï‰ tâˆ’L+1:t to denote the partial trajectory
with a slight notation abuse.

STL formula for agent i. Take the Traffic-jam scenario as an
example, the STL formula Ï†i is the aggregation of several
STL requirements, including reaching the destination, keeping safe distance to other agents, and waiting no more than
Tmax time steps after the broken-down vehicles. The detail
of the STL formula for different tasks will be introduced in
Section V. In the tuple G, reward ri = Ï(Ï†i , Ï‰, t) represents
the STL robustness value, and each agent
PT i aims to maximize
its own total expected return Ri = t=0 Î³ t rit where Î³ is a
discount factor and T is the time horizon.

horizon such as in the A3C algorithm [30]. This enables the
agent to better understand the consequences of its actions.
Hence, in this work, we define the reward based on the
robustness value given the partial trajectory Ï‰. It should
also be noted that the STL requirements encompass various
types, including reaching the goal, safety requirements like
maintaining a safe distance from other agents, and other
temporal requirements. Detailed STL requirements for task
specification and safety specification are illustrated by a case
study in Subsection IV-C.1.

IV. M ETHODOLOGY

Algorithm 1: Pseudocode for STL-guided MARL

In this section, we first introduce the algorithm structure
of STL-guided MARL algorithm. Our framework is shown
in Fig. 2.

Orthogonal initialization for Î¸i and Ï•i , the
parameters for policy Ï€i and critic Vi , respectively;
2 for episode = 1 to M do
3
Episode initialization: replay buffer D â† âˆ…; initial
state s; step t â† 1; rollout step number L;
4
while t â‰¤ tmax do
5
tstart = t;
6
Initialize an empty trajectory Ï‰;
7
for t = tstart to tstart + L âˆ’ 1 do
8
For each agent i, select action
atiq â† Ï€Î¸i (oti ), send atiq into STL safety
shield, and send back ati ;
9
Execute actions at = (at1 , . . . , atN ) chosen
based on STL safety shield, append st to
Ï‰, observe reward rit = Ï(Ï†i , Ï‰, t) for
each agent i given agent iâ€™s STL formula
Ï†i , observe new state st+1 ;
10
st â† st+1 ; t â† t + 1;
11
end
12
end
13
Store in D
{(sÏ„ , aÏ„ , r Ï„ , sÏ„ +1 ) | Ï„ âˆˆ [tstart , . . . , tstart +Lâˆ’1]};
for agent i = 1 to N do
14
Randomly sample mini-batch from D
{(sÏ„ , aÏ„ , r Ï„ , sÏ„ +1 ) | Ï„ âˆˆ [t, . . . , t + L âˆ’ 1]};
15
Update critic by minimizing LVF (Ï•i );
16
Update actor by maximizing L(Î¸i );
17
end
18 end

safety specification ğœ‘ğ‘ ğ‘ğ‘“ğ‘’
(hard requirement)

task specification ğœ‘ğ‘¡ğ‘ğ‘ ğ‘˜
(soft requirement)

specifications
ğœ‘ğ‘¡ğ‘ğ‘ ğ‘˜ âˆ§ ğœ‘ğ‘ ğ‘ğ‘“ğ‘’

guidance

partial
trajectory

safety constraint

multi-agent
reinforcement learning
actions

online monitoring
safe actions
system

STL safety shield

Fig. 2: Methodology overview. The user-provided task specification and
safety specification are expressed in STL formula Ï†task and Ï†saf e ,
respectively. The robustness values of the (partial) trajectory/signal w.r.t.
Ï†task and Ï†saf e are used to generate reward and guide the MARL policy
learning. The STL safety shield, which is constructed based on safety
specification Ï†saf e , is involved to safeguard the decisions made by MARL.

A. Reward Function
To address the challenge of defining a reward function
for a multi-agent system that considers the objective of
each individual agent and the complex interactions among
agents, in this work, we check the partial trace based on
the STL specifications, and provide the robustness value as
the reward. Itâ€™s notable that our method is generalizable
to different MARL algorithms, e.g., MADDPG [28] and
MAPPO [29].
At each time step of the training process, the trajectory
contains the states of all the agents. Given the partial trajectory and STL specification Ï†i , which might consist of both
safety specification Ï†i,saf e and task specification Ï†i,task , the
reward for agent i can be defined as
rit = Ï(Ï†i , Ï‰, t), âˆ€t âˆˆ [Ï„, Ï„ + L âˆ’ 1],

(1)

where Ï is the robustness value. Given the partial trajectory,
the more it satisfies the STL specifications, the larger reward
it obtains. One reason we use partial trajectory robustness
value rather full trajectory is that the former better discriminates different states and actions, while itâ€™s hard for latter
to assign the credit to specific actions, and estimating the
value function also becomes more challenging with longer
future horizons. From MARL algorithm design perspective,
n-step methods allow for credit assignment over a longer time

1

B. Algorithm Structure
In our work, we adopt the centralized training and decentralized execution paradigm. As depicted in Alg. 1, during
training, in rollout time steps t, each agent selects potentially
unsafe action atiq based on its local observations oti . This
action is then passed to the STL safety shield layer, and safe
action ati is returned and deployed such that the satisfaction
of the STL safety specifications is guaranteed. The details
of the STL safety shield layer will be presented in the next
section. The global state st = (ot1 , . . . , otN ), representing
the aggregation of the local observations, is appended to the
previous states to form the partial trajectory Ï‰. By evaluating
the partial trajectory against the STL requirements, each
agent obtains its reward rit .
The actor network is trained to maximize the following

objective:
L(Î¸i ) =

B
1 X

B

+Ïƒ

min(rÎ¸i ,k Ai,k , clip(rÎ¸,k , 1 âˆ’ Ïµ, 1 + Ïµ)Ai,k )

k=1
B
X

1
B

S[Ï€Î¸i (oi,k )],

k=1

(2)
Ï€ (ai,k |oi,k )
denotes
where B is the batch size, rÎ¸i,k = Ï€Î¸ Î¸i (a
i,k |oi,k )
iold
the ratio of the probability under the new and old policies
respectively, Ai,k is the advantage computed by GAE method
[31], S is the policy entropy, and Ïƒ is the entropy coefficient
hyperparameter. BFor the critic network, the loss function is:
1 X
max[(VÏ•i (si,k ) âˆ’ RÌ‚i,k )2 ,
LVF (Ï•i ) =
B
k=1

(clip(VÏ•i (si,k ), VÏ•iold (si,k ) âˆ’ Ïµ, VÏ•iold (si,k ) + Ïµ) âˆ’ RÌ‚i,k )2 ],
(3)
where RÌ‚i,k is the discounted reward-to-go. For actor and
critic network, we use the recurrent neural network (RNN) to
take the input to enable the agents to effectively model and
reason about sequential information in their interactions with
the environment. By maintaining hidden states and updating
them at each time step, RNN can capture the temporal
dynamics and dependencies across multiple time steps.
C. STL Safety Shield
In multi-agent systems with complex dynamics, such as
the Traffic-jam scenario depicted in Fig. 1, ensuring system
safety specification becomes paramount. One critical aspect
of safety is maintaining a safe distance between agents.
Consequently, we incorporate safety requirements into the
Signal Temporal Logic (STL) formulas Ï†saf e , which will
also be elaborated in Section V. To satisfy these safety
requirements specified in the STL formulas, such as â€œthe
distance between agents should always be greater than a
thresholdâ€, we first convert them to CBFs. Then, we employ
the quadratic programming (CBF-QP) to ensure safety. By
leveraging CBF-QP, we assess whether each discrete action
guarantees system safety and filter out any unsafe actions
accordingly.
1) Case Study: We use Traffic-jam scenario to explain
the details of STL safety shield design and the STL specifications. As shown in Fig. 1, in the Traffic-jam scenario,
red broken-down vehicles block three lanes, a group of
autonomous vehicles aim to cross the narrow road and arrive
their destination as soon as possible, while maintaining the
safety of the whole system. Agent iâ€™s observation at time t
include (1) its own locations, velocities, accelerations, orientation (pti , vit , ati , Ïˆit ); (2) other agent jâ€™s shared information
(ptj , vjt , atj , Ïˆjt ), âˆ€j âˆˆ N , (3) its destination pidest . Agent iâ€™s
discrete action space include: ai,1 : keep speed and keep in
current lane; ai,2 : change to left lane; ai,3 : change to right
lane; ai,4 : brake; ai,5 âˆ¼ ai,4+l : l different throttle values,
representing l levels of acceleration and deceleration in the
current lane.
We have the following requirements for each agent i:
(1) (safety) distance between itself and the leading vehicle

in its current lane and neighboring lanes should be always
greater than a safe distance; (2) (task) eventually reach its
destination; (3) (task) it should stop in front of the narrow
road location proad ; (4) (task) its blocked duration twait in
front of narrow road location proad should be less than Tmax .
These specifications can be easily converted to STL formulas
accordingly:
(vjt âˆ’ vit )2
â‰¥ Ïµ1 , âˆ€j âˆˆ N, j Ì¸= i
Ï†i1 = â–¡[0,T âˆ’1] âˆ¥pti âˆ’ ptj âˆ¥ âˆ’
2ail
ï£±
t
ï£´
ï£²Ï†i2 = â™¢[0,T âˆ’1] âˆ¥pi âˆ’ pidest âˆ¥ â‰¤ Ïµ2
Ï†i3 = â–¡[0,T âˆ’1] (Â¬(âˆ¥pti âˆ’ proad âˆ¥ â‰¤ L) âˆ¨ (â™¢[0,Ï„ ] vit â‰¤ 0))
ï£´
ï£³
Ï†i4 = â–¡[0,T âˆ’1] (Â¬(âˆ¥pti âˆ’ proad âˆ¥ â‰¤ L) âˆ¨ (twait < Tmax ))
(4)
CBF for safety: Here we show how to define the barrier
functions to fulfill the safety requirements in STL specifications. Notably, there is exiting work summarizes how to
design CBF given different STL predicates generally [32].
We model the low-level agent dynamic as a nonlinear control
affine system: xt+1 = f (xt ) + g(xt )ut , where xt âˆˆ Rn ,
ut âˆˆ U with U âŠ† Rm denoting the set of permissible
control inputs, f is the nominal unactuated dynamics, and
g is the nominal actuated dynamics. We adopt the widely
used kinematic bicycle model for its simplicity while still
considering the non-holonomic vehicle behaviors [33].
As shown in Fig. 3, during the lane keeping mode, the
ego vehicle should keep a safe distance to the vehicle in
the front in its current lane; while itâ€™s changing the lane,
it should keep a safe distance to both the vehicles in its
front and back. We use f v and bv to denote the front
and back vehicles in the target lane respectively. The safe
distance to the front vehicle can be expressed as Df v =
(1 + Ïµ)v t +

(v t âˆ’vft v )2
,
2al

and the safe distance to the back
(v t âˆ’v t )2

vehicle is Dbv = (1 + Ïµ)v t + bv2al . Note that al and
v t are the ego vehicleâ€™s acceleration limit and current speed
t
respectively, and vft v , vbv
denote the velocity of vehicle in
the front and back respectively. Finally, the CBFs can be
expressed as: hf v (xt , t) = (xtf v âˆ’ xt ) âˆ’ Df v (v t , vft v ) and
t
hbv (xt , t) = (xt âˆ’ xtbv ) âˆ’ Dbv (v t , vbv
).
t
For each discrete action aiq , after being mapped to the
corresponding continuous control input utiq [34], [35], then
the following CBF-QP is solved to return a safe control input:
min âˆ¥ut âˆ’ utiq âˆ¥22
s.t. sup [h(f (xt ) + g(xt )ut , t + 1) âˆ’ h(xt , t)] â‰¥ âˆ’Î³h(xt , t).
ut âˆˆU

(5)
Then, we can have the safety guarantees [36], [37].
Proposition 1: Assume h(xt , t) is a valid time-varying
CBF on C. Then any controller ut (xt ) from (5) for all xt âˆˆ C
will render the set C forward invariant, i.e., the system is safe.
Y
bv

fv

O

X

Fig. 3: Lane change in Traffic-jam scenario.

V. E XPERIMENTS AND C ASE S TUDIES
A. Testbeds and Common Experiment Setup
Testbed environment: We evaluate the performance of
STL-guided MARL algorithm on two benchmarks: the multiagent particle-world environment (MPE) [28] and CARLA
[38]. For MPE, we develop two new scenarios, namely
â€œSimple Coordination IIâ€ and â€œSimple Spread IIâ€, which
feature an increased number of stages for the agents to reach.
These new scenarios introduce additional temporal requirements compared to the existing tasks. Within CARLA, we
consider the Traffic-jam and Traffic-jam Expansion scenario,
which are characterized by intricate interactions, demanding
stringent safety requirements, and imposing higher temporal
constraints on the agentsâ€™ decision-making processes.
Common experimental setup: We conduct a comparative
analysis of the STL-guided MARL algorithm and the MARL
algorithm with the original task reward in both testbeds.
In STL-guided MARL, the STL reward are the weighted
sum of the robustness values of all the
P STL specifications.
We denote the STL reward as rit = j cj Ï(Ï†ij , Ï‰, t) + b,
where cj are weights and b is a constant. To ensure a fair
comparison, the original reward function are based on widely
adopted designs found in the existing literature. We evaluate
the performance of both methods by examining the episode
return. We compute the return using the STL reward in
both algorithms for consistency. This approach allows us to
quantify the extent to which the agents learn to fulfill the
designerâ€™s goals. For our experiments, we utilize a server
equipped with Intel Core i9-10900X processors and four
NVIDIA RTX2080Ti GPUs. The experiments are conducted
using Python 3.6.0, PyTorch 1.6.0, and CUDA 11.0.

added on the current reward for a collision. The reward
function is based on the reward designed in the existing
works[28], [21]. We use MADDPG[28] as our baseline
algorithm.
STL-guided MARL: For both tasks, given a whole trajectory of length T of agent i, the requirements include:
(1) All first part of N landmarks are eventually visited
by their corresponding agent; (2) All second part of N
landmarks are eventually covered by their corresponding
agent; (3) no collision between agents nearby (the distance is
always greater than the safety threshold); (4) The first three
landmarks should be visited at least once before the second
three landmarks are visited.
Therefore, we write the specifications for simple coordi^
nation as:
Ï†i1 = â™¢[0,T ]
|pti âˆ’ pi,landmark, first | â‰¤ Ïµ1 ,
1,2,...

^

Ï†i2 = â™¢â–¡[0,T ]

|pti âˆ’ pi,landmark, second | â‰¤ Ïµ2 .

(6)

1,2,...

Similarly, the STL specifications for simple spread II are:
^
Ï†i1 = â™¢[0,T ]
min |ptj âˆ’ pi,landmark,first | â‰¤ Ïµ1 ,
1,2,...

Ï†i2 = â™¢â–¡[0,T ]

jâˆˆN

^
1,2,...

min |ptj âˆ’ pi,landmark,second | â‰¤ Ïµ2 .

(7)

jâˆˆN

The V common safety requirement
â–¡[0,T ] 1,2,... |pti âˆ’ ptj | â‰¥ Dsaf e , âˆ€j âˆˆ N .

is:

Ï†i3

=

B. MPE Testbed
a) Environment: In MPE, we design two new tasks,
simple coordination II and simple spread II to evaluate our
algorithm. In both tasks, the observation of agent i include
the relative positions to other agents and the landmarks, the
discrete actions space are: stay, left, right, up and down.
Baselines: In both tasks, N agents need to first cover
N landmarks in the first stage, and then another N landmarks in the second stage with least collisions. The difference is: in simple coordination II, agent and landmark
are paired so agent only targets at its own corresponding
landmark; for simple spread II, agents learn to infer the
landmark they must cover, and move there while avoiding other agents. The reward
PN function for simple coordination II is: r = âˆ’c1 i=1 (|pi âˆ’ pi,landmark, goal |) +
PN
c2 i=1 (|pi âˆ’ pi,landmark, others |) where pi is location of
agent i, pi,landmark, goal is location of the current goal
landmark of agent i, pi,landmark, goal is location of the
other landmark. The
PNreward function for simple spread
II is r = âˆ’c1 i=1 (minjâˆˆN |pj âˆ’ pi,landmark, goal |) +
PN
c2 i=1 (minjâˆˆN |pj âˆ’pi,landmark, others |) where pj is location
of agent j, pi,landmark, goal is location of the landmark i in
current goal stage, pi,landmark, others is location of the other
stageâ€™s landmark i. In both tasks, there will be a âˆ’1 penalty

(a) simple coordination II

(b) simple spread II

Fig. 4: Training results in MPE.
TABLE II: Mean episode return in MPE. STL-guided MARL shows higher
mean episode return in both tasks compared with baseline algorithm,
demonstrating the advantages of our method in helping agent learn a better
policy to fulfill the designerâ€™s intentions.
Methods
Simple coordination II Simple spread II
STL-guided MARL âˆ’162.01 Â± 29.93 âˆ’120.47 Â± 18.13
MADDPG
âˆ’169.17 Â± 33.64 âˆ’128.36 Â± 17.90

b) Experiment Results: We train the algorithms for
30000 episodes, with episode length of 25 to evaluate their
performance. Fig. 4 provides insights into the mean and
variance of the average returns for the two tasks. Notably, the
STL-guided MARL algorithm demonstrates superior performance compared to baseline algorithm in terms of episode
return. Specifically, as shown in Fig. 5, the agents trained
with the STL-guided MARL approach exhibit a remarkable
ability to cover the second stage landmarks after visiting
the first stage. On the other hand, the agents trained with
baseline algorithm MADDPG struggle to cover the second
stage landmarks and tend to hover around the first stage.

TABLE III: Mean episode return and safety rate in Traffic-jam scenario.
Methods
STL-guided MARL
MAPPO
MAA2C
MAPPO w/o STL safety shield

Agent 1
1469.98 Â± 43.87
1244.12 Â± 174.18
1131.51 Â± 389.62
853.30 Â± 421.46

Mean episode return
Agent 2
3577.53 Â± 580.04
1838.96 Â± 392.22
1645.80 Â± 693.49
1241.91 Â± 223.38

This disparity in performance highlights the advantage of
the STL-guided approach in facilitating the policy learning.
By incorporating STL specifications, the agents are encouraged to adhere to specific behavioral patterns that result in
more successful navigation and completion of the tasks. In
contrast, the agents trained with a comparison reward, without the STL-guided framework, lack the guidance necessary
to achieve optimal performance and struggle to exhibit the
desired behavior.

Fig. 5: Simple coordination II in MPE. Agents trained with MADDPG hover
around the first stage landmarks and fail to cover the second stage as shown
in (a). Agents trained with STL-guided MARL visit the first stage landmarks
and then covers the second stage landmarks as shown in (b).

C. CARLA Testbed
The Traffic-jam scenario settings and STL specifications
are illustrated in Section IV-C.1. To further validate our algorithm, we add 3 more autonomous vehicles (agents) in the
Traffic-jam scenario, we name it as Traffic-jam Expansion.
a) Baselines: We adopt the reward that are widely used
in the existing literature for lane merging case[39], [40]. The
reward for agent i are defined as follow: ri = w1 rispeed +
w2 ricollision + w3 ridest , where w1 , w2 , w3 âˆˆ R are the weights,
i|
rispeed = v|v
, ricollision = âˆ’Icol with Icol being the collision
max
intensity collected by collision sensor, ridest = âˆ’|pi âˆ’pdest |+c
with c being a constant. we use MAPPO algorithm [29],
MAA2C algorithm[41], and MAPPO algorithm without STL
safety shield as our baseline algorithms.

Agent 3
3699.26 Â± 588.62
2245.07 Â± 150.75
2713.19 Â± 465.45
2102.25 Â± 713.09

Safety rate
97%
74%
88%
65%

TABLE IV: Total mean episode return and safety rate in Traffic-jam
Expansion Scenario. â€Total Mean episode returnâ€ represents the aggregated
total of mean episode returns across all agents.
Methods
STL-guided MARL
MAPPO
MAA2C
MAPPO w/o
STL safety shield

Total mean episode return
31828.34 Â± 2772.05
27617.27 Â± 9504.04
24169.17 Â± 4133.64

Safety rate
90%
64%
68%

21409.98 Â± 2848.80

20%

and the testing visualization is shown in Fig. 6. For Trafficjam Expansion scenario, training results are shown in Table IV. Here safety rate is defined as the proportion of
episodes with no collisions relative to the total number of
episodes. It can be observed that: (1) STL-guided MARL
with STL safety shield largely outperforms the algorithm
without it in terms of safety rate, therefore, showing the
effectiveness of our proposed STL safety shield in ensuring
the safety of the system. (2) While the STL specifications for
each agent remain the same, the mode of interaction during
an episode can vary between competitive and collaborative.
Our proposed method consistently outperforms the baseline
methods in terms of the total mean episode return, as
demonstrated in Table IV. This demonstrates our algorithm
can work in mixed cooperative-competitive environments. (3)
The STL-guided MARL algorithm consistently outperforms
the baseline algorithms in mean episode return. The superior
performance of the STL-guided MARL algorithm can be
attributed to its expressiveness and ability to capture the
designerâ€™s goal. By leveraging STL as a guidance framework,
the algorithm is able to incorporate high-level specifications
and constraints into the learning process. This enables the
agent to learn a policy that aligns more closely with the
desired behavior outlined by the designer. Thus, the STLguided MARL algorithm demonstrates its effectiveness in
improving the learning process and enabling the agent to
achieve better performance.
VI. C ONCLUSION

(a) MAPPO w/o STL (b)
safety shield
MARL

STL-guided (c) STL-guided MARL

Fig. 6: Traffic-jam scenario testing process in CARLA. (a): Agents trained
with the baseline algorithm fail to bypass the broken-down vehicle and
collide. (b) and (c): Agents trained with STL-guided MARL successfully
pass through the only open lane without collision in a timely manner.

b) Experiment Results: All the algorithms are trained
100 episodes with the episode length of 150 steps. For
Traffic-jam scenario, training results are shown in Table III

We propose a multi-agent reinforcement learning algorithm that leverages signal temporal logic (STL) specifications to guide the learning process and ensure the satisfaction of safety requirements and task objectives for each
agent. By incorporating STL safety shields, our algorithm
provides additional safety guarantees in the system. Through
case studies, we demonstrate that our approach outperforms
traditional MARL methods with hand-engineered rewards,
as it learns better policies with higher average rewards and
ensures the system safety. Our work highlights the potential
of using temporal logic and formal languages in MARL
to address the challenges of reward design and safety in
complex multi-agent systems.

R EFERENCES
[1] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton.
Reward is enough. Artificial Intelligence, 299:103535, 2021.
[2] Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer BasÌ§ar, and Lior
Horesh. Decentralized policy gradient descent ascent for safe multiagent reinforcement learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 35, pages 8767â€“8775, 2021.
[3] Zhili Zhang, Songyang Han, Jiangwei Wang, and Fei Miao. Spatialtemporal-aware safe multi-agent reinforcement learning of connected
autonomous vehicles in challenging scenarios.
arXiv preprint
arXiv:2210.02300, 2022.
[4] Javier GarcÄ±a and Fernando FernaÌndez. A comprehensive survey on
safe reinforcement learning. Journal of Machine Learning Research,
16(1):1437â€“1480, 2015.
[5] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi
Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in
robotics: From learning-based control to safe reinforcement learning.
Annual Review of Control, Robotics, and Autonomous Systems, 5:411â€“
444, 2022.
[6] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu.
State-wise safe reinforcement learning: A survey. arXiv preprint
arXiv:2302.03122, 2023.
[7] Jan Corazza, Ivan Gavran, and Daniel Neider. Reinforcement learning
with stochastic reward machines. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6429â€“6436, 2022.
[8] Yiannis Kantaros. Accelerated reinforcement learning for temporal
logic control objectives. In 2022 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pages 5077â€“5082. IEEE,
2022.
[9] Anand Balakrishnan and Jyotirmoy V Deshmukh. Structured reward
shaping using signal temporal logic specifications. In 2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
pages 3481â€“3486. IEEE, 2019.
[10] Lewis Hammond, Alessandro Abate, Julian Gutierrez, and Michael
Wooldridge. Multi-agent reinforcement learning with temporal logic
specifications. arXiv preprint arXiv:2102.00582, 2021.
[11] Kaiqing Zhang, Zhuoran Yang, and Tamer BasÌ§ar. Multi-agent reinforcement learning: A selective overview of theories and algorithms.
Handbook of reinforcement learning and control, pages 321â€“384,
2021.
[12] Jingjing Cui, Yuanwei Liu, and Arumugam Nallanathan. Multiagent reinforcement learning-based resource allocation for uav networks. IEEE Transactions on Wireless Communications, 19(2):729â€“
743, 2019.
[13] Han Qie, Dianxi Shi, Tianlong Shen, Xinhai Xu, Yuan Li, and Liujing
Wang. Joint optimization of multi-uav target assignment and path
planning based on multi-agent reinforcement learning. IEEE access,
7:146264â€“146272, 2019.
[14] Tianshu Chu, Jie Wang, Lara CodecaÌ€, and Zhaojian Li. Multi-agent
deep reinforcement learning for large-scale traffic signal control. IEEE
Transactions on Intelligent Transportation Systems, 21(3):1086â€“1095,
2019.
[15] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe,
multi-agent, reinforcement learning for autonomous driving. arXiv
preprint arXiv:1610.03295, 2016.
[16] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John
Schulman, and Dan ManeÌ. Concrete problems in ai safety. arXiv
preprint arXiv:1606.06565, 2016.
[17] Scott Proper and Kagan Tumer. Modeling difference rewards for
multiagent learning. In AAMAS, pages 1397â€“1398, 2012.
[18] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila
McIlraith. Using reward machines for high-level task specification and
decomposition in reinforcement learning. In International Conference
on Machine Learning, pages 2107â€“2116. PMLR, 2018.
[19] Xiao Li, Zachary Serlin, Guang Yang, and Calin Belta. A formal
methods approach to interpretable reinforcement learning for robotic
planning. Science Robotics, 4(37):eaay6276, 2019.
[20] Mingyu Cai, Shaoping Xiao, Junchao Li, and Zhen Kan. Safe
reinforcement learning under temporal logic with reward design and
quantum action selection. Scientific reports, 13(1):1925, 2023.
[21] Xiao Li, Cristian-Ioan Vasile, and Calin Belta. Reinforcement learning
with temporal logic rewards. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3834â€“3839.
IEEE, 2017.

[22] Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Amato, RuÌˆdiger
Ehlers, Ufuk Topcu, and Lu Feng. Safe multi-agent reinforcement
learning via shielding. arXiv preprint arXiv:2101.11196, 2021.
[23] Borja G LeoÌn and Francesco Belardinelli. Extended markov games
to learn multiple tasks in multi-agent reinforcement learning. arXiv
preprint arXiv:2002.06000, 2020.
[24] Oded Maler and Dejan Nickovic. Monitoring temporal properties of
continuous signals. In Formal Techniques, Modelling and Analysis of
Timed and Fault-Tolerant Systems, pages 152â€“166. Springer, 2004.
[25] Georgios E Fainekos and George J Pappas. Robustness of temporal
logic specifications for continuous-time signals. Theoretical Computer
Science, 410(42):4262â€“4291, 2009.
[26] Jyotirmoy V Deshmukh, Alexandre DonzeÌ, Shromona Ghosh, Xiaoqing Jin, Garvit Juniwal, and Sanjit A Seshia. Robust online
monitoring of signal temporal logic. Formal Methods in System
Design, 51(1):5â€“30, 2017.
[27] Michael L Littman. Markov games as a framework for multi-agent
reinforcement learning. In Machine learning proceedings 1994, pages
157â€“163. Elsevier, 1994.
[28] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel,
and Igor Mordatch. Multi-agent actor-critic for mixed cooperativecompetitive environments. Advances in neural information processing
systems, 30, 2017.
[29] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang,
Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo
in cooperative multi-agent games. Advances in Neural Information
Processing Systems, 35:24611â€“24624, 2022.
[30] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
In International conference on machine learning, pages 1928â€“1937.
PMLR, 2016.
[31] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and
Pieter Abbeel. High-dimensional continuous control using generalized
advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
[32] Lars Lindemann and Dimos V Dimarogonas. Control barrier functions
for signal temporal logic tasks. IEEE control systems letters, 3(1):96â€“
101, 2018.
[33] J Kong, M Pfeiffer, G Schildbach, and F Borrelli. Autonomous driving
using model predictive control and a kinematic bicycle vehicle model.
In Intelligent Vehicles Symposium, Seoul, Korea, 2015.
[34] Shilp Dixit, Saber Fallah, Umberto Montanaro, Mehrdad Dianati, Alan
Stevens, Francis Mccullough, and Alexandros Mouzakitis. Trajectory
planning and tracking for autonomous overtaking: State-of-the-art and
future prospects. Annual Reviews in Control, 45:76â€“86, 2018.
[35] Gianluca Cesari, Georg Schildbach, Ashwin Carvalho, and Francesco
Borrelli. Scenario model predictive control for lane change assistance
and autonomous driving on highways. IEEE Intelligent transportation
systems magazine, 9(3):23â€“35, 2017.
[36] Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada.
Control barrier function based quadratic programs for safety critical
systems. IEEE Transactions on Automatic Control, 62(8):3861â€“3876,
2016.
[37] Jun Zeng, Bike Zhang, and Koushil Sreenath. Safety-critical model
predictive control with discrete-time control barrier function. In 2021
American Control Conference (ACC), pages 3882â€“3889. IEEE, 2021.
[38] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez,
and Vladlen Koltun. Carla: An open urban driving simulator. In
Conference on robot learning, pages 1â€“16. PMLR, 2017.
[39] Sai Krishna Sumanth Nakka, Behdad Chalaki, and Andreas A Malikopoulos. A multi-agent deep reinforcement learning coordination
framework for connected and automated vehicles at merging roadways.
In 2022 American Control Conference (ACC), pages 3297â€“3302.
IEEE, 2022.
[40] Dong Chen, Mohammad Hajidavalloo, Zhaojian Li, Kaian Chen,
Yongqiang Wang, Longsheng Jiang, and Yue Wang. Deep multi-agent
reinforcement learning for highway on-ramp merging in mixed traffic.
arXiv preprint arXiv:2105.05701, 2021.
[41] Georgios Papoudakis, Filippos Christianos, Lukas SchaÌˆfer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint arXiv:2006.07869,
2020.

