Efficient and Scalable Deep Reinforcement Learning for Mean Field
Control Games
Nianli Pengâˆ—1 and Yilin Wangâˆ—1

arXiv:2501.00052v1 [cs.LG] 28 Dec 2024

1

Harvard University

Abstract
Mean Field Control Games (MFCGs) provide a powerful theoretical framework for analyzing systems of infinitely many interacting agents, blending elements from Mean Field Games (MFGs) and Mean Field Control
(MFC). However, solving the coupled Hamilton-Jacobi-Bellman and Fokker-Planck equations that characterize
MFCG equilibria remains a significant computational challenge, particularly in high-dimensional or complex environments.
This paper presents a scalable deep Reinforcement Learning (RL) approach to approximate equilibrium solutions of MFCGs. Building on previous works, We reformulate the infinite-agent stochastic control problem as
a Markov Decision Process, where each representative agent interacts with the evolving mean field distribution.
We use the actor-critic based algorithm from a previous paper [4] as the baseline and propose several versions of
more scalable and efficient algorithms, utilizing techniques including parallel sample collection (batching); minibatching; target network; proximal policy optimization (PPO); generalized advantage estimation (GAE); and
entropy regularization. By leveraging these techniques, we effectively improved the efficiency, scalability, and
training stability of the baseline algorithm.
We evaluate our method on a linear-quadratic benchmark problem, where an analytical solution to the
MFCG equilibrium is available. Our results show that some versions of our proposed approach achieve faster
convergence and closely approximate the theoretical optimum, outperforming the baseline algorithm by an order of magnitude in sample efficiency. Our work lays the foundation for adapting deep RL to solve more
complicated MFCGs closely related to real life, such as large-scale autonomous transportation systems, multifirm economic competition, and inter-bank borrowing problems. Our code is available in the Github Repo:
https://github.com/InsultedByMaths/6.S890/tree/main.

1

Introduction

Multi-agent systems arise in a wide range of engineering, economic, and societal applications, ranging from autonomous vehicle fleets and robotic swarms to large-scale financial markets and energy distribution networks. As
these systems grow in complexity and scale, it becomes increasingly challenging to reason about the collective behavior of individual agents and to design effective strategies for coordination, competition, and control. Traditional
multi-agent reinforcement learning (MARL) approaches struggle with the exponential growth in complexity as the
number of agents increases [5, 16]. When faced with a large or even infinite population of interacting entities, classical
game-theoretic, and MARL techniques become computationally prohibitive.
Mean Field Games (MFGs) and Mean Field Control (MFC) offer powerful mathematical frameworks for addressing
this scalability challenge. By considering the limit as the number of symmetric agents approaches infinity, MFGs
replace the complex interaction of many agents with a representative agentâ€™s interaction against a distribution
describing the aggregate state of the population [10, 8]. Under suitable conditions, the solution of an MFG is
characterized by a pair of coupled partial differential equations (PDEs): a Hamilton-Jacobi-Bellman (HJB) equation
that encodes the optimal control problem for a representative agent, and a Fokker-Planck equation that governs the
evolution of the population distribution.
* Equal contribution, correspondence to nianli_peng@fas.harvard.edu and yilin_wang@fas.harvard.edu

1

While MFGs capture the notion of individual agents optimizing their own objectives, Mean Field Control (MFC)
problems shift the perspective to a central planner seeking to minimize a collective cost for the entire population
[7]. Extending these ideas, Mean Field Control Games (MFCGs) bring together elements of both frameworks. In
MFCGs, multiple groups or organizers may coordinate internally (as in MFC) while competing externally (as in
MFG), resulting in a sophisticated equilibrium concept that must simultaneously satisfy optimal control conditions
and distributional fixed points.
However, solving the coupled PDEs that characterize MFGs, MFCs, and MFCGs remains a significant challenge.
Analytical solutions are rare and typically limited to stylized problems, while numerical methods that discretize the
state space and solve PDEs directly are computationally expensive and scale poorly with dimensionality [1, 2]. As a
result, there is a pressing need for scalable, data-driven methods capable of approximating equilibrium solutions in
complex and high-dimensional settings.
In this paper, we propose a deep Reinforcement Learning (RL) approach to tackle MFCGs at scale. Our method
reformulates the infinite-agent stochastic control problem into a Markov Decision Process (MDP), enabling the
use of standard RL algorithms. Each representative agent learns a policy that accounts for the evolving mean
field distribution, which is itself estimated and updated over the course of training. By adapting an actor-critic
framework and introducing batching and target networks, we significantly improve efficiency and stability. This
approach circumvents the need to directly solve large PDE systems, leveraging function approximation and sampled
trajectories to find near-equilibrium strategies.
We demonstrate our methodology on a linear-quadratic (LQ) benchmark problem that admits an analytical solution
[6], allowing us to quantitatively compare our results to the theoretical optimum. Our experiments show that the
baseline approach (without batching and target networks) struggles with slow convergence and unstable training,
whereas our improved algorithm converges an order of magnitude faster and closely approximates the known equilibrium solution. These findings suggest that our RL-based framework is both scalable and effective, opening the
door to tackling more complex MFCG problems where no closed-form solutions are available.
The remainder of this paper is structured as follows. In Section 2, we review related work on MARL, MFG, MFC,
and MFCG, and highlight the gaps that motivate our RL-based approach. Section 3 formally presents the MFCG
formulation, linking the PDE characterization of equilibrium to a control problem that can be cast as an MDP.
Section 4 describes a reinforcement learning approach towards solving MFCGs. 5 detail a baseline algorithm
from prior work and our versions of the more efficient algorithm, including the actor-critic framework, batching,
mini-batching, the target networks, and commonly used deep RL techniques (PPO, GAE, entropy regularization).
We present and analyze experimental details, results, discussions, limitations, and future works in Section 6. We
conclude our work in Section 7. Some details on formulation and additional ablation experiments are documented
in the appendix 9 for reference.

2

Background and Related Work

In this section, we survey the literature related to multi-agent reinforcement learning, mean field games, mean field
control, and their intersection in mean field control games. We highlight how these frameworks address the scalability
issues of large-population interactions and identify the gaps that motivate our approach.
Classical MARL techniques focus on systems with a finite, and typically small, number of agents [5, 16]. A variety
of learning paradigmsâ€”such as independent learners, centralized training with decentralized execution, and communication protocolsâ€”have been proposed to enable agents to learn coordinated policies. However, as the number
of agents grows, these approaches encounter fundamental scalability challenges. The joint state-action space expands combinatorially, and coordination strategies become increasingly complex. Even in simpler settings, finding
equilibrium policies or stable solutions can be computationally prohibitive. This exponential blow-up in complexity
motivates the need for asymptotic frameworks that simplify analysis and computation.

Page 2

2.1

Mean Field Games (MFG)

Mean Field Games, introduced by Lasry and Lions [10] and further studied by Huang, Caines, and MalhamÃ© [8],
provide a theoretical foundation for understanding large populations of symmetric agents interacting in a dynamic
game. In the limit of an infinite number of agents, the populationâ€™s distribution over states (or actions) serves
as a proxy for the collective behavior. Each individual (infinitesimal) agent then optimizes its policy against this
distribution, rather than explicitly considering every other agent.
Mathematically, MFG equilibria are characterized by a coupled system of PDEs: a Hamilton-Jacobi-Bellman (HJB)
equation governing the optimal control for a representative agent, and a Fokker-Planck (FP) equation describing the
evolution of the distribution of agents under the aggregate policy. Solving the HJB-FP system yields an equilibrium
distribution and a corresponding best-response control. MFG theory has been extensively developed, and under
certain conditions, well-posedness and uniqueness results are available [1].
Yet, from a computational perspective, MFG models still face challenges. Numerical solution of the PDEs can become
intractable for high-dimensional state spaces or complex cost structures, limiting their practical applicability [2].

2.2

Mean Field Control (MFC)

While MFGs consider a Nash equilibrium scenario where all agents optimize individually, Mean Field Control (MFC)
shifts the viewpoint to that of a central planner or social organizer who aims to minimize an aggregate cost for
the entire population [7]. In MFC, the dynamics of the population are still described by McKean-Vlasov-type
stochastic differential equations, but the control problem involves choosing a global control policy that influences the
distributionâ€™s dynamics to achieve a collective objective.
The solution to MFC problems can also be characterized by PDEs similar to those in MFG, but with different
equilibrium conditions. While MFG agents each solve an individual optimal control problem, MFC seeks a centralized
solution that aligns the interests of all agents. This perspective simplifies some aspects, as it removes the strategic
interaction among agents, but the complexity of solving the underlying PDEs remains significant.

2.3

Mean Field Control Games (MFCG)

Mean Field Control Games (MFCGs) combine elements from both MFG and MFC frameworks. Here, multiple
groups (or planners) may engage in competitive interactions while coordinating internally [7]. Each group tries to
optimize a collective cost subject to the distributional dynamics influenced by other groups. This setting leads to a
fixed-point problem that must satisfy both optimal control conditions (as in MFC) and equilibrium constraints (as
in MFG).
MFCGs are particularly well-suited to model scenarios where competition and cooperation coexist at different levels
of aggregation. For instance, one might consider a landscape of multiple firms each optimizing internal resource
allocation (like MFC), while competing for market share against a continuum of other firms (like MFG). Despite
their broad applicability, the complexity of MFCGs is even higher than MFGs or MFC alone, as they must capture
both the strategic interactions among groups and the optimal control structure within each group.
The theoretical developments in MFG, MFC, and MFCG provide a rich understanding of equilibrium conditions
for large-scale multi-agent systems. However, their direct solution often relies on PDE-based methods that do not
scale well to high-dimensional or otherwise complex environments. Analytical solutions are typically limited to
linear-quadratic (LQ) or other highly tractable settings, leaving a large class of practical problems out of reach.
This gap motivates the use of data-driven approximation techniques like reinforcement learning. By framing MFCGs
as Markov Decision Processes (MDPs) for representative agents, one can directly apply RL algorithms to approximate
optimal policies without explicitly solving the HJB-FP system. Prior works on MFGs have explored RL-based
approximations, but scalable and stable algorithms tailored for MFCG settings remain scarce. Our approach aims
to address this need by leveraging deep RL methods, combined with batching and target networks, to efficiently
approximate MFCG equilibria.

Page 3

3

Problem Formulation

We now formalize the Mean Field Control Game (MFCG) setting. MFCGs combine features of Mean Field Games
(MFGs), where agents individually optimize their own strategies against a population distribution, and Mean Field
Control (MFC), where a planner optimizes a collective cost for a continuum of agents. In MFCGs, one can think of
multiple such planners or groups interacting strategically, leading to equilibrium conditions that must simultaneously
account for optimal control and distributional fixed points.
Consider an infinite population of agents, each evolving in a continuous-time, continuous-state environment. Let Xt
denote the state of a representative agent at time t. The evolution of Xt follows a stochastic differential equation
(SDE) with McKean-Vlasov dynamics:
dXt = b(Xt , Âµt , Î±(Xt )) dt + Ïƒ(Xt , Âµt , Î±(Xt )) dWt ,

(1)

where Âµt represents the distribution of states across the infinite population at time t, Î±(Â·) is the control (or policy)
applied by the representative agent, b(Â·) is the drift term, Ïƒ(Â·) is the volatility term, and Wt is a standard Brownian
motion. The dependence of b and Ïƒ on Âµt captures the mean field interaction: each agentâ€™s dynamics depend not
just on its own state and action, but also on the collective distribution of the entire population.
Each agent (or planner, in the case of MFC) aims to minimize a discounted infinite-horizon cost functional. For a
given policy Î± and population distribution Âµ, define the cost:
Z âˆ

âˆ’Î²t
e f (Xt , Âµt , Î±(Xt )) dt ,
(2)
J(Î±; Âµ) = E
0

where f quantifies the running cost, and Î² > 0 is a discount factor ensuring integrability. The cost f typically
includes terms penalizing control effort (e.g., 21 âˆ¥Î±(Xt )âˆ¥2 ) and terms representing â€œunsafenessâ€ or other externalities
that depend on the state distribution Âµt . For instance, f might include a component g(Xt , Âµt ) capturing congestion,
collision risks, or deviations from desired configurations.
In a Mean Field Game, a Nash equilibrium occurs when no agent can reduce its cost by unilaterally deviating from
the equilibrium control, given the equilibrium distribution. Similarly, in Mean Field Control problems, the optimal
control is determined by a planner who takes the distribution as given. A Mean Field Control Game extends these
concepts by considering that each group or planner might be optimizing a collective cost, potentially in competition
or cooperation with others. The equilibrium (Î±Ì‚, ÂµÌ‚) in an MFCG is a fixed point such that:
1. Given the population distribution ÂµÌ‚, the control Î±Ì‚ solves the stochastic optimal control problem:
Î±Ì‚ âˆˆ arg min J(Î±; ÂµÌ‚).
Î±

2. The distribution ÂµÌ‚ is consistent with the law of the state process under the chosen control Î±Ì‚:
ÂµÌ‚t = L(XtÎ±Ì‚,ÂµÌ‚ ) for all t â‰¥ 0,
where XtÎ±Ì‚,ÂµÌ‚ evolves according to (1) with Î± = Î±Ì‚ and the distribution fixed at ÂµÌ‚.
If the problem is time-homogeneous and stationary, we often seek a stationary equilibrium distribution ÂµÌ‚ and a
stationary control Î±Ì‚, leading to a steady-state solution.
Under suitable regularity conditions, the optimal control problem can be characterized via a Hamilton-Jacobi-Bellman
(HJB) equation for the value function associated with the representative agent, and the distribution evolution is
governed by a Fokker-Planck (FP) equation. Together, these form a coupled HJB-FP system:


(HJB): âˆ‚t J(x, t) + min Î± Â· âˆ‡x J(x, t) + f (x, Âµt , Î±) + (diffusion terms) = 0,
Î±

(3)


1
(FP): âˆ‚t Âµt = âˆ’âˆ‡ Â· (Âµt b(x, Âµt , Î±Ì‚(x))) + âˆ‡ Â· ÏƒÏƒ T (x, Âµt , Î±Ì‚(x))âˆ‡x Âµt .
2
Page 4

Solving this PDE system directly can be challenging, especially in high dimensions or when f , b, or Ïƒ are complex.
Instead of directly tackling the coupled PDEs, we can use reinforcement learning to approximate the equilibrium
solution. By discretizing time and approximating the continuous dynamics with, for instance, an Euler-Maruyama
scheme, the representative agentâ€™s optimization problem can be cast as a Markov Decision Process (MDP):
Î±

t
(Xt+1 , Âµt+1 ),
(Xt , Âµt ) âˆ’â†’

with rt = âˆ’f (Xt , Âµt , Î±t ),

and a discount factor Î³ = eâˆ’Î²âˆ†t for a small time step âˆ†t. In this MDP viewpoint, Î±t represents the agentâ€™s action
(control) at state Xt , and the transition probabilities reflect the stochastic dynamics influenced by Âµt .
The main challenge is that the distribution Âµt itself must be learned or approximated. Nevertheless, this RL formulation bypasses the need for explicit PDE solutions. Instead, we can rely on sampling-based methods and function
approximation (e.g., neural networks) to iteratively refine both the agentâ€™s policy and the population distribution.
In the next section, we present our deep reinforcement learning methodology, which combines policy optimization
with techniques to stabilize and accelerate training, ultimately approximating the MFCG equilibrium solution.

4

Reinforcement Learning Approach

While the Mean Field Control Game (MFCG) equilibrium can be theoretically characterized by a coupled system
of PDEs, directly solving these equations is often infeasible for high-dimensional or complex problems. Instead, we
take a data-driven approach by recasting the infinite-agent control problem into a Reinforcement Learning (RL)
setting. This viewpoint allows us to leverage function approximation, sampling-based optimization, and established
RL techniques to approximate equilibrium solutions.
To apply RL, we discretize time and view the representative agentâ€™s decision-making as a Markov Decision Process
(MDP). Suppose we discretize the time horizon into small intervals âˆ†t. The continuous dynamics given by the
McKean-Vlasov equation (1) are approximated using an Euler-Maruyama scheme:
Xt+âˆ†t â‰ˆ Xt + b(Xt , Âµt , Î±t ) âˆ†t + Ïƒ(Xt , Âµt , Î±t ) âˆ†Wt ,

(4)

where âˆ†Wt âˆ¼ N (0, âˆ†t) is a discretized Brownian increment. In this discrete-time setting, the state transitions
depend on the chosen control Î±t and the distribution Âµt , which influences b and Ïƒ.
The agent observes its state Xt , takes an action At = Î±t , and transitions to a new state Xt+âˆ†t . The population
distribution Âµt evolves as well, influencing future transitions. Thus, the representative agentâ€™s problem can be
described by an MDP with state space X , action space A, and a transition kernel that depends implicitly on Âµ.
In the continuous-time formulation, the agentâ€™s objective is to minimize the cost functional:
Z âˆ

J(Î±; Âµ) = E
eâˆ’Î²t f (Xt , Âµt , Î±(Xt )) dt .
0

By setting Î³ = eâˆ’Î²âˆ†t , we obtain a discrete-time discounted objective in the RL setting:
"âˆ
#
X
n
JRL (Î±; Âµ) = E
Î³ f (Xnâˆ†t , Âµnâˆ†t , Î±(Xnâˆ†t ))âˆ†t .
n=0

In RL terminology, we define the reward at each time step as:
rt = âˆ’f (Xt , Âµt , At )âˆ†t,
so that minimizing J(Î±; Âµ) is equivalent to maximizing the expected discounted sum of rewards. This direct mapping
allows us to use standard RL algorithms, which are typically formulated in terms of reward maximization.
In order to implement the RL approach, we must represent and update the mean field distribution Âµ as the system
evolves. Directly estimating a probability density over a high-dimensional state space can be challenging. Instead,
we adopt a score-matching technique [9], which has proven effective in generative modeling [14].

Page 5

Score-matching focuses on learning the score function sÂµ (x) := âˆ‡ log pÂµ (x), where pÂµ is the density corresponding to
Âµ. Rather than modeling pÂµ (x) directly, we parameterize the score function with a neural network Î£Ï† (x) and update
Ï† by minimizing an objective that does not require knowing the normalization constant of pÂµ . Once we have a good
approximation of sÂµ (x), we can generate approximate samples from Âµ using Langevin Monte Carlo:
âˆš
Ïµ
xm+1 = xm + Î£Ï† (xm ) + Ïµ zm ,
2

zm âˆ¼ N (0, I).

These samples yield an empirical approximation of Âµ, denoted Âµn at iteration n. As training progresses, Î£Ï† and thus
Âµn gradually adjust to reflect the evolving mean field distribution induced by the agentâ€™s policy and environment
dynamics.
This score-based approach offers flexibility: it avoids restrictive parametric assumptions about pÂµ , can approximate
complex distributions, and is compatible with parallel sampling. Though it may introduce additional computational overhead (due to gradient calculations and Langevin steps), this overhead is generally more manageable than
attempting a full PDE-based solution.
Once the MFCG is modeled as an MDP, the representative agent attempts to find a policy Î Ïˆ (A|X) that, together
with a stationary distribution ÂµÌ‚, forms an equilibrium. In other words, we seek (Ï€Ì‚, ÂµÌ‚) such that:
"âˆ
#
X
n
Ï€Ì‚ âˆˆ arg max E
Î³ (âˆ’f (Xn , ÂµÌ‚n , An )) ,
Ï€

subject to the consistency condition:

n=0

ÂµÌ‚n = L(XnÏ€Ì‚,ÂµÌ‚ ).

Here, XnÏ€Ì‚,ÂµÌ‚ evolves under the chosen policy and the distribution ÂµÌ‚. Achieving this fixed point involves iteratively
updating Ï€Î¸ and the score network parameters Ï† until stability is reached.
To solve the MDP, we employ deep RL methods that handle continuous action spaces and nonlinear function approximation. We use an actor-critic framework:
â€¢ Actor: A neural network Î Ïˆ parameterizes the policy, updated via policy gradients.
â€¢ Critic: A neural network VÎ¸ (X) estimates the value function, trained using temporal difference methods to
reduce variance and guide the policy update.
In addition to the actor and critic, we maintain networks for the global and local aspects of the distribution (as
represented by the score function Î£Ï† and potentially other auxiliary variables capturing local structure in Âµ). These
networks all interact, as the learned distribution influences transitions and rewards, which in turn affect the policy
and value function updates.

4.1

Technical Challenges in Implementation

Implementing this RL-based framework for MFCGs poses several challenges. Chief among them is the simultaneous
learning and mutual dependence of four neural networks: the actor (Ï€Î¸ ), the critic (VÏ• ), and the distributional
approximation networks (e.g., the score network Î£Ï† and potentially an auxiliary network for local distribution
features).
First, the actor and critic form a feedback loop: the actorâ€™s updates depend on value estimates from the critic, while
the critic must accurately evaluate returns under the current policy. Next, the distributional approximation must
track a moving target since changes in Ï€Î¸ alter the state visitation frequencies and thus the mean field distribution
Âµ. If we introduce both a global and a local distribution representation, these must also remain consistent with each
other and with the evolving environment dynamics.
This intricate web of dependencies can slow convergence and destabilize training. For instance, if the score network
Î£Ï† lags behind the policy updates, Âµn may be poorly estimated, causing the actor and critic to learn suboptimal

Page 6

or unstable policies. Conversely, overly aggressive updates to Î£Ï† in pursuit of a precise distribution estimate can
amplify noise, hindering the policyâ€™s ability to settle into an equilibrium.
Mitigating these issues may require techniques such as:
â€¢ Careful scheduling of learning rates and update frequencies for different networks.
â€¢ Using target networks, batching, and parallelization to stabilize learning.
â€¢ Incorporating regularization or entropy bonuses to prevent overfitting and encourage more robust exploration.
These practical challenges underscore the complexity of MFCGs. While our approach lays a foundation for applying
deep RL to infinite-agent settings, achieving stable and efficient training still demands careful algorithmic design and
parameter tuning, particularly as we scale to richer dynamics and more complex mean field structures.

5

Proposed Algorithm

The proposed algorithm extends the Infinite Horizon Mean Field Control Game Actor-Critic (IH-MFCG-AC) framework [4]. We propose three editions of the algorithms.
â€¢ IH-MFCG-AC-B: Employs parallel sample collection (batching) and added a target network to expedite
learning and stabilize the loss curve. B stands for Batching
â€¢ IH-MFCG-AC-M: Employs parallel sample collection (batching) and adds a target network; At each step,
chunks the large batch into mini-batches and performs the learning process within each mini-batch. M stands
for Minibatch.
â€¢ IH-MFCG-AC-DRL: Based on IH-MFCG-AC-B and employs techniques commonly used in deep reinforcement learning: (1) learn the actor using proximal policy optimization (PPO) [13] instead of REINFORCE [15];
(2) use generalized advantage estimation (GAE) to estimate advantage with variance reduction [12]; and (3)
add entropy regularization to encourage exploration.

5.1

Baseline: IH-MFCG-AC

Algorithm 1 describes the IH-MFCG-AC algorithm. It uses four neural networks: the actor, the critic, the global
score, and the local score. The actor-network learns a policy that selects actions from states and is updated using
policy gradient methods, gradually improving the decision-making process. The critic network estimates the value
function and is trained by minimizing the temporal difference (TD) error, providing stable feedback for the actor.
The global score and local score networks both represent probability distributions as score functions and are trained
using a score-matching loss, enabling them to learn the global and local distributions. At each iteration, states are
sampled from both the global and local distributions using Langevin Monte Carlo, which updates these distributions
as the algorithm proceeds. This combination of actor-critic training and score-based distribution updates leads to
convergence toward an equilibrium policy and stationary distributions in the mean field control game. Please refer
to the original paper for further details [4].

Page 7

Algorithm 1 IH-MFCG-AC: Infinite Horizon Mean Field Control Game Actor-Critic
Require: Initial distribution Î¾; number of time steps N â‰« 0; discrete time step size âˆ†t; neural network learning
rates for actor ÏÎ  , critic ÏV , global score ÏÎ£ , and local score ÏÎ£
e ; Langevin dynamics step size Ïµ.
1: Initialize neural networks:
Actor Î Ïˆ0 : Rd â†’ P(Rk )
Critic VÎ¸0 : Rd â†’ R
Global Score Î£Ï†0 : Rd â†’ Rd
e Î¾ : Rd â†’ Rd
Local Score Î£
0
2: Sample Xt0 âˆ¼ Î¾
3: for n = 0, . . . , N âˆ’ 1 do
4:

Compute score loss for Î£:

5:

Update Î£ with Adam:

6:

2

LÎ£ (Ï†n ) = tr (âˆ‡x Î£Ï†n (Xtn )) + 21 âˆ¥Î£Ï†n (Xtn )âˆ¥2

Ï†n+1 = Ï†n âˆ’ ÏÎ£ âˆ‡Ï† LÎ£ (Ï†n )


2
1 e
e
e
Compute score loss for Î£:
LÎ£
e (Î¾n ) = tr âˆ‡x Î£Î¾n (Xtn ) + 2 Î£Î¾n (Xtn )
2

7:
8:

e with Adam:
Update Î£

Î¾n+1 = Î¾n âˆ’ ÏÎ£
e âˆ‡Î¾ LÎ£
e (Î¾n )




(1)
(2)
(k)
(1)
(2)
(k)
Generate mean field samples Stn = Stn , Stn , . . . , Stn from Î£Ï†n+1 and Setn = Setn , Setn , . . . , Setn from
Pk
Pk
eÎ¾
Î£
using Langevin dynamics with step size Ïµ and compute ÂµS := 1
Î´ (i) and Âµ e := 1
Î´ e(i) .
n+1

9:

Sample action:

10:

Observe reward from the environment:

11:

k

tn

i=1 St
n

k

i=1 St
n

Atn âˆ¼ Î Ïˆn (Â· | Xtn )
rn+1 = âˆ’f (Xtn , ÂµStn , ÂµSet Atn )âˆ†t
n

Observe the next state from the environment:
âˆš
Xtn+1 = b(Xtn , ÂµStn , Atn )âˆ†t + Ïƒ(Xtn , ÂµStn , Atn ) âˆ†t zn ,

13:

Compute TD target: yn+1 = rn+1 + eâˆ’Î²âˆ†t VÎ¸n (Xtn+1 )
Compute TD error: Î´Î¸n = yn+1 âˆ’ VÎ¸n (Xtn )

14:

Compute critic loss:

15:

Update critic with Adam:

16:

Compute actor loss:

17:

Update actor with Adam:

12:

Stn

zn âˆ¼ N (0, 1)

LV (Î¸n ) = Î´Î¸2n
Î¸n+1 = Î¸n âˆ’ ÏV âˆ‡Î¸ LV (Î¸n )

LÎ  (Ïˆn ) = âˆ’Î´Î¸n log Î Ïˆn (Atn | Xtn )
Ïˆn+1 = Ïˆn âˆ’ ÏÎ  âˆ‡Ïˆ LÎ  (Ïˆn )

18: end for
19: return

5.2

eÎ¾ )
(Î ÏˆN , Î£Ï†N , Î£
N

Parallel Sampling and Target Network: IH-MFCG-AC-B

Algorithm 2 shows the IH-MFCG-AC-B algorithm, with the differences with Algorithm 1 highlighted in red. It
introduces two key modifications. First, it employs parallel sampling (i.e., batching), meaning that multiple samples
of states, actions, and rewards are collected in parallel at each iteration, rather than one at a time. At each update
step, we average the loss from each sample. This approach enables us to reduce the variance in the updates at almost
no cost of additional training latency due to its effective utilization of GPU acceleration. It is particularly efficient
for vectorized environments, and should work well with any environments where it is fast to compute responses given
action. Second, we use a target network for the critic, which is a copy of the criticâ€™s parameters updated more slowly
and thus provides a more stable and less noisy target for the value estimation. Together, these changes increase the
stability and scalability of the learning process, making it more robust and less sensitive to randomness (i.e., â€œbad
luck" ).

Page 8

Algorithm 2 IH-MFCG-AC-B: IH-MFCG-AC algorithm with Batching and Target Network
Require: Initial distribution Î¾; number of time steps N â‰« 0; discrete time step size âˆ†t; neural network learning
rates for actor ÏÎ  , critic ÏV , global score ÏÎ£ , and local score ÏÎ£
e ; Langevin dynamics step size Ïµ; batch size B
1: Initialize neural networks:
Actor Î Ïˆ0 : Rd â†’ P(Rk )
Critic VÎ¸0 : Rd â†’ R
Target TÎ¸0 : Rd â†’ R, same as Critic
Global Score Î£Ï†0 : Rd â†’ Rd
e Î¾ : Rd â†’ Rd
Local Score Î£
0
2: Sample Xt0 = {Xt10 , Xt20 , Â· Â· Â· , XtB0 }, where Xti0 âˆ¼ Î¾
3: for n = 0, . . . , N âˆ’ 1 do
4:

Compute score loss for Î£:

5:

Update Î£ with Adam:

6:

LÎ£ (Ï†n ) = B1

PB

i=1 tr


2
âˆ‡x Î£Ï†n (Xtin ) + 21 Î£Ï†n (Xtin ) 2

Ï†n+1 = Ï†n âˆ’ ÏÎ£ âˆ‡Ï† LÎ£ (Ï†n )


2
PB
1 e
1
i
i
e
e
Compute score loss for Î£:
LÎ£
e (Î¾n ) = B
i=1 tr âˆ‡x Î£Î¾n (Xtn ) + 2 Î£Î¾n (Xtn )
2

e with Adam:
Update Î£

8:

Î¾n+1 = Î¾n âˆ’ ÏÎ£
e âˆ‡Î¾ LÎ£
e (Î¾n )




(1)
(2)
(k)
(1)
(2)
(k)
Generate mean field samples Stn = Stn , Stn , . . . , Stn from Î£Ï†n+1 and Setn = Setn , Setn , . . . , Setn from
Pk
Pk
eÎ¾
Î£
using Langevin dynamics with step size Ïµ and compute ÂµStn := k1 i=1 Î´S (i) and ÂµSet := k1 i=1 Î´Se(i) .
n+1

9:

Sample action for i = 1, 2, ..., B (vectorized):

10:

Observe reward from environment for i = 1, 2, ..., B:

7:

tn

n

tn

Aitn ) âˆ¼ Î Ïˆn (Â· | Xtin ))
i
rn+1
= âˆ’f (Xtin , ÂµStn , ÂµSet , Aitn )âˆ†t
n

11:

Observe next state from environment for i = 1, 2, âˆš
..., B (vectorized):
Xtin+1 = b(Xtin , ÂµStn , Aitn )âˆ†t + Ïƒ(Xtin , ÂµStn , Aitn ) âˆ†t zn ,
zn âˆ¼ N (0, 1)

12:

Compute TD target for i = 1, 2, ..., B (vectorized):

13:
14:

Compute TD error for i = 1, 2, ..., B (vectorized):
PB
Compute critic loss: LV (Î¸n ) = B1 i=1 (Î´Î¸i n )2

15:

Update critic with Adam:

16:

Compute actor loss:

17:

Update actor with Adam:

i
i
yn+1
= rn+1
+ eâˆ’Î²âˆ†t TÎ¸n (Xtin+1 )
i
Î´Î¸i n = yn+1
âˆ’ VÎ¸n (Xtin )

Î¸n+1 = Î¸n âˆ’ ÏV âˆ‡Î¸ LV (Î¸n )
PB
LÎ  (Ïˆn ) = B1 i=1 âˆ’ Î´Î¸i n log Î Ïˆn (Aitn | Xtn )
Ïˆn+1 = Ïˆn âˆ’ ÏÎ  âˆ‡Ïˆ LÎ  (Ïˆn )

18:
Tn+1 â† Vn+1 if n divisible by 200, else Tn+1 â† Tn
19: end for
20: return

5.3

eÎ¾ )
(Î ÏˆN , Î£Ï†N , Î£
N

Mini-Batching: IH-MFCG-AC-M

Algorithm 3 shows the IH-MFCG-AC-M algorithm. The difference from the IH-MFCG-AC-B algorithm is highlighted
in purple. Here, at every step, instead of doing the entire learning process once for all samples in a vectorized
manner, we first shuffle the states, partition them into mini-batches, and then perform the update procedures for
each mini-batch. This approach can improve training stability and better utilize computational resources. For
instance, if one has access to large GPU memory, increasing the batch size can lead to higher variance reduction in
stochastic gradients. However, simply processing one massive batch at each step might not fully exploit the potential
computational parallelism or may lead to slower per-iteration updates. By chunking the batch into mini-batches,
the algorithm can perform multiple gradient updates per iteration at a finer granularity, effectively combining the
variance reduction benefits of large batches with more frequent parameter updates.

Page 9

Introducing mini-batching also allows for a more balanced computational load at each iteration. Instead of waiting for
the entire large batch to be processed, updates can be made incrementally, potentially stabilizing training dynamics
and facilitating faster convergence. In practice, this modification translates into improved performance and reduced
training times, especially when combined with optimized parallel computing strategies.
In addition to mini-batching, we make several other improvements over the baseline IH-MFCG-AC-B algorithm:
Improved Trace Estimation for Score Matching. In the previous version, computing the divergence term
required by the score matching objective was done by explicitly looping over each dimension and using automatic
differentiation to compute partial derivatives. This approach is computationally expensive and does not scale well.
To address this, we employ Hutchinsonâ€™s trace estimator, which replaces the explicit computation of the divergence
with a stochastic approximation. If Î£Ï† (x) is the score approximation at state x, and z âˆ¼ N (0, I), then:


div(Î£Ï† )(x) = Ez z âŠ¤ âˆ‡x Î£Ï† (x) â‰ˆ z âŠ¤ âˆ‡x Î£Ï† (x).
In practice, a single z suffices. This avoids explicit loops over the state dimension, leading to a significant computational speed-up. The loss function can thus be computed more efficiently, enabling larger batch sizes and more
complex function approximators without incurring a large computational penalty.
Reduced Frequency of Langevin Updates. Previously, Langevin dynamics were performed at each training
iteration to generate samples from the learned score distributions, ensuring proper exploration of the state space
and stable estimates of the equilibrium distribution. However, running Langevin updates at every iteration is
computationally expensive. In the improved version, we keep the same number of Langevin steps per update but
reduce how frequently we perform these updates. By doing so, we rely on the improved score estimatesâ€”achieved
through better trace estimation and more frequent parameter updates via mini-batchingâ€”to maintain adequate
distributional coverage. This reduction in update frequency decreases computational overhead and can lead to faster
overall training convergence, without significantly compromising the quality of the learned distributions.
Combined Effect of Improvements. Taken together, the introduction of mini-batching, the use of Hutchinsonâ€™s
trace estimator for the divergence, and the reduction in Langevin steps each contribute to making the IH-MFCG-ACM algorithm more computationally efficient. Mini-batching allows multiple updates within each iteration, increasing
training throughput. The Hutchinsonâ€™s estimator streamlines the gradient computations for score matching, making
it feasible to use larger models or larger batch sizes. Reducing Langevin steps further decreases computational costs
and can lead to faster training iterations.
These improvements, while conceptually simple, jointly enhance the scalability and efficiency of the IH-MFCG-AC
algorithm, making it more practical for large-scale problems or more complex function approximators. In the following
sections, we will detail the experimental setup and present empirical evidence demonstrating these performance gains.

Page 10

Algorithm 3 IH-MFCG-AC-M: IH-MFCG-AC Algorithm with Mini-Batching
Require: Initial distribution Î¾; number of time steps N â‰« 0; discrete time step size âˆ†t; neural network learning
rates for actor ÏÎ  , critic ÏV , global score ÏÎ£ , and local score ÏÎ£
e ; Langevin dynamics step size Ïµ; batch size B,
minibatch size b (B divisible by b), number mini-batches C = B/b
1: Initialize neural networks:
Actor Î Ïˆ0 : Rd â†’ P(Rk )
Critic VÎ¸0 : Rd â†’ R
Target TÎ¸0 : Rd â†’ R, same as Critic
Global Score Î£Ï†0 : Rd â†’ Rd
e Î¾ : Rd â†’ Rd
Local Score Î£
0
2: Sample Xt0 = {Xt10 , Xt20 , Â· Â· Â· , XtB0 }, where Xti0 âˆ¼ Î¾
3: for n = 0, . . . , N âˆ’ 1 do
4:

Randomly Permute Xtn

5:

for m = 0, . . . , C âˆ’ 1 do

6:

Compute
score
loss
for
Î£
using
Hutchinsonâ€™s

P(m+1)Â·b âŠ¤
1
1
i
2
i
âˆ¥Î£
(X
)âˆ¥
z
âˆ‡
Î£
(X
)
+
Ï†
x
Ï†
tn 2
tn
n
n
i
i=mÂ·b+1
b
2

7:

Update Î£ with Adam:

8:

e
Compute score
loss
for
Î£
using Hutchinsonâ€™s
P(m+1)Â·b
1
1
âŠ¤
i
i
2
e
e
i=mÂ·b+1 zÌƒi âˆ‡x Î£Î¾n (Xtn ) + 2 âˆ¥Î£Î¾n (Xtn )âˆ¥2
b

9:

e with Adam:
Update Î£

10:

trace

estimator:

LÎ£ (Ï†n )

=

trace

estimator:

LÎ£
e (Î¾n )

=

Ï†n+1 = Ï†n âˆ’ ÏÎ£ âˆ‡Ï† LÎ£ (Ï†n )

Î¾n+1 = Î¾n âˆ’ ÏÎ£
e âˆ‡Î¾ LÎ£
e (Î¾n )



(1)
(2)
(k)
If n is divisible by 50, generate mean field samples Stn = Stn , Stn , . . . , Stn from Î£Ï†n+1 and Setn =


Pk
(1)
(2)
(k)
eÎ¾
Setn , Setn , . . . , Setn from Î£
using Langevin dynamics with step size Ïµ and compute ÂµStn := k1 i=1 Î´S (i)
n+1
tn
Pk
and ÂµSet := k1 i=1 Î´Se(i) .
n

tn

11:

Sample action for i = m Â· b + 1, m Â· b + 2, Â· Â· Â· , (m + 1) Â· b (vectorized):

Aitn ) âˆ¼ Î Ïˆn (Â· | Xtin ))

12:

Observe reward from environment for i = mÂ·b+1, mÂ·b+2, Â· Â· Â· , (m+1)Â·b:

i
rn+1
= âˆ’f (Xtin , ÂµStn , ÂµSet , Aitn )âˆ†t

13:

Observe next state from environment for i = m Â· b âˆš
+ 1, m Â· b + 2, Â· Â· Â· , (m + 1) Â· b (vectorized):
zn âˆ¼ N (0, 1)
Xtin+1 = b(Xtin , ÂµStn , Aitn )âˆ†t + Ïƒ(Xtin , ÂµStn , Aitn ) âˆ†t zn ,

14:

Compute TD target for i = m Â· b + 1, m Â· b + 2, Â· Â· Â· , (m + 1) Â· b (vectorized):

15:
16:

Compute TD error for i = m Â· b + 1, m Â· b + 2, Â· Â· Â· , (m + 1) Â· b (vectorized):
P(m+1)Â·b
Compute critic loss: LV (Î¸n ) = 1b i=mÂ·b+1 (Î´Î¸i n )2

17:

Update critic with Adam:

18:

Compute actor loss:

19:

Update actor with Adam:

20:

Î¸n+1 = Î¸n âˆ’ ÏV âˆ‡Î¸ LV (Î¸n )
P(m+1)Â·b
LÎ  (Ïˆn ) = 1b i=mÂ·b+1 âˆ’ Î´Î¸i n log Î Ïˆn (Aitn | Xtn )
Ïˆn+1 = Ïˆn âˆ’ ÏÎ  âˆ‡Ïˆ LÎ  (Ïˆn )

end for

21:
Tn+1 â† Vn+1 if n divisible by 200, else Tn+1 â† Tn
22: end for
23: return

eÎ¾ )
(Î ÏˆN , Î£Ï†N , Î£
N

Page 11

n

i
i
yn+1
= rn+1
+ eâˆ’Î²âˆ†t TÎ¸n (Xtin+1 )
i
Î´Î¸i n = yn+1
âˆ’ VÎ¸n (Xtin )

5.4

PPO, GAE, Entropy Regularization: IH-MFCG-AC-DRL

Algorithm 4 builds on top of Algorithm 2 by adding components commonly used in deep reinforcement learning.
The difference is highlighted in blue, and the mathematical formulation is detailed in the algorithm. To provide
a brief summary, first, we estimate the advantage function using Generalized Advantage Estimation (GAE) [12].
This approach reduces variance while maintaining low bias in policy gradient estimation. Note that we introduce a
trajectory buffer with a fixed rollout length to store the past trajectories for GAEâ€™s calculation, and we only update
the actor-critic network per rollout length.
Secondly, we replaced the actorâ€™s update rule with Proximal Policy Optimization (PPO) [13], which restricts the
policy update by clipping the probability ratio between old/new actors and improves stability. Note that we have also
considered TRPO [11] but have decided on PPO for efficiency concerns. Lastly, we introduced entropy regularization
in the actorâ€™s loss, which encourages exploration by preventing the policy from converging too quickly into a possibly
local optimum.
Please Continue Next Page

Page 12

Algorithm 4 IH-MFCG-AC-DRL: PPO with entropy regularization and GAE with Rollouts
Require: Initial distribution Î¾; number of time steps N â‰« 0; discrete time step size âˆ†t; neural network learning
rates for actor ÏÎ  , critic ÏV , global score ÏÎ£ , and local score ÏÎ£
e ; Langevin dynamics step size Ïµ; batch size B;
rollout length M ; PPO clipping parameter Ïµclip ; entropy coefficient cent ; GAE coeff Î»; discount Î³ = eÎ²âˆ†t
1: Initialize neural networks:
Actor Î Ïˆ0 : Rd â†’ P(Rk )
Critic VÎ¸0 : Rd â†’ R, Target TÎ¸0 : Rd â†’ R, same as Critic
Global Score Î£Ï†0 : Rd â†’ Rd
e Î¾ : Rd â†’ Rd
Local Score Î£
0
2: Sample Xt0 = {Xt10 , Xt20 , Â· Â· Â· , XtB0 }, where Xti0 âˆ¼ Î¾
3: Storage for Rollout of size M : D = {(Xtim , Aitm , rtim +1 , Xtim +1 ) | m = 0, . . . , M âˆ’ 1; i = 1, . . . , B}
4: for n = 0, . . . , N âˆ’ 1 do
5:
6:
7:
8:
9:

10:
11:
12:
13:
14:
15:


PB
2
Compute score loss for Î£: LÎ£ (Ï†n ) = B1 i=1 tr âˆ‡x Î£Ï†n (Xtin ) + 21 Î£Ï†n (Xtin ) 2
Update Î£ with Adam: Ï†n+1 = Ï†n âˆ’ ÏÎ£ âˆ‡Ï† LÎ£ (Ï†n )


2
PB
1 e
1
i
i
e
e
Compute score loss for Î£:
LÎ£
e (Î¾n ) = B
i=1 tr âˆ‡x Î£Î¾n (Xtn ) + 2 Î£Î¾n (Xtn )
e with Adam:
Update Î£

2

Î¾n+1 = Î¾n 
âˆ’ ÏÎ£
e âˆ‡Î¾ LÎ£
e (Î¾n )



(1)
(k)
(1)
(k)
eÎ¾
Generate mean field samples Stn = Stn , . . . , Stn from Î£Ï†n+1 and Setn = Setn , . . . , Setn from Î£
using
n+1
Langevin dynamics with step size Ïµ, and compute ÂµStn and ÂµSet .
n
Sample action for i = 1, 2, ..., B (vectorized): Aitn âˆ¼ Î Ïˆn (Â· | Xtin )
i
Observe reward from environment for i = 1, 2, ..., B: rn+1
= âˆ’f (Xtin , ÂµStn , ÂµSet , Aitn )âˆ†t
n
Observe next state from environment for i = 1, 2, âˆš
..., B (vectorized):
Xtin+1 = b(Xtin , ÂµStn , Aitn )âˆ†t + Ïƒ(Xtin , ÂµStn , Aitn ) âˆ†t zn , zn âˆ¼ N (0, 1)
i
Store the transition in D: D â† D âˆª {(Xtin , Aitn , rn+1
, Xtin+1 )B
i=1 }
if (n + 1) % M = 0 then
For m = M âˆ’ 1 down to 0, and for each i = 1, . . . , B:
i
i
Î´m
= rm+1
+ Î³VÎ¸n (Xtim+1 ) âˆ’ VÎ¸n (Xtim ),

16:
17:
18:

i
AÌ‚itm = Î´m
+ Î³Î»AÌ‚itm+1

1
Compute critic loss using GAE returns: LV (Î¸n ) = BM
Update critic with Adam: Î¸n+1 = Î¸n âˆ’ ÏV âˆ‡Î¸ LV (Î¸n )

with AÌ‚itM = 0,

RÌ‚tim = AÌ‚itm + VÎ¸n (Xtim )

PM âˆ’1 PB
m=0

i
i
2
i=1 (RÌ‚tm âˆ’ VÎ¸n (Xtm ))
Î Ïˆ (Ai

|X i )

.Compute
Let Ïˆold = Ïˆn (old actor before update). Define the probability ratio rtim (Ïˆn ) = Î Ïˆ n (Atim |Xtm
i
tm
tm )
old
actor loss using PPO with clipping and entropy regularization over the entire rollout:
LÎ  (Ïˆn ) =


M âˆ’1 B 

1 XX
min rtim (Ïˆn )AÌ‚itm , clip(rtim (Ïˆn ), 1 âˆ’ Ïµclip , 1 + Ïµclip )AÌ‚itm + cent H(Î Ïˆn (Â· | Xtim ))
BM m=0 i=1

19:
Update actor with Adam: Ïˆn+1 = Ïˆn âˆ’ ÏÎ  âˆ‡Ïˆ LÎ  (Ïˆn )
20:
Clear the rollout storage: D = âˆ…
21:
else
22:
Ïˆn+1 = Ïˆn , Î¸n+1 = Î¸n (just carry forward)
23:
24:
end if
25:
Tn+1 â† Vn+1 if n divisible by 200, else Tn+1 â† Tn
26: end for
27: return

eÎ¾ )
(Î ÏˆN , Î£Ï†N , Î£
N

Page 13

6

Experimental Results

6.1

The Linear-Quadratic Benchmark

To compare our proposed algorithms against the baseline, we choose the linear-quadratic (LQ) control problem in the
original paper [4] as the evaluation benchmark. The LQ problem is formulated as the minimization of the following:
"Z

 #
âˆ
2
2
Î±,Âµ
Î±,Âµ
Î±,Âµ
âˆ’Î²t 1 2
Î±,Âµ 2
Î±,Âµ 2
E
e
Î± + c1 (Xt âˆ’ c2 m) + c3 (Xt âˆ’ c4 )
+ cÌƒ1 (Xt âˆ’ cÌƒ2 m ) + cÌƒ5 (m ) dt
2 t
0
subject to the dynamics

dXtÎ±,Âµ = Î±t dt + Ïƒ dWt ,

t âˆˆ [0, âˆ)

where m = x dÂµ(x) and m
= x dÂµ (x) and the fixed point condition m = limtâ†’âˆ E(XtÎ±Ì‚,Âµ ) = mÎ±Ì‚,Âµ where Î±Ì‚ is
the optimal action. The LQ problem has an analytical solution, where we could compute the value function, optimal
control, and the limiting distribution of global/local distributions in analytical form. We can therefore evaluate the
success of our algorithm by comparing our learned solutions against these analytical solutions. For brevity, we omit
the analytical forms of these solutions here; they can be found on page 23 of the original paper and we kept it in
appendix 9.1 as reference.
R

6.2

Î±,Âµ

R

Î±,Âµ

Experimental Setup

LQ Benchmark For all experiments, we construct the LQ problem using the following parameters: c1 = 0.5,
c2 = 1.5, c3 = 0.5, c4 = 0.25, cÌƒ1 = 0.3, cÌƒ2 = 1.25, cÌƒ5 = 0.25, discount factor Î² = 1
Neural Network Config For all experiments, we used the same networks for the critic and the global/local scores:
a feed-forward neural network with one hidden layer of size 128 and a Tanh activation function, The actor network
is a feed-forward neural network with 1 hidden layer (with size 64 and Tanh activation function) plus two separate
â€œhead" layers of size 64 to respectively predict the mean and standard deviation. We put a softmax layer on top of
the â€œhead" for the standard deviation layer to ensure positive output.
Hyperparameters We used the same configuration as in the original paper [4]. We used (ÏÎ  , ÏV , ÏÎ£ , ÏÎ£
e) =
(5 Ã— 10âˆ’6 , 10âˆ’5 , 10âˆ’6 , 5 Ã— 10âˆ’4 ). For the Langevin dynamics sampling, we use Ïµ = 5 Ã— 10âˆ’2 for 200 iterations,
drawing k = 1000 samples. For all experiments, we also used learning rate scheduling as follows: For the first 10%
steps, linearly increase all learning rate 10 times; for the rest 90% steps, linearly reduce learning rate to 25% of original.
We empirically observe that this approach seems to expedite convergence. For newly introduced hyperparameters,
we use B = 8192 for batch size. We use M = 256, Ïµclip = 0.2, cent = 0.01, Î³ = 0.95, Î² = 1. For the mini-batch
algorithm, we used the minibatch size b = 1024. We train all algorithms for 200,000 steps (N = 200, 000) except
the IH-MFCG-AC-M algorithm. It is noted in the original paper that the baseline algorithm requires 2,000,000
steps to converge, which means that we expect the baseline algorithm to not converge in our experiments. For the
IH-MFCG-AC-M algorithm, we empirically observed that it converges faster and we decided to train it in only 20,000
steps to show its fast convergence.
Evaluation We use the global/local score networks to draw samples of the global/local distributions and compare
them against the analytical limiting distribution. We use the Critic network to compute the value function and
compare it against the ground truth value function. We run the learning algorithms 5 times and compote the
average along with uncertainty measures.

6.3

Results: IH-MFCG-AC Baseline

Figure 1 shows the learned solutions against the analytical solutions using the baseline IH-MFCG-AC algorithm.
We see that the global/local score and the value function are still pretty different from the analytical solutions. The

Page 14

local distribution is particularly different, shifting horizontally from the theoretical limiting distribution. This is
anticipated, as the original paper reports that it takes 2,000,000 steps for the IH-MFCG-AC algorithm to converge
on the LQ problem, whereas we only trained for 200,000 steps, only 10% of the required amount.

(a) Learned global/local distribution vs Solution

(b) Learned value function vs Solution

Figure 1: Learned global/local distributions and value function using IH-MFCG-AC v.s. theoretical solution

6.4

Results: IH-MFCG-AC-B

Figure 2 shows the learned solutions against the analytical solutions using the baseline IH-MFCG-AC-B algorithm.
We notice that with the use of batching and the target network, The algorithm learns better and is substantially
closer to the analytical solution compared to the baseline, especially on the value function and the local distribution.

(a) Learned global/local distribution vs Solution

(b) Learned value function vs Solution

Figure 2: Learned global/local distributions and value function using IH-MFCG-AC-B v.s. theoretical solution

6.5

Results: IH-MFCG-AC-M

Figure 3 shows the learned solutions against the analytical solutions using the baseline IH-MFCG-AC-M algorithm.
Note that we only run 20,000 steps for this algorithm, which is 10% of the standard amount. We see that the
algorithm converged very well, both in terms of local/global distribution and value function, and it achieved it with a
much smaller number of steps. It is noted that the controls are slightly less well-learned compared to the IH-MFCGAC-B experiment with 200,0000 steps, suggesting that it might benefit from slightly more training (e.g., another
10,000 steps).

Page 15

(a) Learned global/local distribution vs Solution

(b) Learned value function vs Solution

Figure 3: Learned global/local distributions and value function using IH-MFCG-AC-M v.s. theoretical solution

6.6

Results: IH-MFCG-AC-DRL

Figure 4 shows the learned solutions against the analytical solutions using the baseline IH-MFCG-AC-DRL algorithm.
We see that the PPO/GAE-based algorithm did not converge at all. It is also worse than the baseline algorithm.
Several discussion on the potential reason of failure is included in section 7.1

(a) Learned global/local distribution vs Solution

(b) Learned value function vs Solution

Figure 4: Learned global/local distributions and value function using IH-MFCG-AC-DRL v.s. theoretical solution

7

Discussion

In this work, we have shown that deep RL methods can approximate equilibrium solutions to Mean Field Control
Games (MFCGs) without directly solving coupled partial differential equations. By using standard reinforcement
learning tools, we provide a scalable framework that can handle high-dimensional state spaces and complex cost
structures.

7.1

Results v.s. Expectations

In the last section, we see that the baseline IH-MFCG-AC algorithm did not converge to the theoratical solution.
This is expected as we only run 10% of the report steps required for convergence. We see that IH-MFCG-AC-B
algorithm, which introduces batching, is substantially better than the baseline algorithm in terms of convergence
due to better (e.g., low variance) estimation (e.g., policy gradient) at each step. In fact, we observe that the IHMFCG-AC-B algorithm is close to convergence at 100,000 steps, at half of the standard. We record the performance
of this experiment in appendix 9.2. IH-MFCG-AC-M converges even faster at 20,000 steps. This is surprising but

Page 16

not plausible, as the mini-batching procedure updates multiple times within a step, which is somewhat similar to
running more total steps.
It is surprising that IH-MFCG-AC-DRL did not perform very well. It is possible that these two algorithm are
empirically proven to be successful in substantially more complicated, real-life-like tasks, and the LQ problem may
be too simple for these two techniques to be successful. i.e., PPO and GAE may be a â€œoverkill" for the problem that
we are trying to solve. Moreover, IH-MFCG-AC-DRL introduces a lot more hyperparameter (e.g., PPO clipping
parameter Ïµclip , GAE coefficient Î», rollout length M , etc.), which makes hyperparameter tuning more difficult and it
is likely that we have not found the proper combination. It also changes the update mechanism (i.e., we only update
actor and critic once we fill the trajectory buffer, but the local/global score functions are still updated per step),
which might introduce unexpected effect to the algorithm.

7.2

Computation Runtime

The IH-MFCG-AC-B has almost the same latency as the baseline algorithm due to vectorization and GPU acceleration, highlighting its advantage. This advantage holds for any environment that is quick to obtain responses from
the current state-action pairs. In this case, the environment is vectorized, making it very fast to obtain responses
for vectorized input. However, the algorithm may take more time for an environment that is not vectorized or slow
to obtain responses from (e.g., complex environments that require running a costly physics engine or simulations).
The IH-MFCG-AC-DRL algorithm is around twice as slow as the baseline, primarily due to the costly PPO update
operation and the memory overhead introduced by the need to maintain the trajectory buffer.
While the IH-MFCG-AC-B algorithm did not impose additional latency, the IH-MFCG-AC-B imposed latency by
introducing another layer of for-loop. Suppose the cost of running batch and minibatch through the learning process
is approximately the same (true for small networks with fast GPUs), then the IH-MFCG-AC-M algorithm should be
C times slower than the IH-MFCG-AC-B algorithm, where C is the number of mini-batches. However, IH-MFCGAC-M may be preferred over the IH-MFCG-AC-B algorithm in scenarios like (1) we can parallelize the learning on
mini-batches on multiple processes/GPUs, and (2) the cost/latency of obtaining the response from the environment
is high. (2) means that time is mostly spent on looping over all samples to get environmental feedback, which means
the runtime will be similar between looping over all samples together (Batch) and looping over all samples piecewise
(Mini-Batch).

7.3

Limitations

A number of assumptions and simplifications have facilitated our approach:
â€¢ Agent homogeneity and symmetry: We assume a continuum of identical agents, which justifies the mean
field approximation. Heterogeneous agent populations may require more sophisticated modeling.
â€¢ Choice of distribution approximation: The score-matching approach provides a flexible way to represent
continuous-state distributions, but it can be sensitive to hyperparameters and may require careful tuning.
Alternative generative models or density estimation methods could be explored to improve robustness and
scalability.
â€¢ Theoratical Guarantee Despiite that our algorithm outperforms baseline, it still lacks theoratical guarantee.
It would be interesting to see theoretically whether our algorithm improves efficiency.
â€¢ Ablations Our proposed algorithms contain several added component compared to the previous version. Due to
the limit in compute resource and time, we did not perform an ablation study to investigate which component is
most responsible for the observed performance. For example, in the IH-MFCG-AC-DRL algorithm, it is possible
that GAE would improve performance, but PPO is mainly responsible for the performance downgrade.

Page 17

7.4

Future Direction

Refining Distribution Learning. While we have employed a score-matching approach to represent the mean
field distribution, future work might investigate other generative modeling techniques, such as normalizing flows,
GANs, or VAEs, to enhance stability, reduce sensitivity to hyperparameters, and handle higher-dimensional state
spaces.
Heterogeneous Populations and Complex Dynamics. Our methodology has focused on homogeneous populations and Markovian dynamics. Extending the framework to handle heterogeneous agents with differing objectives,
or non-Markovian dynamics that require memory and partial observability, would broaden its applicability.
Integration of Model-Based and Model-Free Approaches. We could combine model-based techniques that
leverage partial knowledge of system dynamics with model-free RL. It might improve convergence speed and offer
better theoretical guarantees.
Alternative RL Architectures and Algorithms. We could explore the application of other algorithms and
architectures in RL on MFCGs. For example, we could try to build Q-learning variants (e.g., DQN) to solve the
MFCG. For larger and more complicated tasks, we could explore the possibility of scaling up not only the sampling
technique but the model itself (e.g., transformer-based networks or diffusion policies).

8

Conclusion and Takeaway

This paper presented a scalable, data-driven approach to solving Mean Field Control Games (MFCGs) by leveraging
deep Reinforcement Learning (RL) techniques. By reformulating the MFCG problem as a Markov Decision Process
and approximating both the representative agentâ€™s policy and the population distribution, we circumvent the need
to solve high-dimensional partial differential equations associated with the Hamilton-Jacobi-Bellman and FokkerPlanck systems. We adopt an algorithm from prior work and increase its efficiency and scalability by introducing
batching and a target network. We name this algorithm IH-MFCG-AC-B. Building on it, we devised a new algorithm
called IH-MFCG-AC-DRL, where we replaced to update the actor using PPO; used generalized advantage estimation
(GAE) to estimate the advantage; and added an entropy regularizer. We also try to improve IH-MFCG-AC-B by
introducing mini-batching and better trace estimation for score matching, leading to the algorithm IH-MFCG-AC-M.
We evaluate our algorithm on a linear-quadratic benchmark where the analytical equilibrium solution is known.
We observe that in resource-constrained settings (i.e., we only run 10% steps required for the baseline algorithm
to converge), the baseline algorithm did not converge. IH-MFCG-AC-B converged, and in fact it converged with
even fewer steps. IH-MFCG-AC-M converges faster, requiring only 10% of steps required for IH-MFCG-AC-B to
converge (which is 1% of steps required for IH-MFCG-AC baseline to converge). However, the IH-MFCG-AC-DRL
did not converge with the given steps, possibly due to under-tuned hyperparameters and the fact that the additional
techniques are usually more effective for tasks substantially more complex than the LQ benchmark.
In summary, we successfully replicated the algorithm and results from a recent paper [4], proposed several versions
of the modified algorithm to improve efficiency and scalability and have seen empirical success. Our work lays the
foundation for more advanced techniques to address more complicated MFCGs closely tied to real-life scenarios,
such as large-scale autonomous transportation systems, multi-firm economic competition, and inter-bank borrowing
problems.

Page 18

References
[1] Yves Achdou, Fabio Camilli, and Italo Capuzzo-Dolcetta. Mean field games: numerical methods for the planning
problem. SIAM Journal on Control and Optimization, 50(1):77â€“109, 2012.
[2] Yves Achdou and Alessio Porretta. Mean field control problems: a probabilistic approach. Applied Mathematics
& Optimization, 81:971â€“997, 2020.
[3] Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, Mathieu Lauriere, and Jimin Lin. Reinforcement learning
algorithm for mixed mean field control games, 2023.
[4] Andrea Angiuli, Jean-Pierre Fouque, Ruimeng Hu, and Alan Raydan. Deep reinforcement learning for infinite
horizon mean field problems in continuous spaces, 2024.
[5] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156â€“
172, 2008.
[6] RenÃ© Carmona. Lectures on BSDEs, Stochastic Control, and Stochastic Differential Games with Financial
Applications. SIAM, 2016.
[7] RenÃ© Carmona and FranÃ§ois Delarue. Probabilistic theory of mean field games with applications I-II. Springer,
2018.
[8] Minyi Huang, Peter E Caines, and Roland P MalhamÃ©. Large-population cost-coupled lqg problems with nonuniform agents: Individual-mass behavior and decentralized Îµ-nash equilibria. IEEE Transactions on Automatic
Control, 52(9):1560â€“1571, 2007.
[9] Aapo HyvÃ¤rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine
Learning Research, 6(24):695â€“709, 2005.
[10] Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese Journal of Mathematics, 2(1):229â€“260,
2007.
[11] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy
optimization, 2017.
[12] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous
control using generalized advantage estimation, 2018.
[13] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms, 2017.
[14] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances
in neural information processing systems, 32, 2019.
[15] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. MÃ¼ller, editors, Advances in
Neural Information Processing Systems, volume 12. MIT Press, 1999.
[16] Kaiqing Zhang, Zhuoran Yang, Tao Liu, and Tamer BaÅŸar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pages 321â€“384, 2021.

Page 19

9

Appendix

9.1

Appendix 1: Analytical Solution of LQ problem

Disclosure: The following content in this subsection is adapted from the original paper [4].
We present the analytic solution to the MFCG problem using notation consistent with the derivation in [3]. The
value function is defined as
"Z

âˆ
1 2
2
2
v(x) := inf E
eâˆ’Î²t
Î±t + c1 (XÎ±,Âµ
âˆ’ c2 m) + c3 (XÎ±,Âµ
âˆ’ c4 )
t
t
Î±âˆˆA
2
0
#
(5)

2

+ cÌƒ1 (XÎ±,Âµ
âˆ’ cÌƒ2 mÎ±,Âµ ) + cÌƒ5 (mÎ±,Âµ )
t

2

dt | X0 = x .

The explicit formula v(x) = Î“2 x2 + Î“1 x + Î“0 can be derived as the solution to the Hamilton-Jacobi-Bellman equation
where
p
âˆ’Î² + Î² 2 + 8 (c1 + c3 + cÌƒ1 )
Î“2 =
4
Î“1 = âˆ’

2Î“2 c3 c4
2

c1 (1 âˆ’ c2 ) + cÌƒ1 (1 âˆ’ cÌƒ2 ) + c3 + cÌƒ5


2
c1 c22 m2 + cÌƒ1 cÌƒ22 + cÌƒ5 (mÎ±,Âµ ) + Ïƒ 2 Î“2 âˆ’ 12 Î“21 + c3 c24
.
Î“0 =
Î²
Then the optimal control for the MFCG is
(6)

Î±Ì‚(x) = âˆ’(2Î“2 x + Î“1 ).
Substituting them yields the Ornstein-Uhlenbeck process
dXt = âˆ’ (2Î“2 Xt + Î“1 ) dt + Ïƒ dWt
whose limiting distribution is
Î±Ì‚,ÂµÌ‚

ÂµÌ‚ = Âµ


=N

Î“1 Ïƒ 2
âˆ’
,
2Î“2 4Î“2



(7)

.

We note that an equation for mÌ‚ and mÎ±Ì‚,ÂµÌ‚ that only depends on the running cost coefficients is
m := mÌ‚ = mÎ±Ì‚,ÂµÌ‚ =

c3 c4
2

c1 (1 âˆ’ c2 ) + cÌƒ1 (1 âˆ’ cÌƒ2 ) + c3 + cÌƒ5

Page 20

.

(8)

9.2

Appendix 2: IH-MFCG-AC-B on LQ benchmark with 100,000 steps

The figures below shows the local/global distribution and the learned value function of the IH-MFCG-AC-B algorithm
on LQ benchmark trained with 100,000 steps. This is a half of the steps compared to all other experiments, and we
observe that the algorithm is close to converging into the theoratical solution.

(a) Learned global/local distribution vs Solution

Page 21

(b) Learned value function vs Solution

