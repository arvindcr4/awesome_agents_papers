Paper: REBEL_Reinforcement_Learning.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
REBEL: Reinforcement Learning via Regressing Relative Rewards  arXiv:2404.16767v4 [cs.LG] 10 Dec 2024  Zhaolin Gao Cornell University zg292@cornell.edu Wenhao Zhan Princeton University wenhao.zhan@princeton.edu Kiant√© Brantley Harvard University kdbrantley@g.harvard.edu  Jonathan D. Chang Cornell University jdc396@cornell.edu Owen Oertell Cornell University ojo2@cornell.edu  Gokul Swamy Carnegie Mellon University gswamy@andrew.cmu.edu  Thorsten Joachims Cornell University tj@cs.cornell.edu  J. Andrew Bagnell Aurora Innovation, CMU bagnell2@andrew.cmu.edu  Jason D. Lee Princeton University jaso...

Specific Evidence for Classification:
- "Lee Princeton University jasonlee@princeton.edu  Wen Sun Cornell University ws455@cornell.edu  Abstract While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
