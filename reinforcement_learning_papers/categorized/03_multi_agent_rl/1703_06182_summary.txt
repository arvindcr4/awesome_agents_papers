Here is the extracted information in the requested format:

**PAPER:** Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability
**TITLE:** Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability
**ARXIV_ID:** 1703.06182v4
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** This paper introduces a decentralized multi-task multi-agent reinforcement learning approach under partial observability. The approach combines hysteretic learners, Deep Recurrent Q-Networks (DRQNs), Concurrent Experience Replay Trajectories (CERTs), and distillation to achieve multi-agent coordination using a single joint policy in a set of Dec-POMDP tasks with sparse rewards.
**KEY_CONTRIBUTIONS:**
* Introduction of a decentralized multi-task multi-agent reinforcement learning approach under partial observability
* Combination of hysteretic learners, DRQNs, CERTs, and distillation to achieve multi-agent coordination
* Evaluation of the approach on a series of increasingly challenging domains, including multi-agent single-target and multi-agent multi-target capture domains
* Demonstration of the approach's ability to learn a single joint policy that performs well in all tasks without explicit provision of task identity
* Comparison with existing approaches, including Dec-DRQN and Multi-HDRQN, and demonstration of the proposed approach's superior performance.
