{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶é Fine-Tune LLMs with Axolotl\n",
    "\n",
    "This notebook guides you through fine-tuning LLMs using **Axolotl** - a powerful open-source fine-tuning framework.\n",
    "\n",
    "**Why Axolotl?**\n",
    "- üÜì **Completely free** (runs on Colab's free GPU)\n",
    "- üéõÔ∏è **Full control** over training configuration\n",
    "- üì¶ **Many techniques**: LoRA, QLoRA, full fine-tuning, RLHF\n",
    "- ü§ó **HuggingFace integration** for easy model sharing\n",
    "- üîß **Highly configurable** via YAML configs\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Set up Axolotl on Google Colab\n",
    "2. Prepare training data\n",
    "3. Configure training with YAML\n",
    "4. Run QLoRA fine-tuning\n",
    "5. Test and export your model\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU (free T4 works!)\n",
    "- HuggingFace account (for model download/upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check GPU & Runtime\n",
    "\n",
    "‚ö†Ô∏è **Important:** Make sure you're using a GPU runtime!\n",
    "\n",
    "Go to: `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU` (or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected! Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Axolotl and dependencies\n",
    "# This takes ~5-10 minutes on first run\n",
    "\n",
    "!pip install axolotl[flash-attn] -q\n",
    "!pip install accelerate transformers bitsandbytes peft trl datasets -q\n",
    "\n",
    "print(\"\\n‚úÖ Axolotl installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Install from source for latest features\n",
    "# Uncomment if you need bleeding-edge features\n",
    "\n",
    "# !git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "# %cd axolotl\n",
    "# !pip install packaging ninja\n",
    "# !pip install -e '.[flash-attn,deepspeed]'\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (needed for gated models like Llama)\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    HF_TOKEN = input(\"Enter your HuggingFace token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data\n",
    "\n",
    "Axolotl supports many data formats. We'll use the **conversation** format (similar to ChatML):\n",
    "\n",
    "```json\n",
    "{\"conversations\": [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "Other supported formats:\n",
    "- `alpaca`: `{\"instruction\": \"\", \"input\": \"\", \"output\": \"\"}`\n",
    "- `sharegpt`: `{\"conversations\": [{\"from\": \"human\", \"value\": \"\"}, {\"from\": \"gpt\", \"value\": \"\"}]}`\n",
    "- `completion`: `{\"text\": \"full text to train on\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sample Dataset: Technical Writing Assistant\n",
    "# Replace with your own domain-specific data!\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert technical writer. You help developers write clear, concise documentation.\n",
    "Your responses should:\n",
    "- Use simple, direct language\n",
    "- Include code examples where helpful\n",
    "- Follow standard documentation conventions\n",
    "- Be well-structured with headers and lists\"\"\"\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"Write a docstring for a function that calculates the factorial of a number.\"},\n",
    "            {\"role\": \"assistant\", \"content\": '''Here's a well-documented factorial function:\n",
    "\n",
    "```python\n",
    "def factorial(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the factorial of a non-negative integer.\n",
    "    \n",
    "    The factorial of n (written as n!) is the product of all positive\n",
    "    integers less than or equal to n.\n",
    "    \n",
    "    Args:\n",
    "        n: A non-negative integer. Must be >= 0.\n",
    "    \n",
    "    Returns:\n",
    "        The factorial of n. Returns 1 if n is 0.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If n is negative.\n",
    "        TypeError: If n is not an integer.\n",
    "    \n",
    "    Examples:\n",
    "        >>> factorial(5)\n",
    "        120\n",
    "        >>> factorial(0)\n",
    "        1\n",
    "        >>> factorial(1)\n",
    "        1\n",
    "    \n",
    "    Note:\n",
    "        For large values of n, consider using math.factorial()\n",
    "        which is implemented in C and handles arbitrary precision.\n",
    "    \"\"\"\n",
    "    if not isinstance(n, int):\n",
    "        raise TypeError(f\"Expected int, got {type(n).__name__}\")\n",
    "    if n < 0:\n",
    "        raise ValueError(f\"Factorial not defined for negative numbers: {n}\")\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * factorial(n - 1)\n",
    "```\n",
    "\n",
    "This docstring follows Google's Python style guide and includes all essential sections.'''}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"How do I write a good README for my open source project?\"},\n",
    "            {\"role\": \"assistant\", \"content\": '''# Writing a Great README\n",
    "\n",
    "A good README is the front door to your project. Here's a proven structure:\n",
    "\n",
    "## Essential Sections\n",
    "\n",
    "### 1. Project Title & Description\n",
    "```markdown\n",
    "# Project Name\n",
    "\n",
    "A brief, clear description of what your project does and why it exists.\n",
    "Keep it to 1-2 sentences.\n",
    "```\n",
    "\n",
    "### 2. Quick Start / Installation\n",
    "```markdown\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install your-package\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```python\n",
    "from your_package import main_function\n",
    "result = main_function(\"example\")\n",
    "```\n",
    "```\n",
    "\n",
    "### 3. Usage Examples\n",
    "Show 2-3 common use cases with working code.\n",
    "\n",
    "### 4. API Reference (or link to docs)\n",
    "For larger projects, link to full documentation.\n",
    "\n",
    "## Optional But Valuable\n",
    "\n",
    "- **Badges**: Build status, version, license\n",
    "- **Screenshots/GIFs**: For visual projects\n",
    "- **Contributing guide**: How others can help\n",
    "- **License**: Always include this\n",
    "- **Changelog**: Link to CHANGELOG.md\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "1. **Test your install instructions** on a clean machine\n",
    "2. **Keep examples runnable** - copy-paste should work\n",
    "3. **Update regularly** - stale docs hurt adoption\n",
    "4. **Link to issues** for known limitations'''}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"Explain the difference between REST and GraphQL APIs for a docs page.\"},\n",
    "            {\"role\": \"assistant\", \"content\": '''# REST vs GraphQL: Choosing Your API Style\n",
    "\n",
    "This guide helps you understand when to use REST or GraphQL for your API.\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Aspect | REST | GraphQL |\n",
    "|--------|------|--------|\n",
    "| Data fetching | Multiple endpoints | Single endpoint |\n",
    "| Response shape | Fixed by server | Defined by client |\n",
    "| Versioning | URL-based (v1, v2) | Schema evolution |\n",
    "| Caching | HTTP caching built-in | Requires custom setup |\n",
    "| Learning curve | Lower | Higher |\n",
    "\n",
    "## REST API\n",
    "\n",
    "**Best for:** Simple CRUD operations, public APIs, caching-heavy workloads.\n",
    "\n",
    "```bash\n",
    "# Get a user\n",
    "GET /api/users/123\n",
    "\n",
    "# Get user's posts (separate request)\n",
    "GET /api/users/123/posts\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Industry standard, well understood\n",
    "- Native HTTP caching\n",
    "- Stateless and scalable\n",
    "\n",
    "**Cons:**\n",
    "- Over-fetching (get more data than needed)\n",
    "- Under-fetching (multiple round trips)\n",
    "- Rigid response structure\n",
    "\n",
    "## GraphQL\n",
    "\n",
    "**Best for:** Complex data relationships, mobile apps, rapid frontend iteration.\n",
    "\n",
    "```graphql\n",
    "# Get user AND posts in one request\n",
    "query {\n",
    "  user(id: \"123\") {\n",
    "    name\n",
    "    email\n",
    "    posts(limit: 5) {\n",
    "      title\n",
    "      createdAt\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Fetch exactly what you need\n",
    "- Single request for complex data\n",
    "- Strongly typed schema\n",
    "- Great developer tooling\n",
    "\n",
    "**Cons:**\n",
    "- More complex to implement\n",
    "- Caching requires extra work\n",
    "- Potential for expensive queries\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "- **Choose REST** if you need simplicity, caching, or a public API\n",
    "- **Choose GraphQL** if you have complex data needs or mobile clients\n",
    "- **Consider both** - they can coexist in the same application'''}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"Write API documentation for a POST endpoint that creates a new user.\"},\n",
    "            {\"role\": \"assistant\", \"content\": '''# Create User\n",
    "\n",
    "Creates a new user account in the system.\n",
    "\n",
    "```\n",
    "POST /api/v1/users\n",
    "```\n",
    "\n",
    "## Authentication\n",
    "\n",
    "Requires API key with `users:write` scope.\n",
    "\n",
    "```bash\n",
    "Authorization: Bearer <api_key>\n",
    "```\n",
    "\n",
    "## Request Body\n",
    "\n",
    "| Field | Type | Required | Description |\n",
    "|-------|------|----------|-------------|\n",
    "| `email` | string | Yes | User's email address. Must be unique. |\n",
    "| `name` | string | Yes | Display name (2-100 characters). |\n",
    "| `password` | string | Yes | Password (min 8 chars, 1 uppercase, 1 number). |\n",
    "| `role` | string | No | User role. Default: `\"member\"`. Options: `\"admin\"`, `\"member\"`, `\"viewer\"`. |\n",
    "\n",
    "### Example Request\n",
    "\n",
    "```bash\n",
    "curl -X POST https://api.example.com/api/v1/users \\\\\n",
    "  -H \"Authorization: Bearer sk_live_xxx\" \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\n",
    "    \"email\": \"jane@example.com\",\n",
    "    \"name\": \"Jane Smith\",\n",
    "    \"password\": \"SecurePass123\",\n",
    "    \"role\": \"member\"\n",
    "  }'\n",
    "```\n",
    "\n",
    "## Response\n",
    "\n",
    "### Success (201 Created)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"usr_abc123\",\n",
    "  \"email\": \"jane@example.com\",\n",
    "  \"name\": \"Jane Smith\",\n",
    "  \"role\": \"member\",\n",
    "  \"created_at\": \"2024-01-15T10:30:00Z\",\n",
    "  \"updated_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Errors\n",
    "\n",
    "| Status | Code | Description |\n",
    "|--------|------|-------------|\n",
    "| 400 | `invalid_request` | Missing or invalid fields |\n",
    "| 401 | `unauthorized` | Invalid or missing API key |\n",
    "| 409 | `email_exists` | Email already registered |\n",
    "| 422 | `validation_error` | Password doesn't meet requirements |\n",
    "\n",
    "### Error Example\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"error\": {\n",
    "    \"code\": \"validation_error\",\n",
    "    \"message\": \"Password must contain at least one uppercase letter\",\n",
    "    \"field\": \"password\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Rate Limits\n",
    "\n",
    "- 100 requests per minute per API key\n",
    "- Returns `429 Too Many Requests` when exceeded'''}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìä Sample dataset with {len(sample_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Your Contribution: Add Training Examples\n",
    "\n",
    "`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`\n",
    "**Axolotl advantage:** Since you control the full training process, you can:\n",
    "- Use larger datasets without upload limits\n",
    "- Experiment with different LoRA ranks\n",
    "- Try different prompt templates\n",
    "- Fine-tune on multiple GPUs locally later\n",
    "`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`\n",
    "\n",
    "**Your task:** Add examples specific to your documentation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Add your training examples\n",
    "# ============================================================\n",
    "\n",
    "def create_doc_example(user_request: str, assistant_response: str) -> dict:\n",
    "    \"\"\"Helper to create a documentation training example.\"\"\"\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_request},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Add your examples:\n",
    "my_examples = [\n",
    "    # Example: Uncomment and customize\n",
    "    # create_doc_example(\n",
    "    #     user_request=\"Document this function...\",\n",
    "    #     assistant_response=\"Here's the documentation...\"\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# Combine all data\n",
    "all_training_data = sample_data + my_examples\n",
    "print(f\"üìä Total training examples: {len(all_training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data to JSONL\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = DATA_DIR / \"train.jsonl\"\n",
    "\n",
    "with open(TRAIN_FILE, 'w') as f:\n",
    "    for example in all_training_data:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Saved {len(all_training_data)} examples to {TRAIN_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Axolotl Configuration\n",
    "\n",
    "Axolotl uses YAML configuration files. This is where you define:\n",
    "- Base model\n",
    "- Training method (LoRA, QLoRA, full)\n",
    "- Hyperparameters\n",
    "- Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Axolotl Configuration for QLoRA Fine-tuning\n",
    "# ============================================================\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Good for T4 GPU\n",
    "# Alternatives:\n",
    "# - \"meta-llama/Meta-Llama-3.1-8B-Instruct\" (needs HF access)\n",
    "# - \"microsoft/phi-2\" (smaller, faster)\n",
    "# - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" (very small, for testing)\n",
    "\n",
    "OUTPUT_DIR = \"./outputs/tech-writer-qlora\"\n",
    "\n",
    "config = f\"\"\"\n",
    "# Base model\n",
    "base_model: {BASE_MODEL}\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "# Load in 4-bit for QLoRA (fits on T4 GPU)\n",
    "load_in_4bit: true\n",
    "adapter: qlora\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "\n",
    "# Dataset configuration\n",
    "datasets:\n",
    "  - path: {TRAIN_FILE}\n",
    "    type: sharegpt\n",
    "    conversation: chatml\n",
    "\n",
    "# Chat template\n",
    "chat_template: chatml\n",
    "\n",
    "# Output\n",
    "output_dir: {OUTPUT_DIR}\n",
    "\n",
    "# Training hyperparameters\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "num_epochs: 3\n",
    "learning_rate: 2e-4\n",
    "lr_scheduler: cosine\n",
    "warmup_ratio: 0.1\n",
    "\n",
    "optimizer: adamw_bnb_8bit\n",
    "weight_decay: 0.01\n",
    "max_grad_norm: 1.0\n",
    "\n",
    "# Training settings\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "gradient_checkpointing_kwargs:\n",
    "  use_reentrant: false\n",
    "\n",
    "# Logging\n",
    "logging_steps: 1\n",
    "save_strategy: epoch\n",
    "save_total_limit: 2\n",
    "\n",
    "# Evaluation (optional)\n",
    "# val_set_size: 0.1\n",
    "# eval_steps: 20\n",
    "\n",
    "# Flash attention (faster if available)\n",
    "flash_attention: true\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "# Save config\n",
    "CONFIG_FILE = \"config.yml\"\n",
    "with open(CONFIG_FILE, 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved to {CONFIG_FILE}\")\n",
    "print(\"\\nüìã Key settings:\")\n",
    "print(f\"   Base model: {BASE_MODEL}\")\n",
    "print(f\"   Method: QLoRA (4-bit quantization)\")\n",
    "print(f\"   LoRA rank: 32\")\n",
    "print(f\"   Epochs: 3\")\n",
    "print(f\"   Learning rate: 2e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding Key Configuration Options\n",
    "\n",
    "`‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`\n",
    "**QLoRA explained:**\n",
    "- **Q** = Quantized (4-bit) base model ‚Üí fits in memory\n",
    "- **LoRA** = Low-Rank Adaptation ‚Üí trains small adapter weights\n",
    "- Result: Fine-tune 7B models on a free Colab T4 GPU!\n",
    "\n",
    "**Key parameters:**\n",
    "- `lora_r`: Rank of adaptation matrices (higher = more capacity, more memory)\n",
    "- `lora_alpha`: Scaling factor (typically `r/2` or `r`)\n",
    "- `sample_packing`: Combines short examples ‚Üí faster training\n",
    "`‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration\n",
    "!python -m axolotl.cli.preprocess {CONFIG_FILE} --debug 2>/dev/null || echo \"Config validation complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (creates tokenized cache)\n",
    "print(\"üì¶ Preprocessing data...\")\n",
    "!python -m axolotl.cli.preprocess {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# This will take 15-60 minutes depending on dataset size and GPU\n",
    "\n",
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "print(\"   This may take 15-60 minutes on Colab's T4 GPU.\\n\")\n",
    "\n",
    "!accelerate launch -m axolotl.cli.train {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training output\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "if output_path.exists():\n",
    "    print(\"‚úÖ Training outputs:\")\n",
    "    for item in sorted(output_path.iterdir()):\n",
    "        size = item.stat().st_size / 1e6 if item.is_file() else \"dir\"\n",
    "        print(f\"   {item.name}: {size if isinstance(size, str) else f'{size:.1f} MB'}\")\n",
    "else:\n",
    "    print(\"‚ùå Output directory not found. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Your Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"üì¶ Loading model...\")\n",
    "\n",
    "# Load base model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Generate a response from the fine-tuned model.\"\"\"\n",
    "    \n",
    "    # Format with ChatML template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model!\n",
    "\n",
    "test_prompts = [\n",
    "    \"Write a docstring for a function that validates email addresses.\",\n",
    "    \"How should I document error handling in my API?\",\n",
    "    \"Create a brief changelog entry for adding dark mode to an app.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing fine-tuned model\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüë§ User: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ü§ñ Assistant:\\n{response}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export & Share Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Merge LoRA weights into base model (larger file, easier to use)\n",
    "\n",
    "MERGED_MODEL_DIR = \"./outputs/tech-writer-merged\"\n",
    "\n",
    "print(\"üîÄ Merging LoRA weights into base model...\")\n",
    "\n",
    "# Merge\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_DIR)\n",
    "\n",
    "print(f\"‚úÖ Merged model saved to {MERGED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Push to HuggingFace Hub\n",
    "\n",
    "HF_USERNAME = input(\"Enter your HuggingFace username: \")\n",
    "MODEL_NAME = \"tech-writer-mistral-qlora\"  # Change this!\n",
    "\n",
    "REPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
    "\n",
    "print(f\"üì§ Pushing to HuggingFace Hub: {REPO_ID}\")\n",
    "\n",
    "# Push LoRA adapter (smaller, requires base model at inference)\n",
    "model.push_to_hub(REPO_ID, use_auth_token=True)\n",
    "tokenizer.push_to_hub(REPO_ID, use_auth_token=True)\n",
    "\n",
    "print(f\"‚úÖ Model uploaded to https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Download to local machine\n",
    "\n",
    "# Zip the output directory\n",
    "!zip -r fine_tuned_model.zip {OUTPUT_DIR}\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('fine_tuned_model.zip')\n",
    "    print(\"‚úÖ Download started!\")\n",
    "except:\n",
    "    print(\"üìÅ Model saved to fine_tuned_model.zip\")\n",
    "    print(\"   Download manually from the file browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use Your Model Later\n",
    "\n",
    "### From HuggingFace Hub:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base + adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = PeftModel.from_pretrained(base_model, \"your-username/tech-writer-mistral-qlora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-username/tech-writer-mistral-qlora\")\n",
    "```\n",
    "\n",
    "### From Local Files:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load merged model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./outputs/tech-writer-merged\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./outputs/tech-writer-merged\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resources\n",
    "\n",
    "- [Axolotl GitHub](https://github.com/OpenAccess-AI-Collective/axolotl)\n",
    "- [Axolotl Examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [PEFT Library](https://huggingface.co/docs/peft)\n",
    "\n",
    "## üí° Tips for Better Results\n",
    "\n",
    "### Data Quality\n",
    "- **100+ examples** recommended for good results\n",
    "- **Consistent format** across all examples\n",
    "- **Cover edge cases** your model will encounter\n",
    "\n",
    "### Training Configuration\n",
    "- **Increase `lora_r`** (64, 128) for more complex tasks\n",
    "- **Lower learning rate** (1e-5) if loss is unstable\n",
    "- **More epochs** (5-10) for smaller datasets\n",
    "\n",
    "### Memory Optimization\n",
    "- Use **gradient checkpointing** (enabled by default)\n",
    "- Reduce **micro_batch_size** if OOM\n",
    "- Use **sample_packing** for short examples"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
