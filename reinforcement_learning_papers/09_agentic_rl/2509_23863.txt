Preprint under review

SPELL: S ELF -P LAY R EINFORCEMENT L EARNING FOR
EVOLVING L ONG -C ONTEXT L ANGUAGE M ODELS
Ziyi Yang1,2∗ Weizhou Shen2 Chenliang Li2 Ruijun Chen1 Fanqi Wan1
Ming Yan2† Xiaojun Quan1† Fei Huang2
1
Sun Yat-sen University 2 Tongyi Lab, Alibaba Group
yangzy39@mail2.sysu.edu.cn, quanxj3@mail.sysu.edu.cn
ym119608@alibaba-inc.com

arXiv:2509.23863v2 [cs.CL] 22 Dec 2025

https://github.com/Tongyi-Zhiwen/Qwen-Doc

A BSTRACT
Progress in long-context reasoning for large language models (LLMs) has lagged
behind other recent advances. This gap arises not only from the intrinsic difficulty
of processing long texts, but also from the scarcity of reliable human annotations
and programmatically verifiable reward signals. In this paper, we propose SPELL,
a multi-role self-play reinforcement learning framework that enables scalable,
label-free optimization for long-context reasoning. SPELL integrates three cyclical
roles—questioner, responder, and verifier—within a single model to enable continual self-improvement. The questioner generates questions from raw documents
paired with reference answers; the responder learns to solve these questions based
on the documents; and the verifier evaluates semantic equivalence between the
responder’s output and the questioner’s reference answer, producing reward signals
to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts
question difficulty to the model’s evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on
large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain
in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its
performance ceiling and showing promise for scaling to even more capable models.

1

I NTRODUCTION

In recent years, reinforcement learning (RL) has emerged as a promising approach for enhancing
the reasoning capabilities of large language models (LLMs) (Guo et al., 2025; Yang et al., 2025;
Jaech et al., 2024; Team et al., 2025). Among these methods, reinforcement learning with verifiable
rewards (RLVR) has shown particular promise in domains where correctness can be programmatically
verified, such as mathematics, logical reasoning, and software engineering (Lambert et al., 2024; Hu
et al., 2025; Liu et al., 2025c; Wei et al., 2025). RLVR methods employ rule-based or programmatic
verifiers to generate reward signals, which then guide policy optimization through algorithms such as
Proximal Policy Optimization (PPO) (Schulman et al., 2017b), Group Relative Policy Optimization
(GRPO) (Shao et al., 2024), and related variants (Shao et al., 2024; Yue et al., 2025; Liu et al., 2025d).
Despite these advances, most RLVR research has been restricted to short-context settings (e.g., <1024
tokens), where models primarily rely on their parametric knowledge for reasoning (Wan et al., 2025).
In contrast, reasoning over long documents like long-context question answering requires not only
locating relevant evidence scattered across extended contexts but also executing multi-step reasoning.
Extending RLVR to long-context reasoning presents significant challenges, which stem from the
inherent difficulty of processing long texts, as well as two critical bottlenecks: the prohibitive cost
and unreliability of human annotations, and the absence of programmatically verifiable rewards.
∗
†

Work done during internship at Tongyi Lab, Alibaba Group.
Corresponding authors.

1

Preprint under review

Questioner

Question

Responder

Memory

Forward
Backward

ar d
Rew

ar d

ion

Re

est

Rew

sp o

Qu

nse

Single Model

Verifier
Reward

Figure 1: (Left) An overview of the SPELL framework, where a single LLM self-evolves by dynamically
adopting the roles of questioner, responder, and verifier. (Right) SPELL consistently boosts performance across
various models (top) and exhibits superior test-time scaling over traditional RLVR (bottom).

Empirical evidence highlights the severity of these issues. On benchmarks such as LongBench-V2,
human accuracy for extra-long multiple-choice reasoning tasks drops to 25.1% — effectively approaching random chance (Bai et al., 2025). This not only limits the performance achievable under
human supervision but also imposes a scalability ceiling, particularly as LLMs approach superhuman
reasoning capabilities (Zhao et al., 2025). Specifically, as context length grows, producing reliable annotations becomes increasingly costly and unstable, and supervision diversity diminishes. Moreover,
the lack of verifiable reward mechanisms in long-context settings further constrains the applicability
of RLVR, posing a fundamental challenge to advancing reasoning capabilities at scale.
To address these limitations, we turn to self-play RL, where a single model learns to self-evolve by
generating and solving its own tasks without human labels (Zhou et al., 2025; Chen et al., 2025b;
Huang et al., 2025). However, applying self-play to long-context reasoning poses a unique challenge:
answers may be semantically correct yet differ substantially in expression, rendering string matching
or naive majority voting unreliable reward signals. Thus, the model should not only generate questions
and answers, but also verify its own solutions reliably. This observation motivates our framework, in
which one LLM assumes three complementary roles: questioning, responding, and verifying.
In this paper, we introduce SPELL (Self-Play Reinforcement Learning for Evolving Long-Context
Language Models), a self-play RL framework for long-context reasoning. In this setup, a unified
policy alternates among three roles: the questioner, which formulates questions with reference
answers from raw documents; the responder, which attempts to solve them; and the verifier, which
compares the responder’s output with the reference answer to produce reward signals for joint
optimization. To steer this process, SPELL incorporates three key design elements. First, a verifier
trained for self-consistency on verifiable tasks produces stable rewards, even for outputs that cannot
be verified by strict rules, thereby overcoming the brittleness of string matching. Second, an
automated curriculum uses a history memory of question–answer pairs and documents to progressively
increase task difficulty. A Gaussian-shaped reward further calibrates difficulty around the responder’s
competence frontier, ensuring questions are neither too easy nor impossibly difficult. Third, a rolespecific dynamic sampling strategy balances contributions across roles to stabilize training of the
shared policy. Together, these components form a self-sufficient, closed-loop system that enables
LLMs to autonomously evolve long-context reasoning without human-labeled data
We evaluate SPELL across 12 open-source LLMs ranging from 4B to 32B parameters, including both
dense and Mixture-of-Experts (MoE) architectures. On six long-context QA benchmarks, SPELL delivers consistent performance gains. Remarkably, training a base model with SPELL enables it to
2

Preprint under review

surpass its instruction-tuned counterpart that relies on extensive human-annotated data, highlighting
the data efficiency of our label-free self-play approach. Against a strong RLVR baseline trained on
a static dataset synthesized by DeepSeek-R1-0528 (Guo et al., 2025), SPELL achieves larger and
more reliable gains. For capable models such as Qwen3-30B-A3B-Thinking, SPELL’s dynamic
curriculum continually elevates performance and enables it to outperform the leading gemini-2.5pro (Comanici et al., 2025) in pass@4. These findings firmly establish our self-play approach as a
scalable and effective path toward advanced long-context reasoning without human supervision.

2

R ELATED W ORK

2.1

L ONG -C ONTEXT A LIGNMENT

Developing long-context language models (LCLMs) has become a central research area, as many
real-world applications require reasoning over extended inputs (Liu et al., 2025b). A dominant
paradigm in this field is to enhance models through various post-training alignment techniques on
well-curated, synthesized datasets. One prominent approach is supervised fine-tuning (SFT). For
instance, LongAlign (Bai et al., 2024a) utilizes a self-instruct pipeline to construct a large-scale,
long-instruction dataset for SFT, while MIMG (Chen et al., 2025c) employs a multi-agent system
to generate more complex, multi-hop reasoning data. Another line of work focuses on preference
optimization. LongPO (Chen et al., 2025a) and SoLoPO (Sun et al., 2025) generate preference
pairs by contrasting outputs from compressed versus full contexts and leverage direct preference
optimization (DPO) (Rafailov et al., 2023) to transfer short-context capabilities to longer inputs. More
recently, QwenLong-L1 (Wan et al., 2025) introduces the concept of long-context reasoning RL and
employs a two-stage progressive context scaling training to develop long-context reasoning models.
While proven effective, all these approaches exhibit some form of reliance on external supervision.
2.2

S ELF -P LAY L ANGUAGE M ODELS

To mitigate the reliance on external supervision, such as human annotation or labeled datasets,
researchers are developing self-play language models (Gao et al., 2025). These models achieve
autonomous improvement by generating their own training data, reward signals, or both. These
approaches can be categorized into two paradigms: multi-model optimization, which co-evolves
several distinct models, and single-model optimization, where one model assumes multiple roles.
Multi-Model Optimization This paradigm orchestrates the co-evolution of multiple specialized
models. In Mutual-Taught (Shi et al., 2025), a policy model and a reward model are reciprocally
and iteratively refined: the policy model generates data to enhance the reward model, which in turn
provides more accurate feedback to improve the policy model. Similarly, in R-Zero (Huang et al.,
2025), a challenger and a solver LLM are independently optimized and co-evolve: the challenger
generates new, challenging math problems for which it is rewarded based on the solver’s uncertainty,
and the solver is fine-tuned on these questions, rewarded for correctly solving them. While effective,
this paradigm substantially increases systemic complexity, and performance gains often plateau after
a finite number of iterations.
Single-Model Optimization In contrast to the multi-model approach, single-model optimization reduces systemic complexity by leveraging a single model to assume multiple roles for selfimprovement. For instance, Absolute Zero Reasoner (AZR) (Zhao et al., 2025) employs one model as
both a proposer that generates complementary coding tasks (induction, abduction, and deduction) and
a solver that addresses them, with code execution feedback serving as the reward signal. Similarly, the
Self-Challenging Agent (SCA) (Zhou et al., 2025) employs a model to formulate novel “Code-as-Task”
questions and subsequently solve them, using self-generated code functions to provide the verification
signal. A significant limitation of such approaches, however, is their dependence on external code executors, which confines their applicability to domains with programmatically verifiable outcomes. To
overcome this limitation and enhance autonomy, other approaches further incorporate self-rewarding
mechanisms that leverage self-consistency (Zuo et al., 2025), internal confidence scores (Li et al.,
2025), or self-generated evaluations (Yuan et al., 2024). For example, Self-Questioning Language
Models (SQLM) (Chen et al., 2025b) utilize a model to propose and then answer questions, adapting
its reward mechanism between self-consistency for arithmetic and proposer-generated unit tests
for coding. Similarly, the Self-Rewarding Self-Improving framework (Simonds et al., 2025) also
generates its own questions and solutions but uses a self-judging mechanism for reward computation.
3

Preprint under review

Our work extends the single-policy self-play paradigm to long-context understanding and reasoning.
Unlike existing methods that focus on short-context tasks like coding or math, SPELL is designed
for reasoning over long documents. In our framework, a single LLM learns by playing three roles: a
questioner, a responder, and a verifier. These roles interact to create a self-sufficient learning loop
for long-context comprehension, thereby addressing a key gap in current self-play learning research.

3

P RELIMINARIES

Long-Context Reinforcement Learning We formulate the long-context generation task as a
reinforcement learning (RL) problem. Given a set of n documents {ci }ni=1 and a question q, the
goal of long-context RL is to optimize a policy model πθ to generate a response y that maximizes
a reward function rϕ (c, q, y). The standard objective is to maximize the KL-regularized expected
reward (Schulman et al., 2017a; Wan et al., 2025):
max Ec,q∼D,y∼πθ (·|c,q) [rϕ (c, q, y)] − βDKL [πθ (y | c, q) || πref (y | c, q)] ,
πθ

(1)

where c = Concat(c1 , c2 , . . . , cn ), D is the training dataset, πref denotes a reference policy, and β
controls the strength of the KL regularization to prevent large deviations from the reference policy.
Group Relative Policy Optimization (GRPO) For long-context inputs, the quadratic complexity
of the attention mechanism renders PPO (Schulman et al., 2017b), which relies on generalized
advantage estimation (GAE) (Schulman et al., 2015) via a value network, computationally prohibitive.
Therefore, we employ GRPO (Shao et al., 2024) to optimize the objective in Eq. (1). For each input
(c, q), GRPO first samples a group of G candidate responses {yi }G
i=1 from the old policy πθold . It then
estimates the advantage through group-wise reward z-score normalization, thereby obviating the need
for a separate value network. Formally, the objective is:
"
|yi |
G

1 X 1 X
JGRPO (θ) = Ec,q∼D,{yi }G
min
ρi,t (θ)Ai ,
∼π
(·|c,q)
θ
i=1
old
G i=1 |yi | t=1
(2)
!#

 
clip ρi,t (θ), 1 − ε, 1 + ε Ai − βDKL (πθ ||πref ) ,
π (y

|c,q,y

)

where ρi,t (θ) = πθθ (yi,ti,t |c,q,yi,<t
is the importance sampling ratio for token t in sequence i. The
i,<t )
old
group-relative advantage Ai is shared across tokens of the i-th sequence and computed by normalizing
the sequence-level rewards {ri }G
i=1 :
Ai =

4

ri − mean({rk }G
k=1 )
.
G
std({rk }k=1 )

(3)

T HE SPELL FRAMEWORK

In this section, we detail the core design of SPELL, a self-play reinforcement learning framework that
enables LLMs to improve their long-context reasoning capabilities without external supervision. The
key principle of SPELL is that a single policy model πθ dynamically assumes three complementary
roles: a questioner (πθque ), a responder (πθres ), and a verifier (πθver ). Through their interaction, the
model autonomously generates and solves questions while producing reliable reward signals. This
closed-loop interaction creates an evolving curriculum in which the model progressively adapts to
longer contexts and more complex reasoning (Section 4.1). Role-specific reward designs (Section 4.2)
and a unified optimization procedure (Section 4.3) jointly drive this co-evolution.
4.1

T HE S ELF -P LAY E VOLUTIONARY L OOP

As illustrated in Figure 2 and Algorithm 1, SPELL proceeds iteratively: given a cluster of n
documents C = {ci }ni=1 and a task type1 τ , the policy πθ first generates new questions,2 then
attempts to solve them, and finally verifies the solutions before performing a unified policy update.
1

Details of dataset construction and task definition are provided in Appendix C.1.
To direct the policy in enacting three distinct roles, we adopt zero-shot prompting using tailored templates
for each role and task type. Details of these templates are provided in Appendix E.
2

4

Preprint under review

Role-Specific Rollout
{(�, �)}

Model

�res
θ

�new
Responding
Full Context

History Memory

Retrieve
que Questioning

�θ

Grounding Filter

�1

�

�

�ver
θ

...

��

Unified Policy Update
Role-Specific

�θ

GRPO
Training

�que

�ver

Dynamic Sampling

... �res
�res
�
1

Advantage
Estimation

�1

ℋ
Verifying

�ver
1,1

�1,1 ... �1,�

... . . . ...
��,1 ... ��,�

... �ver
1,�
... �ver
�,�

... �res
�res
�
1
�que

...

�1

��

��

Data
�1,1 ... �1,�

... . . . ...

�ver
�,1

��

�1

... . . . ...

Role-Specific
Reward

��,1 ... ��,�
�1 ... ��
�

�

Figure 2: Overview of our proposed SPELL for self-evolution of long-context reasoning. The process operates
in a continuous loop that alternates between two stages: (1) Role-Specific Rollout, where a single policy model
enacts three distinct roles—a questioner (πθque ), a responder (πθres ), and a verifier (πθver )—to generate training
data. (2) Unified Policy Update, where the unified policy is refined using the collected data, and the enhanced
model serves as the starting point for the next rollout cycle.

Questioning The questioner πθque generates new question–answer pairs in an iterative curriculum. In
the very first iteration, it is conditioned only on a randomly sampled subset of m documents (m < n)
and produces a pair (q, a). After each solvable pair is created, we append it to a history memory H that
stores the L most recent solvable question–answer pairs and their associated source documents: HC =
{(Cl , ql , al )}L
on both a newly sampled
l=1 . In subsequent iterations, the questioner is conditioned
SL
que
subset Cnew and the stored memory. The resulting context is X = ( l=1 Cl )∪Cnew ∪{(ql , al )}L
l=1 .
As the memory fills, the context for the questioner expands to include both previously seen and newly
sampled documents, which allows the questioner to generate questions that integrate information
across more documents. The history memory also raises difficulty by including past {(ql , al )}: these
exemplars discourage redundancy and, via prompting, push πθque to generate harder questions than
those already solved. Consequently, the questioner’s difficulty increases for two complementary
reasons: (1) the context X que expands over iterations as more documents are brought into scope, and
(2) explicit conditioning on historical {(ql , al )} encourages the model to escalate question complexity.
Responding The responder πθres attempts to solve the generated question based on documents. To
mitigate the generation of non-grounded or hallucinated questions, we employ a grounding filter
process to discard questions that can be trivially answered without documents. For valid questions, the
responder is presented with the complete set of n documents, where the remaining documents unseen
by the questioner serve as distractors to increase grounding and reasoning difficulty. This design
enforces reliance on the provided document context rather than parametric memory. To encourage
exploration of diverse reasoning trajectories, the responder generates G independent rollouts {yi }G
i=1 .
Verifying The verifier πθver evaluates the semantic equivalence between the responder’s output yi
and the questioner’s reference answer a. For each yi , it produces G independent binary judgments
{vi,j }G
j=1 , vi,j ∈ {0, 1}, which are then aggregated through majority voting:


G
X
G
viver = I 
vi,j >  ,
(4)
2
j=1
where I(·) is the indicator function. This ensemble-based verification reduces variance and produces
a stable, semantically aware reward signal, which is essential for sustaining a self-play system. .
This step forces the questioner to create questions that are deeply tied to the provided text, thereby
targeting the grounding ability of the responder.
4.2

ROLE -S PECIFIC R EWARD D ESIGN

The three roles co-evolve under specialized rewards that align their objectives while remaining
compatible within a single shared policy. In what follows, we detail these rewards.
5

Preprint under review

Verifier The verifier is trained to improve its judgment reliability through self-consistency (Wang
et al., 2022; Zuo et al., 2025). For a candidate output yi , the verifier produces G rollouts with
judgments vi,j . Each rollout is then assigned a reward:
ver
ri,j
= I(vi,j = viver ),
(5)
ver
where vi is the majority vote over G rollouts.
Responder The responder’s reward for the i-th solution is the maximum of a deterministic, rulebased check and the verifier’s consensus score, denoted as:
rires = max (Rrule (yi , a), viver ) .
(6)
The rule-based function, Rrule , provides a binary reward based on cover exact match (CEM) criteria (Wan et al., 2025; Song et al., 2025)—it returns 1 if the ground-truth answer a appears in the
generated response yi and 0 otherwise. The maximum reward plays a crucial role: when yi is a correct
paraphrase that CEM fails to capture, a majority vote of viver = 1 prevents the policy from being
misled by false-negative noise, which stabilizes learning and encourages continual improvement.
Questioner The questioner is incentivized to generate questions of intermediate difficulty, as
learning is most efficient at the frontier of the LLM’s capabilities (Bae et al., 2025; Huang et al., 2025).
For binary-reward tasks, this frontier corresponds to a success probability of 0.5, which maximizes
reward variance and provides the richest learning signal. We therefore define the questioner’s reward
as a Gaussian function centered at this optimal point. Given the responder’s average success rate,
PG res
1
r̄res = G
i=1 ri , the reward is:



res
2

exp − (r̄ 2σ−µ)
if 0 < r̄res < 1
2



if r̄res = 0 or r̄res = 1
(7)
rque = 0

−0.5
if the question is not grounded in documents



−1
if the question-answer pair has formatting errors
We set the mean µ = 0.5 to target the point of maximum learning efficiency and the standard deviation
σ = 0.5/3 to concentrate the reward around this level. Additionally, the questioner is penalized for
producing ill-formatted (e.g., non-parsable) question–answer pairs or questions that can be solved
without context, thereby enforcing both correct formatting and strong grounding in the provided text.
4.3

U NIFIED P OLICY O PTIMIZATION

A central feature of SPELL is that samples generated under different roles supervise a single policy
πθ . The optimization must control both sample efficiency and gradient balance across roles.
Role-Specific Dynamic Sampling The raw samples collected for each document instance are highly
imbalanced: one questioner sample, G responder samples, and G2 verifier judgments. To prevent
the verifier’s samples from dominating updates and to prioritize improvements in the responder’s
document-grounded reasoning, we introduce a role-specific sampling strategy that leverages the
statistical structure of each role’s signals. For the responder, we retain all groups with non-zero reward
variance (std({rires }G
i=1 > 0). The associated questions are labeled as positives for the questioner, and
an equal number of negatives are drawn from questions with non-positive reward, as defined in Eq. (7).
For the verifier, we preserve instances where the majority vote agrees with the rule-based check and
subsample groups with conflicting verifications to match the number of questions. This role-specific
sampling strategy reduces the training set to roughly 1/G of all samples, accelerates optimization,
and prevents the responder’s gradients from being overwhelmed by verifier samples. Importantly,
although most verifier samples are omitted, their collection cost is low, see Appendix D.1.
Advantage Estimation For the responder and verifier, which generate G outputs per prompt, we
use group-level advantage estimation as defined in Eq. (3):
rirole − mean({rkrole }G
k=1 )
Arole
=
, role ∈ {res, ver}.
(8)
i
role G
std({rk }k=1
The questioner generates only a single output per instance and thus lacks a group-level baseline.
Therefore, we adapt the normalization method from REINFORCE++-baseline (Hu, 2025) and
normalize its reward against other questioner rewards within the training batch B que :
rque − mean(rque | rque ∈ B que )
Aque =
.
(9)
std(rque | rque ∈ B que )
6

Preprint under review

Unified Policy Update After collecting and sampling a batch of samples, the policy parameters θ
are updated by jointly optimizing the GRPO objective across all three roles:
que
res
ver
JGRPO (θ) = JGRPO
(θ) + JGRPO
(θ) + JGRPO
(θ)

(10)

The updated πθ is reused to execute all roles in the next iteration. This closes the self-evolutionary
cycle and keeps one unified policy for questioning, responding, and verifying.

5

E XPERIMENTS

5.1

E XPERIMENTAL S ETUP

Training Details Our SPELL RL framework is implemented using VeRL (Sheng et al., 2025).
During generation, we employ a sampling temperature of 0.7 and a top-p value of 0.95. The
maximum input length is 16K tokens, while the maximum output length is set to 4K for nonreasoning models and extended to 20K tokens for reasoning models. To balance rollout diversity and
computational efficiency, we utilize a group size of G = 8. The maximum number of recent solvable
question–answer pairs cached in history memory is set to L = 3, and the number of candidate
documents drawn when proposing a new question is set to m = 5. We conduct a purely on-policy
RL training with a batch size of 128 and a constant learning rate of 2 × 10−6 . At the beginning
of each rollout, we randomly sample one of three predefined task formats—document general QA,
financial math QA, or multiple-choice—along with a relevant document list from the corpus. Prompt
templates for each task τ and each role are provided in Appendix E. For the RLVR baseline, we
synthesize a dataset using DeepSeek-R1-0528 (Guo et al., 2025) over the same document corpus and
maintain identical hyperparameters to ensure a fair comparison. For comprehensive details on data
construction, RL algorithm, and baselines, please refer to Appendix C.1, C.3, and C.4.
Evaluation Benchmarks We evaluate our models on six long-context benchmarks, spanning
multiple-choice QA on LongBench-V2 (Bai et al., 2025) and multi-hop QA across Frames (Krishna
et al., 2025), HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi
et al., 2022)3 , and the DocMath (Zhao et al., 2024) for financial report reasoning task. We evaluate all
models with maximum input lengths of 16K and 100K tokens, and report the average accuracy over
eight runs. Further details on the benchmarks and evaluation protocol are available in Appendix C.2.
5.2

M AIN R ESULTS

Table 1 summarizes the results of SPELL across 12 open-source LLMs on six long-context QA
benchmarks under maximum input lengths of 16K and 100K tokens. These results offer valuable
insights into SPELL’s effectiveness and generalization, as elaborated below.
SPELL consistently enhances performance across diverse models. Our self-play framework
exhibits strong universality, and it delivers substantial improvements across different architectures,
sizes, and families. This versatility is evident across the following dimensions. (1) Model types
and sizes: SPELL cultivates complex reasoning skills from scratch. For unaligned base models, the
average improvement at 16K is large and robust, with Qwen2.5-7B, Qwen2.5-14B, and Qwen2.5-32B
improving by 13.9, 14.4, and 9.1 points, respectively. Remarkably, these trained models consistently
outperform their instruction-tuned counterparts of the same size, which are trained with extensive
human-annotated data. This result highlights that SPELL is data-efficient and practically valuable
in scenarios where labeled data is scarce. SPELL also benefits instruction-tuned models, e.g.,
Qwen2.5-7B-Instruct improves by 9.0 points. For highly specialized reasoning models such as
R1-Distill-Qwen-14B, the performance still increases by 3.4 points. (2) Architecture: Beyond dense
models, the framework is also applicable to Mixture-of-Experts (MoE) models, where it improves
Qwen3-30B-A3B-Instruct and Qwen3-30B-A3B-Thinking by 4.4 and 2.0 points, respectively. (3)
Model families: Improvements extend across families. For example, Llama-3.1-8B-Instruct and
R1-Distill-Llama-8B increase by 4.4 and 3.4 points, respectively. Collectively, these results establish
SPELL as a broadly effective paradigm for advancing LLMs in long-context tasks.
SPELL is superior to traditional RL with static data. We compare SPELL against the RLVR
baseline trained on a fixed dataset synthesized by DeepSeek-R1-0528. Although such static data
3

We use the subsets of HotpotQA, 2WikiMultihopQA, and MuSiQue from LongBench (Bai et al., 2024b).

7

Preprint under review

Table 1: Overall results of our proposed SPELL method with maximum input lengths of 16K and 100K on longcontext benchmarks. “LB-MQA” represents the average performance across 2WikiMultihopQA, HotpotQA, and
MuSiQue. “LB-V2” refers to LongBench-v2. For the average score (Avg.), + indicates the relative improvement
over the base model within each group. The best score in each model group is highlighted in bold.
16K

Models
DocMath

Frames

100K

LB-MQA

LB-V2

Avg.

DocMath

Frames

LB-MQA

LB-V2

Avg.

Base Models
Qwen2.5-7B
+ RLVR
+ SPELL

10.9
41.8
40.0

27.9
41.0
39.2

36.7
50.0
50.9

31.2
30.2
32.3

26.7
40.8+14.1
40.6+13.9

16.1
42.7
39.9

24.2
40.3
40.1

31.2
49.2
50.8

22.7
26.0
28.2

23.6
39.6+16.0
39.8+16.2

Qwen2.5-14B
+ RLVR
+ SPELL

38.0
52.2
57.6

37.2
51.0
52.6

41.9
63.3
63.0

32.1
32.9
33.5

37.3
49.9+12.6
51.7+14.4

36.2
53.2
56.8

37.5
52.1
53.0

43.3
64.2
63.2

27.5
30.5
31.2

36.1
50.0+13.9
51.1+15.0

Qwen2.5-32B
+ RLVR
+ SPELL

46.8
58.3
61.8

42.6
50.0
50.2

49.0
59.5
62.1

33.7
32.8
34.2

43.0
50.2+7.2
52.1+9.1

40.7
57.5
60.6

42.2
49.9
52.2

50.1
60.1
62.3

28.7
32.7
34.3

40.4
50.1+9.7
52.4+12.0

Qwen2.5-7B-Instruct
+ RLVR
+ SPELL

38.4
45.0
45.8

40.3
48.7
46.7

45.1
59.6
63.1

29.0
30.1
33.2

38.2
45.9+7.7
47.2+9.0

39.4
44.1
44.5

41.4
48.6
48.2

44.5
57.4
60.7

28.4
28.2
32.4

38.4
44.6+6.2
46.5+8.1

Qwen2.5-14B-Instruct
+ RLVR
+ SPELL

56.3
56.1
59.6

51.6
59.6
62.1

63.0
71.0
72.8

32.2
36.4
36.8

50.8
55.8+5.0
57.8+7.0

56.7
56.7
60.1

52.4
59.9
63.9

64.2
73.4
74.8

36.6
38.5
40.1

52.5
57.1+4.6
59.7+7.2

Qwen2.5-32B-Instruct
+ RLVR
+ SPELL

60.0
59.9
62.3

49.9
60.5
61.2

61.4
70.4
74.4

36.0
36.3
40.1

51.8
56.8+5.0
59.5+7.7

63.0
59.7
63.3

49.4
62.3
62.0

61.5
69.6
74.1

36.2
36.9
40.8

52.5
57.1+4.6
60.1+7.6

Qwen3-30B-A3B-Instruct
+ RLVR
+ SPELL

62.3
62.5
63.0

55.3
59.9
63.1

70.5
71.8
75.1

36.9
39.8
41.5

56.3
58.5+2.2
60.7+4.4

63.0
64.0
64.9

57.8
62.0
63.7

70.3
72.4
74.8

44.1
47.4
48.7

58.8
61.5+2.7
63.0+4.2

Llama3.1-8B-Instruct
+ RLVR
+ SPELL

33.2
37.9
39.2

45.6
45.0
48.9

52.5
58.8
61.6

29.1
27.5
28.4

40.1
42.3+2.2
44.5+4.4

34.9
36.9
39.7

47.3
47.6
50.8

53.5
57.2
60.9

27.1
26.1
26.2

40.7
42.0+1.3
44.4+3.7

R1-Distill-Llama-8B
+ RLVR
+ SPELL

42.0
43.4
48.9

50.3
51.4
53.4

66.8
67.8
68.4

27.9
30.0
30.2

46.8
48.2+1.4
50.2+3.4

41.5
45.4
49.2

52.6
54.0
54.3

69.3
68.0
70.0

26.4
28.3
29.3

47.5
48.9+1.4
50.7+3.2

R1-Distill-Qwen-14B
+ RLVR
+ SPELL

57.7
59.6
61.6

59.2
61.7
62.3

72.4
74.6
76.2

36.2
37.2
39.0

56.4
58.3+1.9
59.8+3.4

59.5
61.0
61.1

60.6
63.8
62.8

73.3
76.0
75.7

33.3
35.9
37.9

56.7
59.2+2.5
59.4+2.7

Qwen3-4B-Thinking
+ RLVR
+ SPELL

58.6
60.5
61.9

56.7
56.6
56.6

69.9
71.1
71.6

32.9
33.8
36.8

54.5
55.5+1.0
56.7+2.2

61.4
63.3
64.8

59.2
58.6
60.6

70.9
71.1
72.4

40.7
43.4
43.0

58.1
59.1+1.0
60.2+2.1

Qwen3-30B-A3B-Thinking
+ RLVR
+ SPELL

62.9
62.7
64.1

64.5
64.7
66.5

75.7
77.0
78.0

39.7
38.5
42.3

60.7
60.7+0.0
62.7+2.0

63.8
63.9
66.7

65.8
67.1
68.1

77.9
77.2
78.4

46.7
49.6
50.5

63.6
64.5+0.9
65.9+2.3

Instruct Models

Reasoning Models

offers high-quality supervision for RL training, it cannot adapt to the policy’s evolving capabilities.
In contrast, SPELL constructs a self-play curriculum that tracks the model’s current ability: the
questioner focuses on instances near the responder’s competence boundary, maintaining alignment
between the training signal and the policy throughout optimization. The advantage becomes increasingly evident as the policy model’s capabilities grow. For Qwen2.5-7B, RLVR achieves performance
comparable to SPELL, indicating that a static corpus appears sufficient for weaker policies. However,
for Qwen3-30B-A3B-Thinking, SPELL improves average scores by 2.0, whereas RLVR yields
no gain. On the more challenging benchmarks for the same model, RLVR decreases accuracy on
DocMath (-0.2) and LongBench-V2 (-1.2), whereas SPELL delivers consistent gains of 1.2 and 2.6
points, respectively. These results validate that when models approach or surpass the quality of static
training data, a self-play curriculum proves more effective for sustaining performance gains.
SPELL generalizes to longer contexts. All models are trained with a 16K input limit and evaluated
at 100K without additional tuning. The results remain consistent under this out-of-distribution
input length, demonstrating that the benefits of SPELL extend beyond the training window. For
Qwen2.5-14B, the average improvement is 14.4 at 16K and increases to 15.0 at 100K. This consistency
suggests that the framework strengthens document-grounded reasoning in a way that remains effective
as input lengths grow substantially, rather than producing gains limited to a specific context length.
SPELL boosts exploration and raises the performance ceiling. We assess test-time exploration
with the pass@k metric at a 100K input limit. As shown in Figure 3, Qwen3-30B-A3B-Thinking
trained with SPELL exhibits a markedly steeper improvement curve as k increases compared to both
8

Preprint under review

Qwen3-30B-A3B-Thinking
DocMath

Overall

Pass Rate (%)

75
72

70.4

69
66
63

72

75

69

72

66

69

62.4

63
1

2

4

+RLVR
Frames

8

1

2

+SPELL
84

8

LongBench-V2
65

82

72.1

55
50

78
1

2

4

8

Number of Samples K (Log Scale)

65.7

60

81.3

80

66
4

gemini-2.5-pro
LongBench-MQA

1

2

4

8

45

1

2

4

8

Figure 3: Test-time scaling performance (pass@k) across all benchmarks. The Qwen3-30B-A3B-Thinking
model trained with SPELL shows a significantly steeper improvement as the number of samples (K) increases
compared to the base model and the RLVR baseline. Notably, its pass@4 performance surpasses gemini-2.5-pro.
Overall

0.8

1 - Pass@1

0.6
0.4
0.2

10

20

30

40

50

Training Steps

60

70

Financial Math QA

Multiple-choice

w/o Update

0.6
0.4
0.2

w/o History Memory

0.8

1 - Pass@1

0.8

1 - Pass@1

General QA

SPELL

0.6
0.4
0.2

10

20

30

40

50

Training Steps

60

70

10

20

30

40

50

Training Steps

60

70

Figure 4: Analysis of question difficulty (1 - pass@1) on three tasks over training steps. (Left): The full
SPELL framework shows a clear upward trend in difficulty. (Middle): Without questioner updates, difficulty
stagnates. (Right): Without the history memory, difficulty becomes erratic and unstable.

the base model and the RLVR baseline. Its pass@8 score reaches 74.5, significantly outperforming the
RLVR baseline (68.1) and the original base model (66.9). This enhanced exploratory ability further
allows the SPELL-trained model to surpass the performance of the leading gemini-2.5-pro (Comanici
et al., 2025) at a pass@4 rate. These results indicate that SPELL effectively broadens the model’s
test-time search space and raises its attainable performance ceiling, highlighting a promising path
toward elevating the capabilities of even more powerful foundation models.
5.3

A BLATION S TUDIES

To validate the key design choices within the SPELL framework, we conduct ablation studies on
Qwen2.5-7B-Instruct. We individually remove each core component of the questioner and verifier
roles to quantify their individual contributions to the overall performance.
Questioner As shown in Table 2, the Table 2: Ablation study of SPELL on Qwen2.5-7B-Instruct with
removal of the format penalty and the a 16K maximum input length. - and + indicate relative decreases
grounding filter degrades the average and increases, respectively, compared to the full SPELL model.
score by 1.1 and 1.0 points, respectively.
Method
DocMath Frames LB-MQA LB-V2 Average
The format penalty keeps the question
SPELL
45.8
46.7
63.1
33.2
47.2
well-formed, and the grounding filter
Questioner
prevents the generation of hallucinated
w/o Format Penalty
46.0
48.2
59.3
31.0
46.1
questions. The largest drops come from
w/o Grounding Filter
47.0
46.4
60.1
31.3
46.2
w/o Update
45.5
43.8
50.9
30.3
42.6
disabling the update mechanism and the
w/o History Memory
45.6
46.6
54.2
30.8
44.3
history memory: freezing the questioner
Verifier
lowers the average score by 4.6, and re- w/o Verifier
39.4
46.6
60.4
29.4
44.0
45.1
48.2
61.4
32.6
46.8
moving history memory lowers it by 2.9. w/o Update
w/o Majority Voting
45.5
48.1
61.9
30.7
46.6
The declines appear across DocMath, w/o Update Consistency
46.6
47.1
57.7
31.3
45.7
Frames, LB-MQA, and LB-V2, which
indicates that these components have a broad impact rather than task-specific effects.
+0.2

+1.5

-3.8

-2.2

-1.1

+1.2

-0.3

-3.0

-1.9

-1.0

-0.3

-2.9

-12.2

-2.9

-4.6

-0.2

-0.1

-8.9

-2.4

-2.9

-6.4

-0.1

-2.7

-3.8

-3.2

-0.7

+1.5

-1.7

-0.6

-0.4

-0.3

+1.4

-1.2

-2.5

-0.6

+0.8

+0.4

-5.4

-1.9

-1.5

We further examine how these components affect generated question difficulty over Qwen2.5-7BInstruct training steps, as measured by 1-pass@1 with an external responder (Qwen3-30B-A3BInstruct) and an external verifier (gpt-oss-120b). The full SPELL model (Figure 4, left) shows a
clear upward trend in overall question difficulty, which ensures the questioner proposes questions
that are challenging enough for the responder’s evolving capabilities. In contrast, freezing the
9

Preprint under review

0.5

0.19

|

0.5

rres
(a) Reward Mapping

Entropy

re s

2|r

1 rres

Response Length

0.22

1

rque

0

AZR

0.25

(rres 0.5)2
2 2

0.16
0.12

0

10

20

30

40

50

Training Steps

60

70

R-Zero

600

48

500

45

Pass Rate (%)

SPELL
1

400
300
200

0

(b) Entropy

10

20

30

40

50

Training Steps

60

(c) Response Length

70

43
40
38

0

10 20 30 40 50 60 70

Training Steps

(d) Performance

Figure 5: Comparison of different reward mapping strategies. (a) Visualization of the reward functions
for SPELL, AZR, and R-Zero. (b) SPELL exhibits more stable entropy dynamics during training. (c)
SPELL maintains a more moderate and controlled growth in response length. (d) These factors contribute to a
consistent performance improvement, ultimately leading our method to achieve the highest final pass rate.

questioner causes difficulty to stagnate (Figure 4, middle), while removing the history memory makes
it erratic (Figure 4, right). The evidence supports the conclusion that continual updates and access to
recent history are necessary to form a stable and progressively more challenging curriculum for the
responder, which is essential for sustained improvement in a self-play system. This dynamic prevents
one role from exploiting the static weaknesses of another, as observed in Liu et al. (2025a).
Verifier Removing the verifier and relying solely on rule-based rewards decreases average score by
3.2 points, with a 6.4-point drop on DocMath. The CEM-based reward function is brittle and can
penalize semantically correct but lexically different answers; the verifier provides a complementary
signal in such cases. Interestingly, disabling verifier updates or switching to single-pass decisions
leads to moderate declines, which indicates that Qwen2.5-7B-Instruct is already competent at the
simpler verification task. However, removing the consistency update mechanism still causes a 1.5point performance drop. This result shows that the verifier’s updates are susceptible to noise from
its own erroneous majority votes, which degrades its reliability. On rule-verifiable tasks, the verifier
learns to filter this noise by aligning its majority vote with the ground-truth rule-based outcome.
This process provides the verifier with reliable learning signals, which in turn enhance its ability to
generate stable rewards for rule-unverifiable outputs. This illustrates how verifiable rewards can guide
the calibration of non-verifiable rewards, a finding that aligns with the self-judging methodology in
Kimi-K2 (Team et al., 2025).
5.4

A NALYSIS OF Q UESTIONER R EWARD M APPING

We compare our Gaussian-mapped reward function for questioner in Eq. (7) with the reward mapping
used in AZR (Zhao et al., 2025) and R-Zero (Huang et al., 2025). Figure 5(a) visualizes these distinct
mapping functions. While the AZR reward function also penalizes high-accuracy questions, it is
susceptible to noise from spurious correctness, which can destabilize the training process. In contrast,
our Gaussian function, which peaks when the average responder accuracy r̄res is 0.5, selectively
encourages questions at the frontier of the responder’s competence. Additionally, this mechanism
mitigates the impact of data noise. Questions with wrong reference answers typically result in success
rates near zero or one, corresponding to scenarios of random guessing on unsolvable questions or
consistent matching with the incorrect reference, respectively. Both extremes naturally fall into the
low-reward tails of the Gaussian function, effectively suppressing incorrect questions during policy
optimization. While R-Zero also centers its peak reward at 0.5, our Gaussian mapping provides a
more targeted reward by offering stronger incentives for questions of moderate difficulty and imposing
a steeper penalty on those that are either too easy or too hard. This creates a focused and smooth
reward distribution that guides the questioner away from generating both trivial and overly difficult
questions. The training dynamics corroborate these design differences. As shown in Figures 5(b)
and (c), our method maintains a more stable training entropy and exhibits more controlled growth
in response length under the Gaussian mapping than under AZR or R-Zero. These advantages in
training stability lead to superior overall performance. As Figure 5(d) demonstrates, our method not
only achieves more consistent performance growth but also reaches the highest final pass rate among
all compared approaches. This evidence supports the view that concentrating the questioner’s reward
at the responder’s competence frontier stabilizes the optimization process while preserving headroom
for their mutual co-evolution.
10

Preprint under review

5.5

H YPERPARAMETERS A NALYSIS

Sensitivity of group size (G) We ex- Table 3: Ablation analysis of SPELL varying the rollout group
amine the impact of the rollout group size G and the standard deviation σ using Qwen2.5-7B-Instruct.
size G on model performance using The default configuration is G = 8 and σ = 0.5/3.
Qwen2.5-7B-Instruct. As shown in TaDocMath Frames LB-MQA LB-V2 Average
ble 3, while G = 8 yields the best over- Settings
SPELL
45.8
46.7
63.1
33.2
47.2
all results, SPELL remains robust across
Group Size (G)
different group sizes. We select G = 8
as the default setting to strike a balance
G=4
44.3
47.1
62.5
31.8
46.4
46.0
47.4
62.7
31.9
47.0
between performance gains and compu- G = 16
tational efficiency during training.
Standard Deviation (σ)
σ = 0.5/6
45.3
47.0
62.3
32.8
46.9
Selection of standard deviation σ
σ = 0.5/2
45.5
46.0
59.4
31.8
45.7
The choice of σ in Eq. (7) is derived
from the statistical properties of the
Gaussian distribution, where approximately 99.7% of data points fall within three standard deviations (3σ) of the mean. In SPELL, we aim to concentrate the questioner’s reward at the point
of the responder’s maximal learning efficiency, where r̄res = 0.5. Accordingly, the mean is set to
µ = 0.5. Given this mean, the distance to either boundary of the valid average responder reward
range [0, 1] is 0.5. By setting 3σ = 0.5, we ensure the effective range of the questioner reward
covers the responder reward space, yielding σ = 0.5/3. To further validate this theoretical choice,
we conduct an ablation study on σ using Qwen2.5-7B-Instruct. As shown in Table 3, narrowing the
curve (σ = 0.5/6) has a minimal negative impact, as the reward remains well-focused. However,
widening the curve (σ = 0.5/2) significantly degrades performance, likely because it assigns higher
rewards to overly easy or hard questions, providing a less targeted training signal. This confirms that
σ = 0.5/3 is both theoretically sound and empirically effective.

5.6

ROLE OF E XTERNAL J UDGES IN V ERIFICATION

We investigate whether replacing the rule-based judge (CEM-based reward function) with a stronger
external model (gpt-oss-120b) benefits the self-play process. As shown in Table 4, introducing
a stronger external judge does not yield a significant overall improvement. This suggests that
Qwen2.5-7B-Instruct is already capable of learning semantic verification through self-play without
external supervision. Notably, when an external judge is introduced, the internal verifier becomes less
important; removing it results in only a minor 0.5-point drop, compared to the significant 3.2-point
drop observed when using the rule-based judge. This highlights the critical role of the internal verifier
in complementing the brittle CEM-based reward function when an external judge is not available.
Table 4: Comparison of SPELL trained with rule-based judge versus an external judge (gpt-oss-120b). The
verifier is crucial when using a rule-based judge, but becomes less critical when including an external judge.
Method

6

DocMath

Frames

LB-MQA

LB-V2

Average

Qwen2.5-7B-Instruct

38.4

40.3

45.1

29.0

38.2

+ SPELL (Rule-based Judge)
+ SPELL (Gpt-oss-120b Judge)

45.8
47.1

46.7
48.0

63.1
61.6

33.2
32.1

47.2
47.2

+ SPELL (Rule-based Judge) w/o Verifier
+ SPELL (Gpt-oss-120b Judge) w/o Verifier

39.4
47.0

46.6
47.2

60.4
61.1

29.4
31.3

44.0
46.7

C ONCLUSION

This work introduces SPELL, a multi-role self-play reinforcement learning framework for evolving
the long-context reasoning capabilities of LLMs without human supervision. A single policy model
alternates among the roles of questioner, responder, and verifier to generate questions, solve them,
and assess the solutions, which reduces reliance on costly and unreliable human annotation while
enabling stable self-evolution. Extensive experiments across 12 models of diverse architectures and
sizes show that SPELL delivers consistent and substantial improvements in long-context reasoning.
This study concludes with three notable findings. First, signals from verifiable tasks can calibrate
and strengthen the verifier’s assessment on non-verifiable outputs, thereby ensuring a reliable self11

Preprint under review

rewarding mechanism. Second, within a multi-role self-play framework, sustaining a dynamic
equilibrium among the capabilities of different roles is critical for the stable evolution of the shared
policy. Finally, our results demonstrate that for models approaching or surpassing human performance, where external supervision emerges as a fundamental bottleneck, autonomous self-evolution
transitions from a promising alternative to an indispensable strategy for sustained advancement.

E THICS S TATEMENT
This research focuses on the development of long-context LLMs through self-play that requires
no human supervision. While we believe our methodology does not inherently raise significant
ethical issues, we acknowledge the potential for misuse of this technology. We also recognize that an
unsupervised learning approach may perpetuate or amplify societal biases in the model. Our research
is conducted using only publicl y available datasets, in compliance with their licenses, and involves
no personally identifiable information. We have adhered to all relevant ethical and legal standards
and declare no conflicts of interest that could have influenced the outcomes of this study.

R EPRODUCIBILITY S TATEMENT
To ensure the reproducibility of our work, we provide full experimental details in Section 5.1 and
Appendix C. These include our methods for dataset construction, training configurations, and the
evaluation setup. The implemented code, the data used, and a comprehensive guide to reproduce our
method are available in the supplementary materials.

R EFERENCES
Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun
Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint
arXiv:2504.03380, 2025.
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
LongAlign: A recipe for long context alignment of large language models. In Findings of the
Association for Computational Linguistics: EMNLP 2024, 2024a.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual,
multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), 2024b.
Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu,
Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench v2: Towards deeper understanding
and reasoning on realistic long-context multitasks. In Proceedings of the 63rd Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), 2025.
Guanzheng Chen, Xin Li, Michael Shieh, and Lidong Bing. Longpo: Long context self-evolution of
large language models through short-to-long preference optimization. In International Conference
on Learning Representations, 2025a.
Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-questioning
language models. arXiv preprint arXiv:2508.03682, 2025b.
Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Hang Yan, Kai Chen, and
Dahua Lin. What are the essential factors in crafting effective long context multi-hop instruction
datasets? insights and best practices. In Proceedings of the 63rd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), 2025c.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
2021.
12

Preprint under review

Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit
Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier
with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
arXiv preprint arXiv:2507.06261, 2025.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu,
Jiahao Qiu, Xuan Qi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial super
intelligence. arXiv preprint arXiv:2507.21046, 2025.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu
Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633–638, 2025.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference on
Learning Representations, 2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In
Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
(Round 2), 2021b.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop
qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International
Conference on Computational Linguistics, 2020.
Jian Hu. Reinforce++: A simple and efficient approach for aligning large language models. arXiv
preprint arXiv:2501.03262, 2025.
Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum.
Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base
model. arXiv preprint arXiv:2503.24290, 2025.
Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin
Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv
preprint arXiv:2508.05004, 2025.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint
arXiv:2412.16720, 2024.
Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam
Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrievalaugmented generation. In Proceedings of the 2025 Conference of the Nations of the Americas
Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume
1: Long Papers), 2025.
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,
Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in
open language model post-training. arXiv preprint arXiv:2411.15124, 2024.
Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence
is all you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395,
2025.
Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston
Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via
multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025a.
13

Preprint under review

Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang,
Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. A comprehensive survey on long context
language modeling. arXiv preprint arXiv:2503.17407, 2025b.
Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng,
Aili Chen, Shiqi Chen, et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning
logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025c.
Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min
Lin. Understanding r1-zero-like training: A critical perspective. In Conference on Language
Modeling (COLM), 2025d.
MAA. American invitational mathematics examination - AIME, 2025. URL https://maa.org/
maa-invitational-competitions/.
OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://cdn.openai.com/
pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.
pdf.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in neural information processing systems, 2023.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,
2015.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,
Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. In Proceedings
of the Twentieth European Conference on Computer Systems, 2025.
Tianyuan Shi, Canbin Huang, Fanqi Wan, Longguang Zhong, Ziyi Yang, Weizhou Shen, Xiaojun
Quan, and Ming Yan. Mutual-taught for co-adapting policy and reward models. In Proceedings
of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), 2025.
Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Artificial intelligence
and statistics, pp. 886–894. PMLR, 2014.
Toby Simonds, Kevin Lopez, Akira Yoshiyama, and Dominique Garmier. Self rewarding self
improving. arXiv preprint arXiv:2505.08827, 2025.
Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian
Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher++: Incentivizing the dynamic
knowledge acquisition of llms via reinforcement learning. arXiv preprint arXiv:2505.17005, 2025.
Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan,
Ming Yan, Ji Zhang, et al. Solopo: Unlocking long-context capabilities in llms via short-to-long
preference optimization. arXiv preprint arXiv:2505.11166, 2025.
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru
Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint
arXiv:2507.20534, 2025.
14

Preprint under review

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop
questions via single-hop question composition. Transactions of the Association for Computational
Linguistics, 10:539–554, 2022.
Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna,
Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context
evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024.
Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei
Huang, Jingren Zhou, and Ming Yan. Qwenlong-l1: Towards long-context large reasoning models
with reinforcement learning. arXiv preprint arXiv:2505.17667, 2025.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171, 2022.
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming
Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi
Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: A more robust and challenging multi-task language
understanding benchmark. In The Thirty-eight Conference on Neural Information Processing
Systems Datasets and Benchmarks Track, 2024.
Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou,
Guoyang Zeng, Chaojun Xiao, et al. Ultra-fineweb: Efficient data filtering and verification for
high-quality llm training data. arXiv preprint arXiv:2505.05427, 2025.
Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried,
Gabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via
reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025.
Tian-Shi Xu, Hsiao-Dong Chiang, Guang-Yi Liu, and Chin-Woo Tan. Hierarchical k-means method
for clustering large-scale advanced metering infrastructure data. IEEE Transactions on Power
Delivery, 32(2):609–616, 2015.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, et al.
Qwen2 technical report. ArXiv, abs/2407.10671, 2024a.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical
expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388,
2025.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, 2018.
Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat,
and Danqi Chen. HELMET: How to evaluate long-context models effectively and thoroughly. In
International Conference on Learning Representations, 2025.
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong
Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.
arXiv preprint arXiv:2503.14476, 2025.
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu,
and Jason Weston. Self-rewarding language models. In International Conference on Machine
Learning, 2024.
15

Preprint under review

Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi
Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for
advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.
Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie,
An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and
reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025.
Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun
Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data.
arXiv preprint arXiv:2505.03335, 2025.
Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru
Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating math reasoning capabilities of
llms in understanding long and specialized documents. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), 2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in neural information processing systems, 2023.
Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging
language model agents. arXiv preprint arXiv:2506.01716, 2025.
Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen
Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint
arXiv:2504.16084, 2025.

16

Preprint under review

A

L IMITATIONS AND F UTURE W ORK

First, while our study provides strong empirical evidence, a theoretical framework explaining the
co-evolutionary dynamics of the three roles within a single model has yet to be explored. Second, due
to framework and efficiency constraints, our experiments are limited to a 16K input context. Although
the acquired skills generalize well to longer contexts, it is a critical next step to develop more efficient
frameworks for self-play RL on ultra-long contexts, such as 128K tokens and beyond. Finally,
our self-play framework still relies on a degree of human intervention, such as pre-processing the
document corpus and crafting prompt templates for each role. Future work could explore pathways
toward greater autonomy, such as a system where an LLM interacts with a real-world environment to
generate and evolve its own tasks, templates, and reward functions.

B

SPELL A LGORITHM

In this section, we outline the step-by-step algorithm for SPELL in Algorithm 1.
Algorithm 1 The SPELL Algorithm
1: Require: Initial policy model πθ ; Dataset D = {(C, τ )}; Subset size m; History memory size

L; Batch size N ; Group size G
2: for (C, τ ) ∈ D do
3:
HC ← Queue(∅, L)

▷ Initialize a history memory for each document cluster

4: for each training step t = 1, 2, . . . do
5:
B que , B res , B ver ← ∅, ∅, ∅
▷ Initialize empty sample batches for three roles
6:
while |B res | < N do
7:
(C, τ ) ∼ D
▷ Sample a document cluster and task type
8:
Cnew ← SampleDocs(C, m)
▷ Sample a subset of m documents for questioner
9:
X que ← GetQuestionerContext(Cnew , τ, HC )
▷ Prepare questioner input; see §4.1
10:
(q, a) ∼ πθque (·|X que )
▷ Questioning: Generate a question with reference answer
11:
yno_context ∼ πθres (·|q)
▷ Attempt to answer the question without document context
12:
if Rrule (yno_context , a) = 1 then
▷ Grounding filter
13:
B que ← B que ∪ {(X que , q, a, rque = −0.5)} ▷ Penalize and store ungrounded question
14:
continue
▷ Discard question and proceed to the next iteration
15:
16:
17:

res
{yi }G
▷ Responding: Generate a group of G responses
i=1 ∼ πθ (·|C, q)
for i ← 1 to G do
ver G
{vi,j
}j=1 ∼ πθver (·|q, yi , a) ▷ Verifying: Generate G verifications for each response

18:
19:
20:
21:

ver
ver
rque , {rires }i , {{ri,j
}}i,j = ComputeReward(q, a, {yi }res
i , {{vi,j }}i,j )
▷ Compute role-specific rewards; see Eq. (4)-Eq. (7)
if rque > 0 then
▷ Append solvable questions to the history memory
PushToHistoryMemory(HC , (Cnew , q, a))
B que ← B que ∪ {(X que , q, a, rque )}
B res ← B res ∪ {(C, q, {yi }i , {rires }i )}
ver
B ver ← B ver ∪ {(q, a, {yi }i , {vi,j }i,j , {ri,j
}i,j )}

22:
23:
24:

B ← SampleBatch(B que , B res , B ver )
πθ ← UpdatePolicy(πθ , B)
27: Ensure: Updated policy model πθ
25:
26:

C

I MPLEMENTATION D ETAILS

C.1

T RAINING DATA C ONSTRUCTION

▷ Apply role-specific dynamic sampling; see §4.3
▷ Unified Policy Update; see §4.3

Our training data supports three distinct tasks: document general QA, financial math QA, and
multiple-choice QA. We construct the dataset from two complementary sources. The first is the
DocMath dataset (Zhao et al., 2024), which provides specialized data comprising long, complex
financial reports that necessitate numerical reasoning. We used only the raw documents, discarding
17

Preprint under review

LongBench-V2
Frames
DocMath
HotpotQA
2wikiMultihopQA
MuSiQue

700
600

Count

500
400
300
200
100
0

0k-4k

4k-16k

16k-32k 32k-64k 64k-128k

Token Length Interval

128k+

Figure 6: (Left) The word cloud of training document labels, with font size indicating frequency. The prominence
of terms like education, analysis, and learning underscores the dataset’s focus on knowledge-intensive content.
(Right) The token length distribution, calculated by the Qwen2 tokenizer, shows that our evaluation benchmarks
cover a wide spectrum of context lengths.

the original unlabeled questions. From this dataset, we select 2,150 instances with a total token length
below 16K, which are designated for the financial math QA task.
The second component is a general-domain corpus designed to enhance the model’s fundamental
document understanding. This dataset originates from the 1.16 billion English documents in the
Ultra-Fineweb corpus (Wang et al., 2025) and is curated through the following multi-stage pipeline.
First, an initial filtering stage retains high-quality texts of appropriate length by selecting documents
with a perfect quality score of 1.0 and character lengths between 100 and 32,768. This step downsamples the corpus to a high-quality subset of 4 million documents. Second, a cleaning procedure ensures
data diversity and prevents test set contamination. We employ the MinHashLSH algorithm (Shrivastava & Li, 2014) with a threshold of 0.8 and 128 permutation functions for deduplication and to
decontaminate the data against all documents in our evaluation benchmarks. This cleaning phase
refines the corpus to 1.1 million unique and decontaminated texts. Third, a hierarchical clustering
approach structures the dataset into different topics. We generate 4096-dimensional embeddings for
each text using the Qwen3-Embedding-8B model (Zhang et al., 2025), which are then grouped into
50,000 distinct clusters via a hierarchical k-means algorithm (Xu et al., 2015). Each cluster contains
an average of 20 semantically related documents, and the resulting domain distribution, visualized in
Figure 6 (left), confirms the dataset’s broad topical diversity. This curated corpus from Ultra-Fineweb
is used to generate training instances for the document general QA and multiple-choice tasks. To
ensure a comparable scale across tasks, the number of clusters selected for these two tasks matches
the data size of the DocMath portion. Ultimately, our combined training data consists of 6,450 unique
document sets, with 2,150 designated for each of the three tasks.
C.2

E VALUATION D ETAILS

Evaluation Benchmarks We evaluate our models using a suite of well-established benchmarks
designed to assess long-context comprehension and reasoning. These benchmarks fall into two
primary categories: multiple-choice and multi-hop question answering (QA). For the multiplechoice task, we use LongBench-V2 (Bai et al., 2025), a benchmark of 503 questions that assesses
deep comprehension across six areas: single-document QA, multi-document QA, long in-context
learning, long-dialogue history understanding, code repository understanding, and long structured data
understanding. For multi-hop QA, our evaluation incorporates several benchmarks: Frames (Krishna
et al., 2025), containing 824 questions on diverse Wikipedia topics such as history, sports, science,
animals, and health; three subsets from LongBench (Bai et al., 2024b), each with 200 questions,
namely HotpotQA (Yang et al., 2018) (2-hop), 2WikiMultihopQA (Ho et al., 2020) (requiring up
to five hops), and MuSiQue (Trivedi et al., 2022) (requiring up to four hops); and DocMath (Zhao
et al., 2024), which focuses on numerical reasoning within financial reports. For DocMath, we use
the testmini subset of 800 queries, which is orthogonal to our training data. As shown in Figure 6
(right), the test instances across these benchmarks cover a wide range of context lengths.
Evaluation Configurations We evaluate all models at two maximum input lengths: 16K tokens,
which aligns with our training configuration, and 100K tokens to test for generalization to longer
contexts. The maximum generation length is 4K tokens for non-reasoning models and is extended to
18

Preprint under review

20K tokens for reasoning models. For prompts exceeding the maximum context window, we employ
the middle truncation strategy from Bai et al. (2024b) to preserve the front and tail portions of the
context. All experiments are conducted using a sampling temperature of 0.7 and a top-p value of 0.95.
For each query, we generate n = 8 candidate responses, reporting the average score (pass@1) for our
main experiments and the pass@k metric for the test-time scaling analysis. The pass@k metric is an
unbiased estimator for the probability that at least one of k candidate solutions is correct, given n
candidates per problem, of which c are correct. It is calculated as:

n−c
k
(11)
pass@k = 1 − n
k

Our scoring is tailored to each benchmark’s format. For multiple-choice tasks, we report standard
accuracy. For multi-hop QA tasks, simple string matching is often insufficient to assess the semantic
correctness of free-form text answers. Thus, we supplement the cover exact match (CEM) score
with LLM-as-a-judge (Zheng et al., 2023), which uses gpt-oss-120b (OpenAI, 2025) to evaluate
semantic equivalence between a model’s prediction and the ground-truth answer. The prompt for this
evaluation is detailed in Table 5. The final score for these tasks is the maximum of the two, providing
a more comprehensive and robust assessment of model performance.
Table 5: Prompt template for LLM-as-a-judge to compare the predicted answer and the ground truth given the
question. Modified from Frames (Krishna et al., 2025).

LLM Judge Prompt for Multi-Hop QA Tasks.
## TASK
I need your help in evaluating an answer provided by an LLM against a ground truth answer.
Your task is to determine if the ground truth answer is present in the LLM’s response. Please
analyze the provided data and make a decision.
## Instruction
1. Carefully compare the “Predicted Answer” with the “Ground Truth Answer”.
2. Consider the substance of the answers - look for equivalent information or correct answers.
Do not focus on exact wording unless the exact wording is crucial to the meaning.
3. Your final decision should be based on whether the meaning and the vital facts of the
“Ground Truth Answer” are present in the “Predicted Answer”.
4. Your decision must be one of the “[[YES]]” or “[[NO]]”.
## Input Data
- Question: {question}
- Predicted Answer: {predicted answer}
- Ground Truth Answer: {ground truth}
## Output Format
Provide your final evaluation in the following format:
“Explanation:” “How you made the decision”
“Decision:” “[[YES]]” or “[[NO]]”
Please proceed with the evaluation.

C.3

RL A LGORITHM D ETAILS

To enhance stability and practical performance, we integrate two key techniques inspired by Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) (Yu et al., 2025) for the baselines
and our method. First, we adopt a token-level policy gradient loss, which normalizes each token’s
contribution by the total number of tokens in the group. This approach ensures every token in the
same group contributes equally to the final objective, which prevents the learning signal from valuable
tokens in high-quality, long responses from being diluted while ensuring that undesirable patterns
in low-quality, lengthy outputs are effectively penalized. Second, we employ a dynamic sampling
strategy: any group of generations where the rewards {ri }G
i=1 exhibit zero variance is discarded from
the training batch. This ensures the advantage in Eq. (3) is always well-defined.
Consistent with recent findings suggesting that removing KL regularization can improve exploration
and accelerate convergence (Hu et al., 2025; Yu et al., 2025; Wan et al., 2025), we set β = 0. Besides,
19

Preprint under review

we operate in a strictly on-policy setting, performing only a single gradient update per batch of
samples. This design choice implies that the policy being updated, πθ , remains identical to the policy
that generated the data, πθold . Since the importance sampling ratio ρi,t (θ) is strictly equal to 1, the
clipping function becomes inactive, and we can remove it from the objective. Note that the advantage
Ai is independent of t, the training objective in Eq. (2) simplifies to:


|yi |
G
X
X
1
P
Ai
ρi,t (θ) .
(12)
JGRPO (θ) = Ec,q∼D,{yi }G
G
i=1 ∼πθold
t=1
j=1 |yj | i=1
C.4

D ETAILS OF BASELINES

We compare SPELL with two categories of baselines. The first category comprises the base models on
which SPELL is built, organized into three groups: (1) base models, including Qwen-2.5-7B, Qwen2.5-14B, and Qwen-2.5-32B; (2) instruction-tuned models, including Qwen-2.5-7B-Instruct, Qwen2.5-14B-Instruct, Qwen-2.5-32B-Instruct, Qwen3-30B-A3B-Instruct, and Llama-3.1-8B-Instruct; and
(3) reasoning models, including R1-Distill-Llama-8B, R1-Distill-Qwen-14B, Qwen3-4B-Thinking,
and Qwen3-30B-A3B-Thinking (Yang et al., 2025; 2024a; Guo et al., 2025; Dubey et al., 2024).
The second baseline is traditional RLVR. For data construction, we use DeepSeek-R1-0528 (Guo
et al., 2025) as both the questioner and responder to synthesize a dataset from the same document
clusters used for SPELL. The synthesized dataset is then filtered by a verifier model (gpt-oss-120b),
and retains only the instances where the answers from the questioner and responder are identical.
This process yields a dataset of approximately 3,000 verifiable samples that covers multiple-choice,
document general QA, and financial math QA tasks. For RLVR training, we employ the cover exact
match (CEM) as the rule-based reward function. We generate eight trajectories per question and
maintain all other hyperparameters identical to those of SPELL to ensure a fair comparison.
C.5

D ETAILS OF O PEN -S OURCE M ODELS AND THE DATASET

In Table 6, we provide the Huggingface repository names of all policy models, the embedding model,
the judge model, and datasets used in our experiments.
Table 6: Details of open-source models and datasets in our experiments.

Model/Dataset
Qwen2.5-7B
Qwen2.5-14B
Qwen2.5-32B
Qwen2.5-7B-Instruct
Qwen2.5-14B-Instruct
Qwen2.5-32B-Instruct
Qwen3-30B-A3B-Instruct
Llama-3.1-8B-Instruct
R1-Distill-Llama-8B
R1-Distill-Qwen-14B
Qwen3-4B-Thinking
Qwen3-30B-A3B-Thinking
Qwen3-Embedding-8B
gpt-oss-120b

Huggingface ID
Qwen/Qwen2.5-7B
Qwen/Qwen2.5-14B
Qwen/Qwen2.5-32B
Qwen/Qwen2.5-7B-Instruct
Qwen/Qwen2.5-14B-Instruct
Qwen/Qwen2.5-32B-Instruct
Qwen/Qwen3-30B-A3B-Instruct-2507
meta-llama/Meta-Llama-3.1-8B-Instruct
deepseek-ai/DeepSeek-R1-Distill-Llama-8B
deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
Qwen/Qwen3-4B-Thinking-2507
Qwen/Qwen3-30B-A3B-Thinking-2507
Qwen/Qwen3-Embedding-8B
openai/gpt-oss-120b

DocMath
Ultra-Fineweb

D

yale-nlp/DocMath-Eval
openbmb/Ultra-FineWeb

A DDITIONAL A NALYSIS

This section provides further insights into the properties of SPELL. We begin by analyzing the
training cost in Section D.1 and exploring generalization to short-context tasks in Section D.2. We
then extend our evaluation to additional long-context benchmarks in Section D.3 and compare our
approach with state-of-the-art long-context alignment baselines in Section D.4. The section concludes
20

Preprint under review

with an analysis of the training dynamics of role evolution in Section D.5 and a discussion on reward
hacking risks and mitigations in Section D.6.
D.1

T RAINING C OST A NALYSIS

Time (s)

We analyze the computational cost of SPELL using
Questioning
Verifying
the Qwen2.5-7B-Instruct model on a single node with
Responding
Updating
200
8 × 80GB NVIDIA A100 GPUs. Figure 7 illustrates
the time breakdown for the two primary stages of
150
our framework: role-specific rollout (questioning, responding, and verifying) and unified updating. The
100
total time per training step averages approximately
seven minutes, and the total training cost is about
50
eight hours. Although the number of verifier rollouts
0 0
is G times greater than that of the responder, as out10
20
30
40
50
60
70
Training Steps
lined in Section 4.3, its computational cost is lower.
This efficiency stems from two factors: the verifier
processes shorter inputs, as it does not require the Figure 7: Time breakdown per training step for the
long documents provided to the responder, and its four main stages of SPELL. Verifying constitutes
a small amount of the total cost.
task of generating brief verifications is less demanding than the responder’s task of reasoning over long contexts. During the later stages of training,
the time for verifying constitutes only about half of the time required for responding. Consequently,
generating G verification judgments for each response to create a reliable reward signal does not
introduce a significant computational bottleneck. As shown in Section 5.3 and Table 2, the verifier
provides a significant 3.2-point performance gain (from 44.0 to 47.2). The significant performance
gain by introducing the verifier far outweighs the minor increase in training cost.
D.2

S HORT-C ONTEXT R EASONING R ESULTS

We further investigate the generalization of our proposed SPELL method from long-context reasoning
to short-context reasoning tasks. Our evaluation suite includes five mathematical benchmarks and the
MMLU Pro general knowledge benchmark as follows:
AIME 24/25 (MAA, 2025) is the American Invitational Mathematics Examination, a prestigious
high-school mathematics competition administered by the Mathematical Association of America
(MAA). The AIME consists of two exams (I and II) annually, each containing 15 problems, for
a total of 30 problems per year. The answer to each question is an integer from 0 to 999. These
problems demand deep mathematical knowledge and creative problem-solving strategies, making
them a challenging benchmark for advanced mathematical reasoning.
AMC 234 (Yang et al., 2024b) refers to the 2023 American Mathematics Competition. This competition features 25 questions designed to test advanced high school mathematics, covering topics such
as trigonometry, advanced algebra, and advanced geometry.
MATH (Hendrycks et al., 2021b) is a dataset of math problems ranging in difficulty from middle
school to high school competition level. It is designed to test a wide range of mathematical skills,
including algebra, geometry, number theory, and counting & probability. For our evaluation, we
utilize a subset of 500 problems, referred to as MATH-500.
GSM8K (Cobbe et al., 2021) is a dataset of 1,319 grade school math word problems. These problems
are designed to be solvable by a capable middle school student and require two to eight steps of
reasoning using basic arithmetic operations.
MMLU-Pro (Wang et al., 2024) is an enhanced version of the MMLU (Hendrycks et al., 2021a)
dataset, designed to address issues such as noisy data and reduced difficulty resulting from advances
in model capabilities and increased data contamination. MMLU-Pro raises the difficulty by expanding
the multiple-choice options from four to as many as ten and incorporating expert-reviewed annotations
for improved quality and reduced noise.
4

https://huggingface.co/datasets/AI-MO/aimo-validation-amc

21

Preprint under review

Table 7: Evaluation results for base models on short-context reasoning tasks.
Model

AIME24

AIME25

AMC23

GSM8K

MMLU Pro

Average

Qwen2.5-7B
+ SPELL

5.42
9.17+3.75

3.33
5.00+1.67

33.44
40.31+6.87

MATH500
53.60
63.60+10.00

76.57
86.28+9.71

40.24
49.78+9.54

35.43
42.36+6.93

Qwen2.5-14B
+ SPELL

6.67
12.08+5.41

5.83
10.42+4.59

37.81
50.31+12.50

61.68
72.40+10.72

84.31
91.36+7.05

46.67
58.86+12.19

40.50
49.24+8.74

Qwen2.5-32B
+ SPELL

9.17
15.83+6.66

5.83
8.33+2.50

45.31
55.62+10.31

66.25
76.00+9.75

87.34
90.25+2.91

48.89
60.22+11.33

43.80
51.04+7.24

Consistent with our main experiments, all models are evaluated with a maximum output of 4K tokens,
a sampling temperature of 0.7, and a top-p value of 0.95. We report the accuracy averaged over 8
independent runs for each task. As shown in Table 7, SPELL, trained solely on the long-context
data, improves performance on short-context reasoning benchmarks across all base models. The
average scores of Qwen2.5-7B, Qwen2.5-14B, and Qwen2.5-32B increase by 6.93, 8.74, and 7.24
points, respectively. The consistent gains indicate that reasoning competencies acquired through
long-context self-play transfer effectively to short-context settings.
D.3

A DDITIONAL L ONG -C ONTEXT B ENCHMARKS

We further evaluate SPELL on two challenging long-context benchmarks: MRCR (Vodrahalli et al.,
2024)5 , a multi-needle “Needle in a Haystack” benchmark, and three subsets of HELMET (Yen
et al., 2025), which covers Retrieval-Augmented Generation (RAG), In-Context Learning (ICL),
and Summarization (Summ). We evaluate the Qwen2.5 base models against the RLVR baseline
and SPELL with a maximum input length of 16K and maximum output length of 4K. The results
in Table 8 show that SPELL consistently and significantly outperforms both the base models and
the RLVR baseline across these diverse long-context tasks, demonstrating strong generalization
capabilities beyond the standard QA tasks.
Table 8: Evaluation results for base models on MRCR and HELMET subsets. The best score in each model
group is highlighted in bold.
Model

MRCR-2needle

MRCR-4needle

MRCR-8needle

Helmet-RAG

Helmet-ICL

Helmet-Summ

Average

Qwen2.5-7B
+ RLVR
+ SPELL

6.9
22.0
34.5

2.0
12.5
16.5

2.2
10.5
16.0

50.0
49.3
54.2

3.5
2.6
10.4

4.1
14.3
13.7

11.5
18.5
24.2

Qwen2.5-14B
+ RLVR
+ SPELL

23.1
20.9
35.0

10.5
9.5
22.1

10.0
9.1
17.8

42.4
46.7
52.3

1.5
42.0
39.2

3.7
24.7
23.0

15.2
25.5
31.6

Qwen2.5-32B
+ RLVR
+ SPELL

27.0
36.7
38.0

11.5
14.6
18.5

12.0
13.0
14.7

59.0
52.7
61.4

42.8
16.7
56.4

16.2
21.0
21.2

28.1
25.8
35.0

D.4

C OMPARISON WITH L ONG -C ONTEXT A LIGNMENT BASELINES

We compare SPELL against three recent long-context alignment baselines—SoLoPO (Sun et al.,
2025), LongPO (Chen et al., 2025a), and QwenLong-L1 (Wan et al., 2025)—using Qwen2.5-7BInstruct as the base model. To ensure a fair comparison, we reimplement these methods using the
same document corpus employed in SPELL. For SoLoPO and LongPO, the core step is to construct preference pairs from short contexts containing key information and long contexts containing
distractors given the same question. Specifically, for each data instance comprising n documents,
we first randomly sample m = 5 documents as the short text and employ DeepSeek-R1-0528 as
the questioner to generate QA pairs. Then, we use DeepSeek-R1-0528 as the responder to answer the proposed questions using the full set of n documents. We retain only those QA pairs
where the answers from the questioner and responder are consistent. Next, we take the 5 documents from the questioning stage as short texts, corresponding to xshort in SoLoPO and xS in
LongPO, and take all n documents as long texts, corresponding to xlong in SoLoPO and xL in
5

https://huggingface.co/datasets/openai/mrcr

22

Preprint under review

LongPO. Finally, we apply their respective preference pair construction strategies and training
configurations on Qwen2.5-7B-Instruct to reproduce these methods. For QwenLong-L1, we use
the same synthesized data as our RLVR
Table 9: Comparison of SPELL against different long-context
baseline and follow their official GRPO alignment baselines. The best score is highlighted in bold
training setup. We evaluate both our
Model
DocMath Frames LB-MQA LB-V2 Average
reimplemented models and the official
Qwen2.5-7B-Instruct
38.4
40.3
45.1
29.0
38.2
LongPO checkpoint using a maximum
+ RLVR
45.0
48.7
59.6
30.1
45.9
+ LongPO (Reimpl.)
41.4
44.2
53.7
32.0
42.8
input length of 16K and a maximum
+ LongPO (Official)
42.3
41.4
45.7
30.9
40.1
output length of 4K. The results in Ta- + SoLoPO (Reimpl.)
45.3
43.9
56.0
31.6
44.2
46.7
60.0
32.0
46.1
ble 9 demonstrate that SPELL consis- + QwenLong-L1 (Reimpl.) 45.6
+ SPELL (Ours)
45.8
46.7
63.1
33.2
47.2
tently outperforms these long-context
alignment baselines.
D.5

E VOLUTIONARY DYNAMICS OF Q UESTIONER AND V ERIFIER

To understand the self-evolutionary process and identify potential failure modes, we analyze the
behavior of the questioner and verifier roles across different training steps.

Proportion (%)

Disagreement Rate (%)

Questioner dynamics We track the distribution of valid questions generated by
General QA
Financial Math QA
Multiple-choice
the questioner throughout the training pro25
70
cess. As shown in Figure 8(a), the task
20
distribution is notably imbalanced during
50
15
the first 10 steps, with Financial Math QA
10
30
accounting for over 70% of solvable tasks.
5
This imbalance likely occurs because the
10
model transfers its strong mathematical rea10
20
30
40
50
60
70
10
20
30
40
50
60
70
Training Steps
Training Steps
soning capabilities to the Financial Math
(a) Questioner Task Distribution
(b) Verifier Disagreement Rate
QA task, which necessitates substantial numerical calculation. As training progresses Figure 8: (a) Evolution of the task distribution generated by
the questioner. (b) The disagreement rate between the learned
and the policy evolves, the distribution be- verifier and the rule-based judge.
comes increasingly balanced. This indicates that SPELL effectively drives the questioner to explore a diverse range of task types.
Verifier calibration We analyze the disagreement rate between the verifier’s majority vote and the
rule-based judge (CEM). Figure 8(b) plots this metric for General QA and Financial Math QA, both of
which contain questions that are partially non-verifiable by strict rules. Initially, the verifier struggles
with General QA, which often involves open-ended semantic equivalence that rule-based checks fail
to capture. In contrast, Financial Math QA exhibits a lower initial disagreement rate, attributed to the
model’s relatively strong numeric reasoning ability. The disagreement rate consistently decreases
for both tasks, indicating that the verifier progressively aligns with the rule-based judge. This trend
further suggests that the verifier’s updates guide the questioner toward generating questions that are
more verifiable by the rule-based judge.
D.6

A NALYSIS OF R EWARD H ACKING

Reward hacking is a significant concern in self-play systems. Throughout our exploratory experiments,
we identified several potential failure modes and implemented specific mitigations.
Questioner stagnation Without the automated curriculum and history memory, the questioner
tends to repeatedly propose similar, trivial questions about the same document to maximize the
responder’s success rate. The history memory module conditions the questioner on recently solvable
QA pairs and newly introduced documents. The prompt in Appendix E explicitly instructs the
questioner to produce novel and more difficult questions.
Responder mode collapse When the responder receives rewards for outputs that are merely
substrings of the ground-truth answer, it can hack the metric by generating short phrases like “The
answer is”. We address this by enforcing a stricter cover exact match (CEM) criteria, which requires
the responder to include the complete ground-truth answer to receive a positive reward.
Verifier self-delusion Updating the verifier solely based on its own majority vote as a pseudo-label
can lead to verifier hacking. Without external supervision, verification errors accumulate, eventually
23

Preprint under review

causing the verifier to label all responders’ outputs as correct. To mitigate this, we introduce the
consistency check mechanism, which aligns the verifier’s judgments with rule-based rewards on
verifiable questions, thereby preventing this self-delusion.
These observations confirm that the architectural components of SPELL—specifically the history
memory, prompt templates, consistency checks, and CEM-based reward function—are essential to
mitigate reward hacking and ensure stable self-evolving.

E

P ROMPT T EMPLATE

In this section, we detail the prompt templates for the questioner, responder, and verifier across
all tasks. For the questioner, we apply different prompting strategies for document clusters with
and without history memory; these prompts are modified from the guidelines for human annotators
in LongBench-V2 (Bai et al., 2025). The responder prompt for financial math QA is modified
from DocMath (Zhao et al., 2024), and the prompts for document general QA and multiple-choice
are modified from LongBench-V2. The verifier prompt for document QA tasks is modified from
Frames (Krishna et al., 2025) and requires the model to evaluate the semantic equivalence of a
generated answer against a ground-truth reference.
Questioner Prompt for Document General QA Task without History Memory
You are an expert in document analysis. We are building a benchmark to evaluate the
capabilities of large language models (LLMs) on fact retrieval, reasoning across multiple
constraints, and accurate synthesis of information into coherent responses. Your primary
task is to propose a challenging question based on the provided document context enclosed
between <text> and </text>. The question must require both document comprehension and
multi-hop reasoning. You must also provide the correct answer and a detailed step-by-step
derivation showing how the answer is obtained from the document.
## Principles for Question Design
Adhere strictly to the following principles when crafting your question, answer, and derivation
1. Language Requirement: Questions, answers, and derivations must be in English.
2. Standalone & Context-Independent: Questions should not contain any references to
“Article 1”, “Article 2”, etc. They should be understandable without any additional context.
3. Unambiguous Answer: Each question should have a single, clear, and factual answer.
4. Multi-hop Reasoning: Answering each question should require combining information
from ALL provided documents. The final answer cannot be found in any single document.
5. Guideline for Question Phrasing: Strive for a natural and seamless integration of
information from each document. A good question often:
- Starts with a clear question word (What/How/Where/When).
- Links constraints from different documents using logical connectors.
- Example connectors: ‘in relation to’, ‘given the condition of’, ‘as a result of’, ‘which also
affects’, ‘in addition to’.
6. Answer & Step-by-Step Derivation:
- The answer must be a concise phrase or sentence. An answer with more than 20 tokens is
forbidden.
- The derivation must be a clear, step-by-step logical chain. Each step must explicitly cite
the specific data point or phrase and its source from the context (e.g., “From Table 3, Row
‘Revenue’, Year 2023...” or “As stated in paragraph 2...”).
## Output Format
Your response must conclude with a JSON object containing the following keys: “question”
and “answer”, placed after your reasoning.
{
“question”: “<A well-structured English question that adheres to all design principles>”,
“answer”: “<A concise answer, under 20 tokens>”,
}
## Document Context
<text>
{context}
</text>

24

Preprint under review

Questioner Prompt for Financial Math QA Task without History Memory
You are an expert in document analysis and numeric reasoning. We are building a benchmark
to evaluate the numerical reasoning capabilities of large language model’s (LLMs) when
analyzing specialized documents containing both text and tables. Your primary task is to
propose a challenging question based on the provided document context enclosed between
<text> and </text>. The question must require both document comprehension and multi-step
mathematical reasoning to arrive at a single, non-zero numerical answer. You must also
provide the correct numerical answer and a detailed step-by-step derivation showing how
the answer is obtained from the document.
## Principles for Question Design
Adhere strictly to the following principles when crafting your question, answer, and derivation:
1. Language Requirement: Questions, answers, and derivations must be in English.
2. Complexity and Reasoning Depth:
- The question must be challenging, requiring the LLM to go beyond simple retrieval. It
should not be solvable trivially or in a few inference steps.
- It must involve multi-step mathematical reasoning (e.g., requiring two or more distinct
calculation steps).
- It should necessitate integration of information from different parts of the document
(e.g., combining data from a table with information from a text paragraph, or using multiple
rows/columns from a table).
- Aspects like summarization or complex information extraction can be part of the process.
3. Avoided Question Types:
- Simple Counting: Avoid questions like “How many X are there?” if X is easily countable or
directly stated. If counting is involved as an intermediate step for a larger calculation and the
count is small (<=10), it’s acceptable.
- Direct Retrieval: Avoid questions answerable by looking up a single, isolated piece of
information.
- Excessive External Knowledge: Questions should primarily rely on the provided document.
Only common sense or minimal domain-specific knowledge (e.g., basic financial concepts
like ‘profit = revenue - cost’ if contextually appropriate and derivable) inferable from the
document is allowed.
4. Information Obscurity:
- Start with a clear question word (What/How/Where/When).
- Do not explicitly mention or paraphrase key numerical values from the document within the
question itself. The LLM should identify and extract these values.
- Phrase questions to require inference and understanding of relationships between data points
rather than just locating them.
5. Factual Grounding:
- All information required to answer the question must be present in or directly derivable from
the provided document.
- Do not introduce hypothetical scenarios, fictional data, or assumptions not supported by the
document.
- Questions should not contain any references to “Article 1”, “Article 2”, etc. They should be
understandable without any additional context.
6. Numerical Answer:
- The final answer must be a single non-zero numerical value.
- An answer with more than two numerical values is unacceptable.
- If the document implies units (e.g., millions of dollars, percentages), the question should be
phrased such that the numerical answer alone is sufficient (e.g., “What is the value in millions
of dollars?” rather than expecting the answer to include “million dollars”).
7. Step-by-Step Derivation:
- Provide a clear, step-by-step derivation for your answer.
- This derivation must explicitly reference specific data points or phrases from the document
(e.g., “From Table 3, Row ‘Revenue’, Year 2023...” or “As stated in paragraph 2...”).
- Detail all mathematical operations performed in each step. This helps verify the question’s
solvability and reasoning path.
## Output Format

25

Preprint under review

Your response must conclude with a JSON object containing the following keys: “question”
and “answer”, placed after your reasoning.
{
“question”: “<A well-structured English question that adheres to all design principles>”,
“answer”: “<A single, non-zero numerical answer>”
}
## Document Context
<text>
{context}
</text>

Questioner Prompt for Document Multiple-Choice Task without History Memory
You are an expert in document analysis. We are building a benchmark to evaluate the
capabilities of large language models (LLMs) on fact retrieval, reasoning across multiple
constraints, and accurate synthesis of information into coherent responses. Your task is
to generate a multiple choice question based on the provided document context enclosed
between <text> and </text>. The question must require document comprehension and
multi-hop reasoning. You must provide one correct answer and three plausible, distinct
distractors. Crucially, you must also provide a detailed explanation for why the correct
answer is correct (including derivation steps) and why each distractor is incorrect.
## Principles for Question and Option Design
Adhere strictly to the following principles when crafting your question, answer, options, and
derivation:
1. General Requirements:
- All questions, options, and explanations must be in English.
- Questions should be challenging, requiring more than simple retrieval or a few inference
steps.
2. Cognitive Complexity Requirements for the Question:
- Must necessitate multi-step reasoning (e.g., involving three or more distinct logical or
calculation steps).
- Should require the integration of at least three distinct data points from different parts of the
document (e.g., combining data from a table with text, or using multiple rows/columns/cells).
- Should demand the synthesis of quantitative data with qualitative information found in the
text.
- The problem setup should have the potential for common misinterpretations, which will
inform distractor design.
3. Content Validity Criteria:
- The question and all options must be exclusively answerable using information from the
provided document. No external knowledge beyond common sense or very basic, universally
understood concepts (e.g., profit = revenue - cost, if directly applicable and data is provided)
should be required.
- If applicable to the document type (e.g., financial reports), prioritize questions with regulatory/compliance implications or those highlighting significant financial outcomes.
- Ensure numerical values involved in the question or options require contextual interpretation
within the document, not just direct look-up.
- Avoid trivia; focus on questions that address material information or key insights derivable
from the document.
4. Distractor Development Guidelines:
- Each of the three distractors must be plausible yet clearly incorrect upon careful analysis.
- Distractors should represent distinct error paths or common misinterpretations.
- At least one distractor should represent a common conceptual misunderstanding related to
the document’s content or how information is presented.
5. Forbidden Question/Option Patterns:
- Simple Counting: Avoid questions like “How many X are there?” if X is easily countable or
directly stated. Small counts (<=5) as part of a larger calculation are acceptable.

26

Preprint under review

- Direct Retrieval: Avoid questions where the answer (or its direct components) can be found
in a single, obvious location without further processing.
- Excessive External Knowledge: Questions must not require significant domain-specific
knowledge not provided or clearly inferable from the document.
- No Fabricated Information: Strictly adhere to document content. Do not introduce
hypothetical scenarios, data, or assumptions not explicitly stated or directly inferable.
- Ambiguous Scenarios: The question must have one unambiguously correct answer based
solely on the provided document.
- Vague Options: All options, including distractors, must be precise and unambiguous.
6. Answer and Explanation Requirements:
- The correct answer must be <correct_answer>.
- A detailed derivation for the correct answer must be provided, showing step-by-step calculations and referencing specific parts of the document (e.g., “From Table X, Row Y...”, “As
stated in paragraph Z...”).
- For each distractor, provide a brief explanation of why it is incorrect, ideally linking it to
the type of error it represents (e.g., “Option A is incorrect because it omits the X deduction
mentioned in...”, “Option B results from incorrectly summing X and Y instead of finding their
difference...”).
## Output Format
Your response must conclude with a JSON object containing the following keys: “question”,
“options”, and “answer”, placed after your reasoning.
{
“question”: “<A well-structured multiple choice English question, exclude choices and answer>”,
“options”: {
“A”: “<Text for choice A>”,
“B”: “<Text for choice B>”,
“C”: “<Text for choice C>”,
“D”: “<Text for choice D>”
},
“answer”: “<correct_answer>”
}
## Document Context
<text>
{context}
</text>
Questioner Prompt for Document General QA Task with History Memory
You are an expert in document analysis. We are building a benchmark to evaluate the capabilities of large language models (LLMs) on fact retrieval, reasoning across multiple constraints,
and accurate synthesis of information into coherent responses. Your primary task is to propose
ONE new, significantly more difficult question based on the provided document context and
a set of existing, simpler questions. The new question must be fundamentally different and
more complex than the provided examples, requiring both document comprehension and
multi-hop reasoning. You must also provide the correct answer and a detailed step-by-step
derivation showing how the answer is obtained from the document.
## Principles for Question Design
Adhere strictly to the following principles when crafting your question, answer, and derivation
1. Language Requirement: Questions, answers, and derivations must be in English.
2. Standalone & Context-Independent: Questions should not contain any references to
“Article 1”, “Article 2”, etc. They should be understandable without any additional context.
3. Unambiguous Answer: Each question should have a single, clear, and factual answer.
4. Multi-hop Reasoning: Answering each question should require combining information
from ALL provided documents. The final answer cannot be found in any single document.
5. Guideline for Question Phrasing: Strive for a natural and seamless integration of
information from each document. A good question often:

27

Preprint under review

- Starts with a clear question word (What/How/Where/When).
- Links constraints from different documents using logical connectors.
- Example connectors: ‘in relation to’, ‘given the condition of’, ‘as a result of’, ‘which also
affects’, ‘in addition to’.
6. Escalate Question Difficulty: The new question must demonstrate a higher order of
reasoning than the Previous Examples. First, analyze the examples to identify their simple
reasoning patterns (e.g., fact retrieval, single-step comparison). Then, create a new question
that incorporates one or more of the following advanced reasoning types:
- Quantitative Reasoning & Calculation: Requires performing mathematical operations (e.g.,
addition, subtraction, percentage change, averaging) on data from multiple sources.
- Comparative & Superlative Analysis: Requires comparing multiple entities based on synthesized criteria to find the one that is highest, lowest, best, etc.
- Conditional or Causal Reasoning: Structured as an “if-then” scenario or asks for the
cause/effect of a situation by linking different documents (e.g., “What would be the total cost
if the discount from Document A were applied to the price listed in Document B?”).
- Synthesis Across Data Types: Forces connection between qualitative information (e.g., a
policy description) and quantitative data (e.g., a number in a table) to reach a conclusion.
7. Answer & Step-by-Step Derivation:
- The answer must be a concise phrase or sentence. An answer with more than 20 tokens is
forbidden.
- The derivation must be a clear, step-by-step logical chain. Each step must explicitly cite
the specific data point or phrase and its source from the context (e.g., “From Table 3, Row
‘Revenue’, Year 2023...” or “As stated in paragraph 2...”).
## Output Format
Your response must conclude with a JSON object containing the following keys: “question”
and “answer”, placed after your reasoning.
{
“question”: “<A well-structured English question that adheres to all design principles>”,
“answer”: “<A concise answer, under 20 tokens>”,
}
## Document Context
<text>
{context}
</text>
## Previous Examples
### Example 1:
Question: {question 1}
Answer: {answer 1}
...
### Example K:
Question: {question k}
Answer: {answer k}
Questioner Prompt for Financial Math QA Task with History Memory
You are an expert in document analysis and numeric reasoning. We are building a benchmark
to evaluate the numerical reasoning capabilities of large language models (LLMs) when
analyzing specialized documents containing both text and tables. Your primary task is to propose ONE new, significantly more difficult question based on the provided document context
and a set of existing, simpler questions. The new question must be fundamentally different
and more complex than the provided examples, requiring both document comprehension
and multi-step mathematical reasoning to arrive at a single, non-zero numerical answer.
You must also provide the correct numerical answer and a detailed step-by-step derivation
showing how the answer is obtained from the document.
## Principles for Question Design
Adhere strictly to the following principles when crafting your question, answer, and derivation:
1. Language Requirement: Questions, answers, and derivations must be in English.

28

Preprint under review

2. Complexity and Reasoning Depth:
- The question must be challenging, requiring the LLM to go beyond simple retrieval. It
should not be solvable trivially or in a few inference steps.
- It must involve multi-step mathematical reasoning (e.g., requiring two or more distinct
calculation steps).
- It should necessitate integration of information from different parts of the document
(e.g., combining data from a table with information from a text paragraph, or using multiple
rows/columns from a table).
- Aspects like summarization or complex information extraction can be part of the process.
3. Avoided Question Types:
- Simple Counting: Avoid questions like “How many X are there?” if X is easily countable or
directly stated. If counting is involved as an intermediate step for a larger calculation and the
count is small (<=10), it’s acceptable.
- Direct Retrieval: Avoid questions answerable by looking up a single, isolated piece of
information.
- Excessive External Knowledge: Questions should primarily rely on the provided document.
Only common sense or minimal domain-specific knowledge (e.g., basic financial concepts
like ‘profit = revenue - cost’ if contextually appropriate and derivable) inferable from the
document is allowed.
4. Escalate Question Difficulty: The new question must demonstrate a higher order of
reasoning than the Previous Examples. First, analyze the examples to identify their simple
reasoning patterns (e.g., direct lookups, single calculations). Then, create a new question that
incorporates one or more of the following advanced reasoning types:
- Period-over-Period Calculation: Requires calculating growth, decline, or change between
different time periods.
- Ratio or Metric Derivation: Requires calculating a financial metric or ratio not explicitly
stated in the document.
- Aggregation and Filtering:
Requires aggregating data across multiple
rows/columns/sections after filtering based on a text-based condition.
- Projection or Implication: Requires using data from the document to answer a “what if” or
forward-looking question based only on the provided numbers.
5. Information Obscurity:
- Start with a clear question word (What/How/Where/When).
- Do not explicitly mention or paraphrase key numerical values from the document within the
question itself. The LLM should identify and extract these values.
- Phrase questions to require inference and understanding of relationships between data points
rather than just locating them.
6. Factual Grounding:
- All information required to answer the question must be present in or directly derivable from
the provided document.
- Do not introduce hypothetical scenarios, fictional data, or assumptions not supported by the
document.
- Questions should not contain any references to “Article 1”, “Article 2”, etc. They should be
understandable without any additional context.
7. Numerical Answer:
- The final answer must be a single non-zero numerical value.
- An answer with more than two numerical values is unacceptable.
- If the document implies units (e.g., millions of dollars, percentages), the question should be
phrased such that the numerical answer alone is sufficient (e.g., “What is the value in millions
of dollars?” rather than expecting the answer to include “million dollars”).
8. Step-by-Step Derivation:
- Provide a clear, step-by-step derivation for your answer.
- This derivation must explicitly reference specific data points or phrases from the document
(e.g., “From Table 3, Row Ŕevenue,́ Year 2023...” or “As stated in paragraph 2...”).
- Detail all mathematical operations performed in each step. This helps verify the question’s
solvability and reasoning path.
## Output Format

29

Preprint under review

Your response must conclude with a JSON object containing the following keys: “question”
and “answer”, placed after your reasoning.
{
“question”: “<A well-structured English question that adheres to all design principles>”,
“answer”: “<A single, non-zero numerical answer>”
}
## Document Context
<text>
{context}
</text>
## Previous Examples
### Example 1:
Question: {question 1}
Answer: {answer 1}
...
### Example K:
Question: {question k}
Answer: {answer k}

Questioner Prompt for Document Multiple-Choice Task with History Memory
You are an expert in document analysis. We are building a benchmark to evaluate the
capabilities of large language models (LLMs) on fact retrieval, reasoning across multiple
constraints, and accurate synthesis of information into coherent responses. You will be
provided with a document context and a set of simpler, existing questions. Your primary task
is to generate ONE new, highly challenging multiple-choice question with one correct answer
and three plausible, distinct distractors. The new question must be fundamentally different
and more complex than the provided examples, requiring both document comprehension
and multi-hop reasoning. You must provide one correct answer and three plausible, distinct
distractors. Crucially, you must also provide a detailed explanation for why the correct
answer is correct (including derivation steps) and why each distractor is incorrect.
## Principles for Question and Option Design
Adhere strictly to the following principles when crafting your question, answer, options, and
derivation:
1. General Requirements:
- All questions, options, and explanations must be in English.
- Questions should be challenging, requiring more than simple retrieval or a few inference
steps.
2. Cognitive Complexity Requirements for the Question:
- Must necessitate multi-step reasoning (e.g., involving three or more distinct logical or
calculation steps).
- Should require the integration of at least three distinct data points from different parts of the
document (e.g., combining data from a table with text, or using multiple rows/columns/cells).
- Should demand the synthesis of quantitative data with qualitative information found in the
text.
- The problem setup should have the potential for common misinterpretations, which will
inform distractor design.
3. Content Validity Criteria:
- The question and all options must be exclusively answerable using information from the
provided document. No external knowledge beyond common sense or very basic, universally
understood concepts (e.g., profit = revenue - cost, if directly applicable and data is provided)
should be required.
- If applicable to the document type (e.g., financial reports), prioritize questions with regulatory/compliance implications or those highlighting significant financial outcomes.
- Ensure numerical values involved in the question or options require contextual interpretation
within the document, not just direct look-up.

30

Preprint under review

- Avoid trivia; focus on questions that address material information or key insights derivable
from the document.
4. Distractor Development Guidelines:
- Each of the three distractors must be plausible yet clearly incorrect upon careful analysis.
- Distractors should represent distinct error paths or common misinterpretations.
- At least one distractor should represent a common conceptual misunderstanding related to
the document’s content or how information is presented.
5. Forbidden Question/Option Patterns:
- Simple Counting: Avoid questions like “How many X are there?” if X is easily countable or
directly stated. Small counts (<=5) as part of a larger calculation are acceptable.
- Direct Retrieval: Avoid questions where the answer (or its direct components) can be found
in a single, obvious location without further processing.
- Excessive External Knowledge: Questions must not require significant domain-specific
knowledge not provided or clearly inferable from the document.
- No Fabricated Information: Strictly adhere to document content. Do not introduce
hypothetical scenarios, data, or assumptions not explicitly stated or directly inferable.
- Ambiguous Scenarios: The question must have one unambiguously correct answer based
solely on the provided document.
- Vague Options: All options, including distractors, must be precise and unambiguous.
6. Escalate Question Difficulty: The new question must demonstrate a higher order of
reasoning than the Previous Examples. First, analyze the examples to identify their simple
reasoning patterns (e.g., fact retrieval, single-step comparison). Then, create a new question
that incorporates one or more of the following advanced reasoning types:
- Quantitative Reasoning & Calculation: Requires performing mathematical operations (e.g.,
addition, subtraction, percentage change, averaging) on data from multiple sources.
- Comparative & Superlative Analysis: Requires comparing multiple entities based on synthesized criteria to find the one that is highest, lowest, best, etc.
- Conditional or Causal Reasoning: Structured as an “if-then” scenario or asks for the
cause/effect of a situation by linking different documents (e.g., “What would be the total cost
if the discount from Document A were applied to the price listed in Document B?”).
- Synthesis Across Data Types: Forces connection between qualitative information (e.g., a
policy description) and quantitative data (e.g., a number in a table) to reach a conclusion.
7. Answer and Explanation Requirements:
- The correct answer must be <correct_answer>.
- A detailed derivation for the correct answer must be provided, showing step-by-step calculations and referencing specific parts of the document (e.g., “From Table X, Row Y...”, “As
stated in paragraph Z...”).
- For each distractor, provide a brief explanation of why it is incorrect, ideally linking it to
the type of error it represents (e.g., “Option A is incorrect because it omits the X deduction
mentioned in...”, “Option B results from incorrectly summing X and Y instead of finding their
difference...”).
## Output Format
Your response must conclude with a JSON object containing the following keys: “question”,
“options”, and “answer”, placed after your reasoning.
{
“question”: “<A well-structured multiple choice English question, exclude choices and answer>”,
“options”: {
“A”: “<Text for choice A>”,
“B”: “<Text for choice B>”,
“C”: “<Text for choice C>”,
“D”: “<Text for choice D>”
},
“answer”: “<correct_answer>”
}
## Document Context
<text>

31

Preprint under review

{context}
</text>
## Previous Examples
### Example 1:
Question: {question 1}
Answer: {answer 1}
...
### Example K:
Question: {question k}
Answer: {answer k}
Responder Prompt for Document General QA Task
Please read the following text and answer the question below.
<text>
{content}
</text>
Question: {question}
Format your answer as follows: “The correct answer is (insert answer here).”
Responder Prompt for Financial Math QA Task
You are an expert in document analysis and numeric reasoning, you are supposed to answer
the given question based on the provided context. You need to first think through the problem
step by step, documenting each necessary step. Then you are required to conclude your
response with the final answer in your last sentence as “Therefore, the answer is (insert answer
here)”. The final answer should be a numeric value.
<text>
{content}
</text>
Question: {question}
Please reason step by step, and format your answer as follows: “Therefore, the answer is
(insert answer here).”
Responder Prompt for Document Multiple-Choice Task
Please read the following text and answer the question below.
<text>
{content}
</text>
Question: What is the correct answer to this question: {question}
Choices:
(A) {choice_A}
(B) {choice_B}
(C) {choice_C}
(D) {choice_D}
Format your answer as follows: “The correct answer is (insert answer here).”
Verifier Prompt for Document QA Task
## TASK
You are an expert in verifying if two answers are the same. Your input is a problem and two
answers, Answer 1 and Answer 2. You need to check if they are equivalent. Your task is to
determine two answers are equivalent, without attempting to solve the original problem.
## Instruction
1. Carefully compare the Answer 1 and Answer 2.

32

Preprint under review

2. Compare the answers to verify they represent identical values or meaning, even when
written in different forms or notations.
3. For numerical answers, you should allow a ±0.15% tolerance.
4. Your decision must be one of the “[[YES]]” or “[[NO]]”.
## Input Data
- Problem: {problem}
- Answer 1: {answer_1}
- Answer 2: {answer_2}
## Output Format
Provide your final evaluation in the following format:
“Explanation:” Provide an explanation for why the answers are equivalent or not.
“Decision:” “[[YES]]” or “[[NO]]”
Please proceed with the evaluation.

33

