Here is the extracted information from the research paper:

**PAPER:** YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning
**TITLE:** YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning
**ARXIV_ID:** 2410.03997v2 [cs.MA]
**RESEARCH_METHOD:** 02_rlhf_alignment (RLHF Alignment)
**METHOD_DESCRIPTION:** YOLO-MARL is a novel framework that leverages the high-level planning capabilities of large language models (LLMs) to enhance multi-agent reinforcement learning (MARL) policy training. The framework integrates four key components: Strategy Generation, State Interpretation, Planning Function Generation, and MARL training process with the LLM generated Planning Function incorporated throughout. YOLO-MARL requires only one interaction with the LLM per environment, reducing computational overhead and mitigating instability issues associated with frequent LLM interactions during training.
**KEY_CONTRIBUTIONS:**
* YOLO-MARL synergizes the planning capabilities of LLMs with MARL to enhance policy learning in challenging cooperative game environments.
* YOLO-MARL requires minimal LLM involvement, significantly reducing computational overhead and mitigating communication connection instability concerns during training.
* The approach leverages zero-shot prompting and can be easily adapted to various game environments, with only basic prior knowledge required from users.
* YOLO-MARL outperforms traditional MARL algorithms in two different environments: Level-Based Foraging (LBF) and Multi-Agent Particle (MPE) environments.
