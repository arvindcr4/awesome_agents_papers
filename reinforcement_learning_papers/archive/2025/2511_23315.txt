arXiv:2511.23315v1 [cs.LG] 28 Nov 2025

Emergent Coordination and Phase Structure
in Independent Multi-Agent Reinforcement
Learning
Azusa Yamaguchi
School of Physics and Astronomy
University of Edinburgh
ayamaguc@staffmail.ed.ac.uk
December 1, 2025
Abstract
A clearer understanding of when coordination emerges, fluctuates, or
collapses in decentralized multi-agent reinforcement learning (MARL) is
increasingly sought in order to characterize the dynamics of multi-agent
learning systems. We revisit fully independent Q-learning (IQL) as a
minimal decentralized testbed and run large-scale experiments across environment size L and agent density ρ. We construct a phase map using
two axes—cooperative success rate (CSR) and a stability index derived
from TD-error variance—revealing three distinct regimes: a coordinated
and stable phase, a fragile transition region, and a jammed/disordered
phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent’s
effective transition kernel induced by others’ policy updates. Synchronization analysis further shows that temporal alignment is required for
sustained cooperation, and that drift–synchronization competition generates the fragile regime. Ablation without agent identifiers removes drift
entirely and collapses the three-phase structure, demonstrating that small
inter-agent asymmetries are a necessary driver of drift. Overall, our results show that decentralized MARL exhibits a coherent phase structure
governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution–interaction
driven phase phenomenon.

1

Introduction

A central aim of artificial intelligence is to develop agents that can perceive,
adapt, and interact with others in complex environments. Reinforcement learning (RL) provides a basic framework for learning through experience, with many
of its successes demonstrated in single-agent domains [1, 10].
1

In multi-agent reinforcement learning (MARL), however, several agents update their policies concurrently, altering each other’s transition dynamics and
creating an inherently non-stationary learning problem [7]. Such non-stationarity
is frequently associated with instability, divergence, and failures of coordination.
A number of influential approaches mitigate these challenges by introducing centralized critics or structural biases. CTDE methods such as MADDPG
[11] and COMA [5] stabilize learning through centralized information, while
value-decomposition approaches such as VDN [17] and QMIX [14] impose additive or monotonic structures to promote cooperation. These techniques are
highly effective but rely on explicit architectural assumptions that constrain the
joint value landscape [12, 15]. Consequently, they offer limited insight into how
coordination might emerge—or collapse—without centralized bias.
Emergent coordination has also been examined from perspectives including evolutionary games [19, 3], social dilemmas [9], and collective multi-agent
systems [8, 4, 16]. These studies suggest that MARL behaviors can resemble physical collective phenomena. However, because coordination is actively
enforced in these frameworks, they offer limited insight into how coordination
might arise—or fail spontaneously—in fully decentralized settings without centralized critics, structural assumptions, or explicit coordination mechanisms.
In this work, we revisit Independent Q-Learning (IQL), which provides a
clean testbed for examining emergent coordination because it operates without
imposing centralized inductive biases or decompositional structures.
Although IQL is often regarded as unstable or limited [18, 6], it is precisely its
lack of centralized critics, structural decomposition, or enforced cooperation that
makes it an ideal setting for studying spontaneous coordination. By scanning
a broad range of (L, ρ) conditions, we show that this minimal MARL system
exhibits a rich phase structure driven by distributional non-stationarity.
Our contributions are as follows:
1. A phase map of coordination and stability. Combining the coordination success rate (CSR) with a TD-error–variance stability index S, we
identify coordinated, fragile, and jammed/disordered regimes separated
by a double instability ridge in the (L, ρ) plane.
2. Kernel drift as a mechanism for MARL non-stationarity. Temporal analysis of TD-error variance and gradient-norm variance shows that
instability correlates strongly with kernel drift—the time-varying drift of
the effective transition kernel induced by others’ policy updates.
3. Synchronization as a requirement for sustained coordination.
Arrival-time spread and co-reach statistics reveal that temporal synchronization is necessary for maintaining coordination and that insufficient
synchronization characterizes the fragile regime.
4. Spontaneous coordination in decentralized MARL. Even without
centralized bias, certain scale–density combinations yield stable coordi2

nated behavior, suggesting that decentralized MARL can exhibit phasetransition-like phenomena.
Together, these results provide a unified perspective in which emergent coordination, fluctuation, and collapse arise from interactions among scale, density,
and kernel drift, forming a coherent phase structure.

2

Methods

This section describes the environment, learning setup, and evaluation metrics
used to analyze emergent coordination, non-stationarity, and kernel drift in fully
decentralized Independent Q-Learning (IQL).

2.1

Environment: Grid-Based Multi-Agent Navigation

We use an L × L grid world with L ∈ {8, 16, 24, 32}, containing N agents and a
single goal placed without overlap. Agent density is defined as
ρagents = N/L2 ,

ρ ∈ {0.03125, 0.0625, 0.125, 0.25, 0.5},

excluding (L = 32, ρ = 0.5) due to computational cost.
Agents choose from
A = {stay, up, down, left, right},
and receive −0.005 per step, +1 upon reaching the goal, and a hold action
thereafter which no longer affects the transition kernel.
Episodes terminate once the accumulated reward reaches the target score
0.8N , or when the maximum horizon of 8L steps is reached. Each condition
is trained for up to 1500 episodes. A single goal ensures that changes in (L, ρ)
naturally induce sparsity and congestion.

2.2

Learning Algorithm: Parameter-Shared Double DQN

All agents share parameters and learn via Double DQN. Given online parameters
θ and target parameters θ− , the TD target is
a′ = arg max Qθ (s̃, a),
a

y = r + γ(1 − d) Qθ− (s̃, a′ ).

We optimize the Huber loss
L(θ) = E[ℓHuber (y − Qθ (s, a))],
and update the target network via Polyak averaging:
θ− ← (1 − τ )θ− + τ θ.
Exploration follows exponentially decaying ϵ-greedy, and evaluation uses ϵ =
0. Parameter sharing improves sample efficiency while retaining decentralized
execution.
3

2.3

Network, Optimization, and Replay Buffer

We use a two-layer MLP (128 units each, ReLU), gradient clipping, and the
Adam optimizer (PyTorch defaults β1 = 0.9, β2 = 0.999). Full hyperparameters
appear in Appendix 1.
A shared replay buffer D stores 105 transitions; training updates start once
it contains at least 1,500 transitions, with a mini-batch size of 64. To permit spontaneous role differentiation despite parameter sharing, each transition
augments observations with a one-hot agent identifier:
(i)

oID = [ o(i) ∥ IDi ].

2.4

Phase Structure: Coordination and Drift Metrics

We characterize each (L, ρ) condition using two axes:
• Cooperative Success Rate (CSR),
• Stability Index S based on TD-error variance.
2.4.1

Cooperative Success Rate (CSR)

During evaluation episodes Keval (with ϵ = 0),
CSR(L, ρ) =

2.4.2

1
Keval

K
eval
X

1{all agents reached}.

k=1

Stability Index S: TD-Error Variance as Kernel Drift Proxy

Non-stationarity arises from temporal drift in the effective transition kernel Pit
caused by policy updates of other agents. We estimate this effect using the
per-episode variance of the TD error
δt = yt − Q(st , at ),
denoted v(L, ρ). Normalizing by the maximum variance across all conditions,
S(L, ρ) = 1 −

v(L, ρ)
,
vmax

where high S indicates weak drift (stable) and low S indicates strong drift
(unstable). Gradient-norm variance is analyzed in the Appendix.
2.4.3

Reference Point and Phase Distance

Thresholds are defined as the 60th percentiles of CSR and S:
τCSR = Perc60 (CSR),

4

τS = Perc60 (S).

The phase distance is
dphase (L, ρ) =

p

(CSR − τCSR )2 + (S − τS )2 .

(1)

This yields three regimes:
• Stable Coordinated Phase: high CSR, high S; kernel drift suppressed.
• Fragile Transitional Region: near the minimum of dphase ; coordination
fluctuates.
• Jammed / Disordered Phase: low CSR, low S; congestion-induced
stagnation.

2.5

Experimental Setup

2.5.1

Training

• 1,500 episodes,
• Maximum steps = 8L,
• 50 random seeds (mean and 95% confidence interval),
• Learning rate 1.5 × 10−4 , discount γ = 0.95, Polyak τ = 10−3 .
2.5.2

Evaluation Conditions

CSR is computed during a separate evaluation phase using greedy policies (ϵ =
0). In contrast, the stability index S is derived from TD-error statistics recorded
during training, under the standard ϵ-greedy exploration schedule. These two
quantities are then combined to construct the phase maps.
2.5.3

Compute Resources

Experiments were conducted on the University of Edinburgh’s Eddie HPC cluster (Rocky Linux 9, Altair GridEngine).

3

Results

3.1

Global Structure of Coordination and Non-coordination:
CSR–Stability Phase Map

Fig. 1 shows CSR and the stability index S (TD-error–variance-based) for 19
conditions (L ∈ {8, 16, 24, 32} and ρ ∈ {0.03125, . . . , 0.5}), computed from the
last 25% of training episodes.
At small scales and low densities, CSR and S are simultaneously high, indicating that coordinated behavior can emerge even under fully decentralized
IQL.
5

Order (CSR)

0.5

S based with TD Error Variance

0.5
0.9231

0.9231

0.8205

0.4

0.8205

0.4

CSR

0.5128
0.4103

0.2

10

15

25

30

0.3077

0.1

0.1026
20
L (grid size)

0.4103

0.2

0.0000

10

15

20
L (grid size)

25

0.2051
0.1026

0.4

0.10.0
08

0.5128

0.8

4

0.20

0.6154

0.3

0.6

0.2051

0.0

0.1

0.3077

TD Error Variance

0.3

0.7179

0.2

0.6154

agent density

agent density

0.7179

30

0.0000

Figure 1: Cooperative success rate (CSR, left) and stability index S (right) for
all (L, ρ) conditions, computed from the last 25% of episodes. Coordination
and stability occur only at small scales and low densities, while both metrics
collapse sharply as scale or density increases. The low-S region marks strong
non-stationarity and forms an Instability Ridge.
As density increases, CSR drops sharply toward zero across all scales, while
TD-error variance increases. This pattern suggests that congestion-induced exploration difficulty amplifies kernel drift, making the policy updates increasingly
unstable.
At fixed density, increasing scale also degrades both CSR and S monotonically, revealing that coordination becomes progressively fragile as exploration
cost and agent interactions intensify. Taken together, the CSR–S axes provide a clear separation between coordinated, partially coordinated, and noncoordinated regimes, and illustrate that coordination in independent MARL is
strongly dependent on environmental scale and density, collapsing rapidly with
growing non-stationarity.

3.2

Phase Geometry via Distance from the Coordinated
Attractor

Fig. 2 visualizes the normalized distance dphase from Eq. 1. A notable feature
is the presence of two contour lines around dphase ≈ 0.4, forming a double
Instability Ridge that separates coordinated and non-coordinated behavior.
This structure partitions the (L, ρ) space into three regimes:
• Coordinated & Stable Phase (dphase > 0.4 in the low-L, low-ρ region
): High CSR and high S, with weak drift and stable synchronization.
Convergence toward a coordinated attractor is consistently observed.
• Fragile Region (dphase < 0.4,between the two ridges): Synchronization
and collapse alternate, producing non-monotonic and transitional dynamics. Kernel drift competes with synchronization, yielding only temporary

6

Distance to coordinated & stable phase
0.5

0.9231
0.8205
0.7179
0.6154

0.3

1 dphase

agent density

0.4

0.5128
0.4103

0.2

0.3077
0.1
0.8

0.6

10

0.6

0.4

0.2

0.4

15

0.2051
0.1026

20
L (grid size)

25

30

0.0000

Figure 2: Phase geometry based on the normalized distance dphase . Two contour
lines near dphase ≈ 0.4 form a double Instability Ridge. The low-L, low-ρ region
corresponds to coordinated and stable behavior, the region between the ridges to
a fragile transitional regime, and larger scales to jammed/disordered outcomes.
or partial coordination.
• Jammed/Disordered Phase (dphase > 0.4 at larger L): CSR is near
zero and S remains low. Although kernel drift weakens, gradient noise
later dominates, leading to irreversible jammed/disordered behavior.
This three-part geometry indicates that IQL exhibits bifurcating dynamics
between a coordinated attractor and drift-driven instability, resembling a phase
transition.

3.3

Synchronization and Partial Coordination Near the
Instability Ridge

Fig. 3 compares synchronization (arrival-time spread) and coordination (coreach) dynamics for two ridge-adjacent conditions.
In the pre-ridge case, drift remains weak, spread collapses quickly, and coreach increases smoothly—reflecting stable convergence to a coordinated attractor.
On the ridge, spread and co-reach repeatedly undergo collapse–recovery cycles. This oscillatory pattern reflects competition between synchronization and
7

Synchronization and coordination emergence

L = 24, = 0.031

Arrival spread (train rolling mean)
Co-reach rate (eval)

1.0

Arrival-time spread
(max min arrival per episode)

0.8

80

0.6

60

0.4
40

Synchronization and coordination emergence
Arrival spread (train rolling mean)
Co-reach rate (eval)

140

0.2

1.0

120

0.8

100

0.6

All-agents reach rate (eval)

L = 16, = 0.062

All-agents reach rate (eval)

Arrival-time spread
(max min arrival per episode)

100

80

0.4

60

0.2
40

20
0.0
0

50

100

150

200

250

Training episode

300

350

20

400

(a) L = 16, ρ = 0.0625 (pre-ridge)

0.0
0

100

200

300

400

Training episode

500

600

700

(b) L = 24, ρ = 0.03125 (on the ridge)

Figure 3: Temporal profiles of arrival-time spread (synchronization) and coreach rate for two representative conditions near the Instability Ridge. Preridge (L=16, ρ=0.0625) shows rapid convergence toward coordination, whereas
on-ridge (L=24, ρ=0.03125) exhibits alternating collapse–recovery cycles.
kernel drift, giving rise to fragile coordination that does not persist.
These observations support the view that synchronization is a necessary
condition for coordination, and that drift near the ridge disrupts this mechanism. (High-density cases transition into jammed or oscillatory behavior; see
Appendix Fig. 9.)

3.4

Kernel Drift vs. Gradient Noise: Two Forms of Instability

Fig. 4 shows that the dominant source of instability changes with density:
• Near-ridge (ρ = 0.0625): TD variance grows throughout training, and
gradient variance increases later. Drift does not settle, preventing convergence to a coordinated fixed point.
• On-ridge (ρ = 0.03125): TD variance remains high, and gradient variance increases gradually, producing fragile oscillatory dynamics.
• Outside-ridge (ρ = 0.125): Both variances saturate early, reflecting
premature lock-in to incomplete patterns rather than coordination.
• High densities (ρ ≥ 0.25): Drift weakens but gradient variance diverges
in late training, causing loss of update coherence and jammed/disordered
outcomes.
These trends indicate that the Instability Ridge corresponds to a transition
from drift-dominated to gradient-noise–dominated instability, offering a unified perspective on coordination, partial coordination, and collapse. Complete
time-series statistics of TD error, gradient norms, and their variances across
all conditions are provided in Appendix Fig. 10, with further discussion in Appendix A.3.
8

Var[TD error]

×10 5

Kernel drift and policy instability across densities and scales

8
6
4
2

Var[

]

0.0003
0.0002
0.0001
0.0000

0

100

= 0.03125, L = 24
= 0.06250, L = 24

200

Episode

= 0.12500, L = 24

300

= 0.25000, L = 24

400

500

= 0.50000, L = 24

Figure 4: TD-error variance (top) and gradient-norm variance (bottom) for
L = 24 across densities. The dominant source of instability differs by density: persistent variance growth near the Ridge, oscillatory behavior on the
Ridge, early saturation outside the Ridge, and late-stage divergence at high
densities.These four densities were selected because they represent pre-ridge,
on-ridge, post-ridge, and high-density regimes, respectively.

3.5

Macroscopic Statistics of the Coordination Transition:
Synchronization and Density Thresholds

Fig. 5a shows that only conditions with small spread achieve high co-reach.
The absence of points with large spread and high co-reach indicates that nonsynchronized coordination is not observed. Large fluctuations near the ridge
again reflect drift–synchronization competition.
Fig. 5b shows the effective coordinated throughput ρeff = ρagents ·CSR, which
reveals scale-dependent critical densities ρcrit (L). At medium and large scales,
throughput peaks at intermediate densities and drops sharply at high densities.
This behavior mirrors the synchronization and coordination collapse observed
in Fig. 3, appearing as a macroscopic phase transition.

9

Density (color)

0.8

0.6

0.35

Grid size L (marker)
L=8
L = 16
L = 24
L = 32

= 0.031
= 0.062
= 0.125
= 0.250
= 0.500

Effective coordinated throughput per area
Density (color)
= 0.031
= 0.062
= 0.125
= 0.250
= 0.500

0.30
0.25
eff = agents CSR

Team co-reach rate (higher = better coordination)

Coordination phase plot

0.4

Grid size L (marker)
L=8
L = 16
L = 24
L = 32

0.20
0.15
0.10

0.2

0.05

0.0
0

50

100

150

Arrival-time spread (lower = more synchronized)

0.00

200

0.1

(a)

0.2

0.3

agents = Nagents/L 2

0.4

0.5

(b)

Figure 5: (Left) Relationship between arrival-time spread and co-reach across all
conditions. High co-reach occurs only when spread is small; large fluctuations
appear near the Instability Ridge. (Right) Effective coordinated throughput
ρeff = ρagents CSR, showing scale-dependent critical densities where throughput
peaks and then declines.

4

Discussion

These findings position kernel drift not merely as a correlate of instability, but as
a mechanistic driver whose growth rate predicts the onset of fragile and jammed
regimes.
Non-stationarity is a central challenge in multi-agent reinforcement learning (MARL), arising from the fact that each agent’s policy update modifies
the effective transition dynamics faced by all others. Prior work has examined
how replay-induced drift [6], non-convex–non-concave game structures [20], and
regularization or learning-rate control [13] contribute to instability. This interpretation complements prior taxonomies of non-stationarity Hernandez-Leal
et al. (2019), but identifies kernel drift as a unifying mechanism driving the
observed phase structure.
Our results suggest that these diverse forms of non-stationarity can be viewed
through a single unifying mechanism: kernel drift, the distributional drift in each
agent’s effective transition kernel. For agent i, the kernel
X
Y
Pit (s′ |s, ai ) =
P (s′ |s, ai , a−i )
πjt (aj |s)
(2)
a−i

j̸=i

fluctuates as other agents update, producing a drift term ∆Pit . These fluctuations amplify the variance of TD targets, create a mismatch between replayed and current dynamics, and push policy updates away from stable fixed
points. This mechanism is qualitatively consistent with observations in meanfield MARL, replicator dynamics, and actor–critic oscillations.
The resulting phase diagram reveals three regimes shaped by the interaction
between kernel drift and gradient noise:

10

• Stable coordinated phase: At low density and small scale, kernel drift
is weak, synchronization holds, and learning consistently converges toward
a coordinated attractor.
• Fragile transitional phase: Near the Instability Ridge, kernel drift
grows critically and competes with synchronization, producing alternating
collapse–recovery cycles and highly non-monotonic dynamics.
• Failure phase (jammed/disordered): Outside the ridge and at higher
densities, kernel drift weakens but gradient noise becomes dominant, leading to early lock-in or jammed/disordered behavior from which coordination does not recover.
Notably, with a single shared goal, increasing the scale L reduces collisioninduced asymmetries, weakening kernel-drift sources and causing the dynamics
at L = 24 and L = 32 to converge toward similar regimes—an observation that
further motivates the ID-removal ablation examined below.
The ablation in Appendix A.4 further clarifies the mechanism: removing
the agent ID restores full input symmetry, yielding identical transition kernels
t
≡ 0. Kernel drift is therefore canceled over time. TDfor all agents, ∆Pbrk
error and gradient-norm variances remain low and flat, CSR becomes densityindependent, and neither coordinated nor transitional phases emerge. This
provides direct evidence that the observed phase structure is not an artifact
of algorithmic bias, but rather arises from small symmetry-breaking differences
that accumulate through distributional interaction.
These observations align with prior work on role emergence or symmetry
breaking in MARL [2], evolutionary games [19, 3], and collective robotics [4].
The phase-transition–like structure observed here appears to be a spontaneous
phenomenon rooted in distributional interaction rather than an explicit coordination mechanism.
Overall, kernel-drift dynamics highlight how MARL learning trajectories
may be understood as collective processes shaped by distribution-level feedback. A rigorous theoretical account remains open, but our empirical findings
point toward promising connections with distributional or mean-field stability
analyses.

5

Conclusion

This work examined how independent multi-agent reinforcement learning (IQL),
despite having no centralized critic or structural coordination bias, exhibits systematic patterns of emergent coordination, fragility, and failure across environment scale L and agent density ρ. By combining cooperative success (CSR) with
a stability index based on TD-error variance, we constructed phase diagrams
that reveal three regimes:
• Coordinated & stable: synchronization holds and learning converges
toward a coordinated attractor.
11

• Fragile coordination: kernel drift competes with synchronization, producing collapse–recovery oscillations.
• Failure / jammed–disordered: kernel drift weakens but gradient noise
dominates, preventing recovery of coordination.
The Instability Ridge emerges as a sharp transition boundary where kernel drift amplifies and the learning dynamics switch from drift-dominated to
gradient-noise–dominated behavior. These observations indicate that MARL
learning trajectories may exhibit phase-transition–like structure shaped by distributional interaction.
Ablation experiments further showed that removing agent IDs eliminates the
entire phase structure: TD-error variance stays uniformly low, CSR becomes
flat across densities, and neither coordination nor fragility nor collapse emerges.
This supports the interpretation that the observed dynamics arise from spontaneous symmetry breaking and distributional interaction, rather than from
explicit coordination mechanisms.
Taken together, our results suggest that coordination in MARL may be understood as a spontaneous, distribution-driven phenomenon governed by physicalstyle parameters such as scale, density, kernel drift, and gradient noise. Treating
kernel drift as a distributional fluctuation in effective transition dynamics provides a unifying view on non-stationarity and offers a foundation for future
stability analyses based on distributional or mean-field models.

Implications for Multi-Agent Economics and Financial Markets
The structure observed here—emergence, fluctuation, and breakdown of coordination driven by distributional interactions—has natural parallels in multi-agent
economics and market microstructure. Many-agent systems in which actions
modify the distributional environment that, in turn, shapes optimal behavior
share the same feedback structure as MARL.
The phenomena observed in this work— emergence of coordination (price
stability), transient or partial synchronization, critical-density collapse, and
drift-induced deviation from equilibrium— suggest that scale, density, and distributional drift may provide useful explanatory principles for when markets
stabilize and when they destabilize.
These connections point toward future work on the stability of many-agent
financial models, critical phenomena in order-book dynamics, and MARL-based
market simulations. Understanding coordination and its breakdown through the
lens of phase geometry may offer a promising direction for integrating ideas from
AI, economics, and statistical physics.

12

6

Acknowledgments

A.Y. received support from a sponsored research agreement with Intel Corporation, which provided partial funding for this research.

References
[1]

Kai Arulkumaran et al. “Deep Reinforcement Learning: A Brief Survey”.
In: IEEE Signal Processing Magazine 34.6 (2017), pp. 26–38.

[2]

Bowen Baker et al. Emergent Tool Use From Multi-Agent Autocurricula.
2020. arXiv: 1909.07528 [cs.LG].

[3]

Daan Bloembergen et al. “Evolutionary dynamics of multi-agent learning:
a survey”. In: J. Artif. Int. Res. 53.1 (May 2015), pp. 659–697.

[4]

Dmitry Bratsun and Kirill Kostarev. “Phase Transition in a Dense Swarm
of Self-Propelled Bots”. In: Fluid Dynamics & Materials Processing 20.8
(2024), pp. 1785–1798.

[5]

Jakob Foerster et al. “Counterfactual Multi-Agent Policy Gradients”. In:
AAAI Conference on Artificial Intelligence. 2018.

[6]

Jakob Foerster et al. Stabilising Experience Replay for Deep Multi-Agent
Reinforcement Learning. 2018. arXiv: 1702.08887 [[cs.AI](http://cs.ai/)].

[7]

Pablo Hernandez-Leal et al. A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity. 2019. arXiv: 1707.09183.

[8]

Jason Hindes et al. “Critical transition for colliding swarms”. In: Physical
Review E 103.6 (June 2021).

[9]

Joel Z. Leibo et al. Multi-agent Reinforcement Learning in Sequential Social Dilemmas. 2017. arXiv: 1702.03037 [cs.MA].

[10]

Yuxi Li. “Deep Reinforcement Learning: An Overview”. In: arXiv preprint
arXiv:1701.07274 (2017).

[11]

Ryan Lowe et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive
Environments”. In: Advances in Neural Information Processing Systems
(NeurIPS). 2017.

[12]

Anuj Mahajan et al. MAVEN: Multi-Agent Variational Exploration. 2020.
arXiv: 1910.07483 [cs.LG].

[13]

Georgios Papoudakis et al. Dealing with Non-Stationarity in Multi-Agent
Deep Reinforcement Learning. 2019. arXiv: 1906.04737 [cs.LG].

[14]

Tabish Rashid et al. QMIX: Monotonic Value Function Factorisation
for Deep Multi-Agent Reinforcement Learning. 2018. arXiv: 1803.11485
[cs.LG].

[15]

Tabish Rashid et al. Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. 2020.
arXiv: 2006.10800 [cs.LG].
13

[16]

Michael Rubenstein, Alejandro Cornejo, and Radhika Nagpal. “Programmable
self-assembly in a thousand-robot swarm”. In: Science 345.6198 (2014),
pp. 795–799.

[17]

Peter Sunehag et al. Value-Decomposition Networks For Cooperative MultiAgent Learning. 2017. arXiv: 1706.05296 [[cs.AI](http://cs.ai/)].

[18]

Ming Tan. “Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents”. In: Proceedings of the Tenth International Conference
on Machine Learning (ICML 1993). Morgan Kaufmann, 1993, pp. 330–
337.

[19]

K.P. Tuyls and A. Nowé. “Evolutionary Game Theory and Multi-Agent
Reinforcement Learning”. English. In: Knowledge Engineering Review 20(01)
(Jan. 2005), pp. 63–90.

[20]

Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms. 2021.
arXiv: 1911.10635 [cs.LG].

A

Appendix

This appendix provides additional analyses and ablation studies that complement the main text. In particular, we present: (i) full hyperparameter specifications, (ii) extended phase maps, (iii) additional examples of synchronization–
coordination dynamics, (iv) scale-dependent properties of kernel-drift indicators, and (v) ablations in which agent identifiers are removed to examine the
collapse of the phase structure.

A.1

Hyperparameter Details

A.2

Stability Index Based on Gradient-Norm Variance
and the Corresponding Phase Maps

In addition to the TD-error–variance stability index STD , we also construct a
gradient-norm–based stability index S∇ and visualize the corresponding phase
maps in Figs. 6 and 7.
At the global level, S∇ recovers the same broad structure as STD : a coordinated and stable phase at small scale and low density, and a jammed/disordered
phase at high density. However, the phase boundaries become noticeably less
sharp under S∇ . In particular, at L = 24 and ρ = 0.25, S∇ exhibits a pronounced local drop (Fig. 8a), whereas STD shows a clear peak around ρ ≈ 0.06–
0.08 (Fig. 8b), precisely identifying the Instability Ridge.
This discrepancy arises because gradient-norm variance is sensitive not only
to kernel drift—the source of distributional non-stationarity—but also to curvature of the local loss landscape and optimizer momentum. For instance, at
(L, ρ) = (24, 0.25), CSR and arrival-time spread remain in a metastable semi-

14

Table 1: Hyperparameters used in all experiments.
Item

Symbol

Value / Setting

Number of episodes (max)
Max steps per episode
Number of random seeds per condition

–
Tmax
–

1500
8L
50

Learning rate
Discount factor
Polyak coefficient (soft target update)
Optimizer

α
γ
τ
–

Batch size
Replay buffer capacity
Warm-up transitions

B
|D|
–

1.5 × 10−4
0.95
10−3
Adam (β1 = 0.9, β2 =
0.999)*
64
105
1500

Initial exploration rate
Minimum exploration rate
Exploration decay (per episode, exponential)
Exploration during evaluation

ϵstart
ϵmin
λ
ϵeval

1.0
0.01
0.98
0 (greedy)

Network architecture

–

Activation function
Gradient clipping
Loss function

–
–
–

Input–FC(128,
ReLU)–FC(128,
ReLU)–Output
ReLU
∥∇∥2 ≤ 1.0
Huber loss (smooth L1)

* PyTorch’s default hyperparameters for Adam were used.

jammed configuration, yet gradient norms fluctuate due to fine-scale pattern
adjustments, which blurs the phase boundary at the map level.
Taken together, these observations indicate that STD is the more reliable
primary indicator for detecting the growth of kernel drift and for identifying the
Instability Ridge. We therefore present the S∇ –based maps only as supplementary analysis.

A.3

Kernel Drift Dynamics Across Scales

Fig. 10 reports the TD error, gradient norm, and their variances across all (L, ρ)
conditions (50 seeds; 95The main trends are as follows.
1. TD-error mean Across all settings the mean TD error quickly approaches
zero, indicating that the Bellman update itself stabilizes early.
2. TD-error variance Var[TD] depends primarily on scale L, not density.
For small grids (L = 8, 16) the variance is larger and more dynamic,
whereas for large grids (L = 24, 32) it remains small and often nearly
flat—especially in jammed/disordered regimes such as (L = 24, ρ > 0.125)
and (L = 32, ρ > 0.0625).
15

3. Gradient norm and its variance Gradient statistics also exhibit strong scale
dependence. Large grids show late-episode divergence at specific densities
(e.g., L = 24, ρ = 0.25 and L = 32, ρ = 0.125), indicating gradientnoise–dominated instability even when drift is weak.
4. Density effects Density modulates the timing and magnitude of fluctuations but does not produce a consistent monotonic trend across scales.
Scale L and drift–synchronization competition near the Instability Ridge
(discussed in Fig. 4) dominate the global behavior.
5. Overall structure These patterns complement the phase diagram in Fig. 2,
showing that:
• drift suppression yields stable coordination,
• drift–synchronization competition produces fragile dynamics, and
• weak drift with strong gradient noise leads to jammed/disordered
outcomes.

A.4

Ablation: Removing Agent ID and Symmetry Breaking

To isolate the role of symmetry breaking in the emergence of kernel drift and the
phase structure reported in the main text, we conduct ablations in which agent
identifiers (IDs) are removed. Without IDs, all agents share identical observation and policy spaces; hence their policy mappings become fully symmetric.
Following the definition of the effective transition kernel in Eq. 2, removing
agent IDs makes all agents share identical observation and policy spaces. As a
result, each agent i satisfies
Pit (s′ |s, ai ) ≈ P̄ ,

∆Pit := Pit − P̄ ≈ 0,

t
and the asymmetry-driven component ∆Pbrk
in the decomposition ∆Pit =
t
t
∆Psym + ∆Pbrk vanishes. Kernel drift therefore cannot accumulate.

Learning dynamics without IDs. The TD-error mean decreases rapidly at
the beginning, similar to the ID-present case, but then exhibits a secondary rise
followed by a slow, noise-like decay. The TD-error variance shows an early peak
comparable to the ID-present setting, yet does not settle; instead, it undergoes
repeated small-amplitude fluctuations. These patterns indicate that distributional drift is suppressed, and that the remaining non-stationarity is dominated
by weak exploration noise arising from policy symmetry.
Gradient-norm statistics. Removing IDs produces a sharper distinction:
gradient magnitudes become substantially larger—often several times those observed with IDs—and the corresponding variance rises steeply before decaying
slowly. When all agents share an identical policy, update directions lose diversity, so even small perturbations are coherently amplified, creating persistent
and exaggerated update noise.
16

Order (CSR)

0.5

S based with TD Error

0.5
0.9231

0.9231

0.8205

0.4

0.8205

0.4

0.7179

0.3

CSR

0.5128
0.4103

0.2

0.2051

4
0.0

0.20

0.10.0
08

10

15

25

30

0.5128
0.4103

0.2

0.6
0.8

0.3077
0.4

0.1

0.1026
20
L (grid size)

0.6154

0.2

0.2

0.1

0.3077

0.3

0.4

TD Erroe

0.6154

agent density

agent density

0.7179

0.0000

10

15

20
L (grid size)

25

30

0.2051
0.1026
0.0000

Figure 6: Phase map using the gradient-norm–based stability index S∇ . Left:
CSR. Right: S∇ . Global trends resemble the TD-based map, but ridge boundaries become noticeably less distinct due to optimizer- and curvature-induced
fluctuations.
Consequences for phase behavior. Overall, removing IDs yields a characteristic structure in which (i) TD-error variance remains suppressed, confirming
the disappearance of kernel drift, while (ii) enlarged gradient norms and heightened gradient-norm variance show that update noise becomes the dominant
driver of instability. This produces superficially similar “non-coordination” outcomes to the jammed/disordered regime observed with IDs, but the underlying
mechanism is fundamentally different: the instability arises not from drift, but
from homogeneous, noise-amplifying updates due to complete policy symmetry.
These findings demonstrate that the phase transitions in the main text are
driven by
t
kernel drift ∝ ∆Pbrk
,
i.e., by small but persistent inter-agent asymmetries. Removing such asymmetries suppresses kernel drift and eliminates both coordinated and fragile regimes.
This result is consistent with prior work on symmetry breaking in learning dynamics [19, 3], role emergence in MARL [2], and self-organized differentiation
in collective robotic systems [4]. It supports the interpretation that the coordination transition observed in decentralized MARL arises as a spontaneous
distribution–interaction phenomenon rather than an architectural artifact.
This appendix is self-contained and provides supplementary analyses and
ablations that support and extend the results presented in the main text.

17

Distance to coordinated & stable phase
0.5

0.9231
0.8205
0.7179
0.6154

0.6

0.3

1 dphase

agent density

0.4

0.5128
0.4103

0.2

0.3077

0.4

0.2051

0.1

10

0.4

15

20
L (grid size)

0.6

0.6

0.2

0.8

25

30

0.1026
0.0000

Figure 7: Phase-distance map constructed from the gradient-norm–based stability index. A ridge structure remains visible, but is substantially blurred
compared to the TD-based counterpart.

B

Appendix Figures

B.1

Phase Maps Based on Gradient-Norm Variance

B.2

Stability Index Slices at Fixed Scale

B.3

Synchronization and Coordination Dynamics

B.4

Kernel Drift and Gradient-Noise Regimes

———————————————————-

B.5

Effect of Removing Agent IDs

18

Stability slice at L = 24withTv
1.0

0.8

0.08

0.9

0.06

0.6

0.04

0.4
Stability S
S = 0.87
CSR
CSR = 0.07

0.2

0.1

0.2
0.3
agent density

0.4

0.10

Stability S
S = 0.41
CSR
CSR = 0.07

0.08

0.8

0.06

0.7

CSR

Stability S with Tv

0.10

CSR

Stability S with Gv

Stability slice at L = 24withGv
1.0

0.04

0.6

0.02

0.5

0.00

0.4

0.02
0.00

0.5

0.1

(a) S∇ at L = 24.

0.2
0.3
agent density

0.4

0.5

(b) STD at L = 24.

Figure 8: Stability-index slices at L = 24. Left: S∇ shows a local collapse at
ρ = 0.25, which obscures the phase boundary. Right: STD exhibits a distinct
peak near ρ ≈ 0.06–0.08, sharply locating the Instability Ridge.
Arrival spread (train rolling mean)
Co-reach rate (eval)

17.5

1.0

Arrival-time spread
(max min arrival per episode)

0.8

15.0

0.6

12.5
10.0

0.4

7.5

0.2

5.0
2.5

Arrival spread (train rolling mean)
Co-reach rate (eval)

50

100

150

200

Training episode

250

300

0.6

80

0.4

60

0.2

40

0.0
0

350

(a) L = 8, ρ = 0.03125

100

150

Training episode

200

250

300

(b) L = 16, ρ = 0.50

Synchronization and coordination emergence

L = 32, = 0.031

Arrival spread (train rolling mean)
Co-reach rate (eval)

180

50

160

Arrival-time spread
(max min arrival per episode)

All-agents reach rate (eval)

0.6
120

Arrival spread (train rolling mean)
Co-reach rate (eval)

200
0.8

140

Synchronization and coordination emergence

225

1.0

100

0.4

80

0.2

1.0
0.8

175

All-agents reach rate (eval)

L = 24, = 0.250

1.0
0.8

100

0.0
0

Arrival-time spread
(max min arrival per episode)

Synchronization and coordination emergence

120

All-agents reach rate (eval)

Arrival-time spread
(max min arrival per episode)

L = 16, = 0.500

Synchronization and coordination emergence

All-agents reach rate (eval)

L = 8, = 0.031
20.0

150

0.6

125

0.4

100
75

0.2

50

60
0.0
0

200

400

600

800

Training episode

1000

1200

25

1400

(c) L = 24, ρ = 0.25

0.0
0

200

400

600

800

Training episode

1000

1200

1400

(d) L = 32, ρ = 0.03125

Figure 9: Time-series dynamics of arrival-time spread (synchronization) and
co-reach rate (coordination) across four representative conditions. (a) L =
8, ρ = 0.03125: rapid synchronization and stable coordinated behavior. (b)
L = 16, ρ = 0.50: approaching the Instability Ridge from the high-density side,
where synchronization is hindered and co-reach exhibits large fluctuations. (c)
L = 24, ρ = 0.25: a jammed/disordered regime outside the Ridge, with persistently large spread and low co-reach. (d) L = 32, ρ = 0.03125: approaching
the Ridge from the low-density, large-scale side, showing partial synchronization
with drift-induced fluctuations.

19

[TD error]

Var[TD error]

0.00016

0.0020
0.00014
0.00012

0.0015

0.00010
0.00008

0.0010

0.00006
0.0005

0.00004
0.00002

0.0000

0.00000

Episode

Episode

[| |]

Var[| |]
0.0014

0.06

0.0012

0.05

0.0010
0.04
0.0008
0.03

0.0006

0.02

0.0004

0.01

0.0002
0.0000

0.00
0

200

400

600

800

Episode

=0.03125, L=8
=0.03125, L=16
=0.03125, L=24
=0.03125, L=32

1000

1200

=0.06250, L=8
=0.06250, L=16
=0.06250, L=24

1400

0

=0.06250, L=32
=0.12500, L=8
=0.12500, L=16

=0.12500, L=24
=0.12500, L=32
=0.25000, L=8

200

400

=0.25000, L=16
=0.25000, L=24
=0.25000, L=32

600

800

Episode

1000

1200

1400

=0.50000, L=8
=0.50000, L=16
=0.50000, L=24

Figure 10: TD-error and gradient-norm statistics across all (L, ρ) conditions.
Both metrics exhibit strong scale dependence: larger grids (L = 24, 32) suppress
TD-error variance except near the Instability Ridge, whereas gradient-norm
variance shows late-episode divergence only in jammed/disordered regimes (e.g.,
L = 24, ρ = 0.25; L = 32, ρ = 0.125). These patterns reflect a transition
from drift-dominated to gradient-noise–dominated instability. Averaged over
50 seeds; 95% CIs shown.

20

[TD error]

Var[TD error]

0.00175

0.00016

0.00150

0.00014

0.00125

0.00012

0.00100

0.00010

0.00075

0.00008

0.00050

0.00006

0.00025

0.00004

0.00000

0.00002

0.00025

0.00000

Episode

Episode

[| |]

Var[| |]

0.200

0.010

0.175
0.008

0.150
0.125

0.006

0.100
0.004

0.075
0.050

0.002

0.025
0.000

0.000
0

200

400

600

800

Episode

=0.03125, L=8
=0.03125, L=16

1000

1200

=0.06250, L=8
=0.06250, L=16

1400

0

=0.12500, L=8
=0.12500, L=16

=0.25000, L=8
=0.25000, L=16

200

400

=0.50000, L=8

600

800

Episode

1000

1200

1400

=0.50000, L=16

Figure 11: Mean and variance of the TD error (top) and gradient norm (bottom) for all (L, ρ) conditions in the ID-removed setting. Removing agent
IDs suppresses the asymmetry-driven component of kernel drift, resulting in
low and non-accumulating TD-error variance. In contrast, gradient norms remain substantially larger and exhibit pronounced early-episode fluctuations,
reflecting update-noise amplification under complete policy symmetry. Curves
show averages over 25 seeds with 95% confidence intervals for L ∈ {8, 16} and
ρ ∈ {0.03125, . . . , 0.5}.

21

Stability slice at L = 16

Stability S

0.90

0.02

0.85

0.00

0.80
0.75

0.02

0.70

0.04

0.65

0.1

0.2
0.3
agent density

0.4

0.40

0.12

0.35

0.10
0.08

0.30

CSR

0.04

Stability S

0.95

Stability slice at L = 16

Stability S
S = 0.85
CSR
CSR = 0.00

CSR

1.00

0.06
0.04

0.25
Stability S
S = 0.41
CSR
CSR = 0.07

0.20

0.5

0.1

0.2
0.3
agent density

0.4

0.5

0.02
0.00

Figure 12: Left: Without agent IDs—CSR and the stability index S remain flat across densities; no coordinated, fragile, or jammed phases emerge.
Right: With IDs—the coordinated, fragile, and jammed regimes re-emerge,
confirming that symmetry breaking is required for sustaining kernel drift and
for producing the phase structure observed in the main text.

22

