SMAC-Hard: Enabling Mixed Opponent Strategy
Script and Self-play on SMAC

arXiv:2412.17707v2 [cs.AI] 24 Dec 2024

Yue Deng1 , Yan Yu2 , Weiyu Ma3 , Zirui Wang 1 , Wenhui Zhu 4 , Jian Zhao 4 , Yin Zhang 1
1
College of Computer Science and Technology, Zhejiang University
2
University of Science and Technology of China
3
Institute of Automation, Chinese Academy of Sciences
4
Polixir
devindeng@zju.edu.cn, yy1140730050@mail.ustc.edu.cn, maweiyu2022@ia.ac.cn
ziseoiwong@zju.edu.cn, wenhui.zhu@polixir.ai, jian.zhao@polixir.ai
zhangyin98@zju.edu.cn

Abstract
The availability of challenging simulation environments is pivotal for advancing
the field of Multi-Agent Reinforcement Learning (MARL). In cooperative MARL
settings, the StarCraft Multi-Agent Challenge (SMAC) has gained prominence
as a benchmark for algorithms following centralized training with decentralized
execution paradigm. However, with continual advancements in SMAC, many
algorithms now exhibit near-optimal performance, complicating the evaluation of
their true effectiveness. To alleviate this problem, in this work, we highlight a
critical issue: the default opponent policy in these environments lacks sufficient diversity, leading MARL algorithms to overfit and exploit unintended vulnerabilities
rather than learning robust strategies. To overcome these limitations, we propose
SMAC-HARD, a novel benchmark designed to enhance training robustness and
evaluation comprehensiveness. SMAC-HARD supports customizable opponent
strategies, randomization of adversarial policies, and interfaces for MARL selfplay, enabling agents to generalize to varying opponent behaviors and improve
model stability. Furthermore, we introduce a black-box testing framework wherein
agents are trained without exposure to the edited opponent scripts but are tested
against these scripts to evaluate the policy coverage and adaptability of MARL
algorithms. We conduct extensive evaluations of widely used and state-of-the-art
algorithms on SMAC-HARD, revealing the substantial challenges posed by edited
and mixed strategy opponents. Additionally, the black-box strategy tests illustrate
the difficulty of transferring learned policies to unseen adversaries. We envision
SMAC-HARD as a critical step toward benchmarking the next generation of MARL
algorithms, fostering progress in self-play methods for multi-agent systems. Our
code is available at https://github.com/devindeng94/smac-hard.

1

Introduction

Recent advances in Multi-agent reinforcement learning (MARL) have led to significant progress in a
wide range of applications such as autonomous vehicle teams (Cao et al., 2012) and sensor networks
(Zhang & Lesser, 2011). The domain of MARL has experienced rapid progress, significantly driven by
the advent of advanced simulation environments that serve as benchmarks to assess the effectiveness
of novel algorithms. These benchmarks play a pivotal role in bridging the gap between theoretical
developments and practical implementations by enabling the evaluation of MARL strategies in
scenarios that reflect real-world complexities. One of the most influential platforms in this space is
Preprint. Under review.

the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019a), which provides a suite of
micromanagement tasks set within the StarCraft II environment.
While SMAC initially gained widespread adoption as a benchmark, its limitations have become
apparent with the advancement of MARL techniques (Gorsane et al., 2022). Many algorithms now
attain near-optimal performance on SMAC‚Äôs pre-defined tasks (Hu et al., 2021; Yu et al., 2022a),
raising concerns about its ability to distinguish genuinely innovative approaches from those that
exploit specific task structures. This phenomenon of benchmark saturation highlights the need
for more intricate and diverse challenges to evaluate the robustness and generalizability of MARL
algorithms effectively.
To address these limitations, SMACv2 (Ellis et al., 2024a) was introduced, offering increased
complexity, scalability, and realism through procedurally generated scenarios. These scenarios require
agents to generalize to previously unseen conditions during testing, thereby discouraging overfitting
and encouraging the development of more adaptable algorithms. However, the added complexity of
SMACv2 has also introduced new difficulties, such as challenges in achieving convergence for many
MARL algorithms, suggesting that while it presents a tougher benchmark, it may currently exceed
the capabilities of existing methodologies (Singh et al., 2023).
Another critical issue with current benchmarks lies in the limited diversity of default opponent policies.
This constraint often results in overfitting, where MARL algorithms exploit specific weaknesses in
static opponents rather than learning robust strategies (Mitra, 2024). Consequently, these benchmarks
may fall short of preparing algorithms for the variability and unpredictability encountered in realworld applications. To overcome this, there is a pressing need for benchmarks that feature a broader
spectrum of opponent behaviors, promoting the creation of more resilient and adaptable algorithms.
Motivated by these considerations, we propose SMAC-HARD, an innovative benchmark tailored to
enhance the evaluation of MARL algorithms. SMAC-HARD introduces features such as opponent
strategy editing, random selection of opponent strategies, and self-play interfaces, requiring agents to
generalize across diverse opponent behaviors during training. Furthermore, it includes a black-box
testing framework that evaluates policy coverage by testing agents against previously unseen opponent
scripts. These enhancements provide a robust evaluation pipeline, facilitating the development of
MARL algorithms equipped to handle diverse and unpredictable environments with greater stability
and reliability.

2

Related Work

2.1

MARL Environments

The development of MARL is inseparable from a variety of multi-agent environments. (Peng et al.,
2021) adapted multi-agent systems to the MuJoCo environment by decomposing the robot‚Äôs joints
and controlling them collaboratively. MPE (Lowe et al., 2017) involves controlling the movement of
different particles in a 2D space to complete a series of tasks, which presents simple communication
in the cooperation and competition among agents. PettingZoo (Terry et al., 2021) is a library of
MARL environments, encompassing a variety of game types that allow for competitive, cooperative
and mixed agent relations. It includes classic scenarios such as Atari (Mnih et al., 2013), board
games, and particle control. In addition, many classic human games have also been applied in MARL
research. Overcooked-AI (Micah et al., 2019), based on the popular human game, expects agents
to learn task distribution and coordination to achieve high rewards in a cooking scenario. Google
Research Football (Kurach et al., 2020) offers a physics-based 3D soccer simulation, which presents
a challenging RL problem as soccer requires balancing short-term control and skill learning (such
as passing) with higher-level strategies. HOK (Wei et al., 2022) is a high-complexity Multiplayer
Online Battle Arena (MOBA) environment. Players compete by gathering resources while interfering
with their opponents to win the game. With multiple heroes and complex state and action spaces,
HOK provides an excellent environment for academic research on complex control problems. SMAC
(Samvelyan et al., 2019b) is one of the most popular environments for MARL. Based on the widely
known real-time strategy (RTS) game StarCraft II, SMAC scenarios are carefully designed and
require learning one or more micromanagement techniques to defeat enemies. Each scenario involves
a confrontation between two armies, with varying initial positions, unit types, and terrain features like
high ground or impassable obstacles. SMACv2 (Ellis et al., 2024b) addresses the issue of randomness
in SMAC by generating unit types for each agent with a fixed probability distribution, making the
2

environment more challenging. This significantly enhances the diversity and complexity of the
environment, providing a richer setting for contemporary
2.2

Algorithms for MARL

Value-based MARL algorithms have made significant progress in recent years. IQL (Tampuu et al.,
2017) treats other agents as part of the environment and trains independent Q-value networks for
agents. However, it may not always converge for the non-stationarity of the environment caused
by the changing policies of other agents. A series of methods (VDN (Sunehag et al., 2017), QMIX
(Rashid et al., 2020), QTRAN (Son et al., 2019), QPLEX(Wang et al., 2020)) decompose the global
Q-value function into individual Q-value functions to each agent. LDSA (Yang et al., 2022) proposes
an ability-based subtask selection strategy to assign agents to different subtasks reasonably and
dynamically group agents with similar abilities into the same subtask. Some other algorithms apply
TRPO (Schulman et al., 2015) or PPO (Schulman et al., 2017) to multi-agent problems. For instance,
IPPO (Witt et al., 2020) apply PPO and enforce parameter sharing under the assumption that all agents
have the same action space. MAPPO (Yu et al., 2022b) enhances IPPO by introducing a joint critic
function and improving the implementation techniques. HATRPO and HAPPO (Kuba et al., 2021)
extend the theory of trust region learning to cooperative MARL, which holds in general and does not
require any assumption that agents share parameters or the joint value function is decomposable.
2.3

LLM for Decision Making

The emergence of large language models (LLMs) has significantly advanced the research on decisionmaking for agents in complex environments. LLMs leverage massive human data to gain a deep
understanding of various complex scenarios, including robotic control, gaming, etc. In robotics,
(Liang et al., 2023) proposed "code as policy" for robotic control. In this approach, LLMs generate
Python code to process sensory outputs and parameterize control primitives. The Eureka algorithm
(Ma et al., 2023) utilizes LLMs for human-level reward design in reinforcement learning tasks, which
ables to generate reward functions that surpass those designed by experts. LLMs have also shown
exceptional performance across a wide range of game types, from classic board games to open-world
video games. ChessGPT (Feng et al., 2024) demonstrated the ability of LLM agents to understand
and strategize in strategic games. (Jin et al., 2024) and (Xu et al., 2023) has explored the social
deduction game such as Werewolf, which presented unique challenges in strategic communication
and decision-making. The MineDojo environment (Fan et al., 2022) facilitated projects like GITM
(Zhu et al., 2023) and Voyager (Wang et al., 2023) in Minecraft, demonstrating LLM agents‚Äô ability
to navigate and perform tasks in complex 3D environments. The Cradle framework (Tan et al.,
2024) introduced a novel approach allowing LLM agents to interact with various software and games
through a unified interface of screenshots and keyboard/mouse inputs. This demonstrated the potential
for general-purpose game-playing agents across multiple commercial video games and software
applications.

3

Background

3.1

MARL

A fully cooperative multi-agent task is described as a Dec-POMDP (Oliehoek & Amato, 2016)
task which consists of a tuple G = ‚ü®S, A, P, r, Z, O, N, Œ≥‚ü© in which s ‚àà S is the true state of
the environment in the centralized training phase and N is the number of agents. At each time
step, each agent i ‚àà N ‚â° {1, . . . , n} chooses an action ai ‚àà A which forms the joint action
a ‚àà A ‚â° AN . The transition on the environment is according to the state transition function that
P (¬∑|s, a) : S √ó A √ó S ‚Üí [0, 1]. The reward function, r(s, a) : S √ó A ‚Üí R, is shared among all the
agents, and Œ≥ ‚àà [0, 1) is the discount factor for future reward penalty. Partially observable scenarios
are considered in this paper that each agent draws individual observations z ‚àà Z of the environment
during the decentralized execution phase according to the observation functions O(s, i) : S √óN ‚Üí Z.
Meanwhile, the action-observation history, œÑi ‚àà T ‚â° (Z √ó A)‚àó , is preserved for each agent and
conditions the stochastic policy œÄi (ai |œÑi ) : T √ó A ‚Üí [0, 1].
Value-based MARL algorithm aims to find the optimal joint action-value function Q‚àó (s, a; Œ∏) =
r(s, a) + Œ≥Es‚Ä≤ [maxa‚Ä≤ Q‚àó (s‚Ä≤ , a‚Ä≤ ; Œ∏)] and parameters Œ∏ are learned by minimizing the expected TD
3

error. VDN learns a joint action-value function Qtot (œÑ, a) as the sum of individual value functions:
Pn
‚àÇQQMIX
tot (œÑ,a)
QVDN
tot (œÑ, a) =
i=1 Qi (œÑi , ai ). QMIX introduces a monotonic restriction ‚àÄi ‚àà N, ‚àÇQi (œÑi ,ai ) > 0
to the mixing network to meet the IGM assumption. In policy-based algorithms, agents use a policy
œÄŒ∏ (ai |œÑi ) parameterized by Œ∏ to produce an action ai P
from the local observation and jointly optimize
the discounted accumulated reward J(Œ∏) = Eat ,st [ t Œ≥ t r(st , at )] where at is the joint action at
time step t. In the AC-based algorithm, MAPPO algorithm, the actor is updated by optimizing the
t t
P
œÄŒ∏ (at |st )
|s )
t t
t t
target function JŒ∏k (Œ∏) = st ,at min( œÄœÄŒ∏k(a
(at |st ) AŒ∏ k (s , a ), clip( œÄŒ∏k (at |st ) , 1‚àíœµ, 1+œµ)AŒ∏ k (s , a )),
Œ∏
where the œµ is the clip parameter and AŒ∏k (st , at ) is the advantage function. The critic training is
similar to value-based Q learning by calculating TD-error and TD targets. During the TD training
process, the target value is calculated by bootstrapping from the existing Q-function according to
temporal difference methods or Monte Carlo returns.
3.2

SMAC

Instead of addressing the complexities of the full StarCraft II game, the StarCraft Multi-Agent
Challenge (SMAC) concentrates on micromanagement scenarios where each military unit is controlled
by an individual learning agent. During testing, units are restricted by a limited field-of-view and lack
explicit communication mechanisms. Featuring a diverse range of challenging setups, SMAC has
become a widely adopted benchmark in the MARL community for evaluating algorithms. It includes
23 micromanagement scenarios categorized into three types: symmetric, asymmetric, and microtrick. Symmetric scenarios involve equal numbers and types of units for both allies and enemies.
Asymmetric scenarios present additional units for the enemy, making them more challenging. Microtrick scenarios require specialized strategies, such as in 3s_vs_5z, where three allied stalkers must
kite five zealots, or in corridor, where six zealots must block a narrow passage to defeat 24 zerglings
without being overwhelmed.
SMACv2 builds upon the original SMAC by addressing its shortcomings and providing a more robust
framework for cooperative MARL evaluation. It emphasizes diversity, scalability, and realism in
multi-agent tasks while resolving the exploitable weaknesses of its predecessor. SMACv2 introduces
new maps with varied unit compositions, asymmetrical team setups, and heterogeneous agents,
offering more intricate and balanced challenges. Additionally, it incorporates stochastic elements in
agent behaviors and environmental dynamics, fostering the development of algorithms capable of
handling uncertainty. Empirical evaluations highlight SMACv2 as a significantly more demanding
benchmark, encouraging research into generalizable MARL approaches. By setting a higher standard,
SMACv2 promotes innovation and ensures advancements are relevant to real-world multi-agent
systems.

4

Limitation on Default Opponent Strategy

In this section, we examine how the variation in opponents‚Äô strategies affects the performance of two
widely used baseline algorithms, QMIX and MAPPO. Our analysis demonstrates that relying on a
single deterministic opponent strategy can cause the MARL training process to overfit to a particular
policy, leading to a reduction in the generalization capabilities of the trained models. Additionally,
when the opponent employs a single deterministic strategy that has certain weaknesses, agents may
exploit these vulnerabilities, resulting in suboptimal solutions.
4.1

Default Opponent Policy

In the StarCraft II (SC2Map) configuration, the default opponent policy is defined within the map
files rather than the Python environment of SMAC. These map files can be edited using the StarCraft
II Editor, which is officially provided by Blizzard. As illustrated in Figure 1, most map configurations
specify two spawning positions for both agents and their opponents. An internal script directs the
opponent units to move towards the agents‚Äô starting positions. The opponent units will automatically
target and attack the nearest agents within their sight range, based on a hate value. This causes the
opponent units to continuously adjust their targets.
Typically, MARL models direct agents toward the opponent‚Äôs side and execute micro-management
tasks based on observations. However, as depicted in Figure 2, there are situations where the opponent
4

(a)

(b)

Figure 1: (a) Agents spawn at the Team 1 point and the opponent units spawn at Team 2 point. (b)
The internal opponent script is defined in the SC2Map file. The opponent controls all the units to
attack toward the Team 1 point. Order all units in (Any units in (Entire map) owned by player 2
matching Excluded: Missile, Dead, Hidden, with at most Any Amount) to (Attack targeting Team
1)(Replace Existing Orders).
agents are not drawn to any hate values and end up becoming stuck at their designated destination
points. This flaw in the default opponent behavior presents an opportunity for the agents to exploit a
strategy of initially hiding from the opponent and then attacking each opponent unit individually. This
is particularly evident in the 3s5z_vs_3s6z map, where many MARL models exploit this vulnerability
to secure victories. Consequently, the default opponent policy is incomplete in terms of its policy
space, rendering it insufficient for adequately evaluating MARL algorithms.

(a)

(b)

Figure 2: (a) Three opponent Zealot units arrive at the agents‚Äô starting point and are stuck at that
point. (b) MARL algorithms easily achieve high performance when the tricky strategy is explored.
4.2

Performance on Mixing Policies

Recent advancements in Reinforcement Learning (RL) have resulted in substantial improvements
in tackling complex control systems, including applications in robotics and Atari games. However,
one challenge that persists in RL is the tendency for the models to overfit to the specific strategies
of the fixed opponent, limiting the transferability of the learned policies. As a result, RL models
face difficulties when adapting to new tasks or even when encountering different opponent strategies
within the same task.
To illustrate this phenomenon, we perform an experiment on the 10m_vs_11m scenario using two
distinct, slightly varied opponent strategies alongside a mixed strategy. We employ the QMIX and
MAPPO algorithms, utilizing the default hyperparameter settings outlined in the Appendix, and run
the simulation for 2 million time steps. Unlike the original script, where the enemies are drawn
5

towards each other based on hate values, we simplify the setup by modifying the opponent‚Äôs strategy.
The three strategies tested are: 1) focusing fire on the closest enemy, 2) targeting the enemy with the
lowest health and shield, and 3) randomly selecting one of the two strategies. The resulting learning
curves are presented in Figure 3.

Figure 3: The learning curve of the models from QMIX and MAPPO algorithms when facing
‚Äôattacking the nearest enemy‚Äô (N), ‚Äôattacking the weakest enemy‚Äô (W), and the ‚Äôrandomly choosing
from the two strategies‚Äô (M). The x-axis is the time steps (1e6) being evaluated and the y-axis is the
average winning rate of 5 different seeds from 32 evaluation processes.
As demonstrated in Figure 3, the MARL algorithms rapidly converge to optimal strategies when
facing individual opponent strategies. Both QMIX and MAPPO achieve either perfect or near-perfect
performance within 2 million time steps. However, when the algorithms encounter the mixed strategy,
their winning rates drop significantly to 36.88% for QMIX and 39.38% for MAPPO. Additionally,
the average Q-values at the 2 million time step are 1.03 for the strategy targeting the nearest enemy,
0.76 for attacking the weakest enemy, and 0.58 for the mixed strategy. This indicates that the mixed
strategy induces a more conservative approach in the predicted Q-values.
More intensively, the analysis of the replays generated by the three strategies reveals that the learned
skills of fire focusing, kiting, and health management are essential response tactics. In scenarios
where the opponent employs a strategy of attacking the nearest enemy, an agent with lower health
can temporarily retreat to minimize exposure to enemy fire. Conversely, when facing the strategy of
targeting the weakest enemy, the weakest agent should actively move forward to draw the opponent‚Äôs
fire, thereby allowing the other agents to carry out their attacks. However, when confronted with
a mixed strategy, agents must devote additional time to assessing whether they should continue
retreating, based on the likelihood of the opponent‚Äôs chosen strategy.
Therefore, training a single MARL model to handle diverse opponent strategies proves to be a significant challenge for current baseline algorithms. To address this, we extend the SMAC environment by
introducing SMAC-HARD, which supports mixed opponent scripts to enhance the model‚Äôs ability
to distinguish between different strategies. Additionally, these opponent scripts can be customized
through the pysc2 package grammar. To further diversify the opponent strategies, we align the
observation, action, and reward interfaces of the opponent agents with those of the learning agents.
This alignment facilitates training agents‚Äô policies using self-play methods or alternative MARL
algorithms, promoting more complex and adaptive learning dynamics.

5

SMAC-HARD

The default opponent strategy in SMAC and SMACv2, as previously discussed, is determined by the
map configuration, which limits the variety of policies for each task. To overcome these limitations,
we introduce three key modifications: an opponent script editing interface, random selection of
scripts, and alignment of the opponent MARL interface with the agent‚Äôs interface. These changes
incrementally enhance the diversity of the opponent strategies, making the tasks more challenging
for MARL algorithms while simultaneously improving the transferability of MARL models across
different scenarios.
6

5.1

LLM Script

While MARL-based models have demonstrated impressive performance in competitive games, they
frequently overfit to particular opponent strategies, leading to instability when encountering unfamiliar
adversaries. In comparison, decision trees exhibit greater stability across diverse opponents and offer
enhanced interpretability. Nonetheless, decision trees demand substantial prior knowledge and are
susceptible to errors in edge cases, resulting in unforeseen outcomes.

Figure 4: The unit information, map information, and task description serve as a system prompt
and are passed to the planner. The planner plans strategy for both sides and the coders implement
the strategy correspondingly. Then the python scripts are the red and blue side of SMAC-HARD to
simulate. Finally, the critic module analyse the simulation results and provide refinement suggestions
to the planner and the coders.
Motivated by the recent work, LLM-SMAC (Deng et al., 2024), which leverages LLMs to address
SMAC tasks, we adopt a similar pipeline for generating decision trees for both agents and opponents.
As illustrated in Figure 4, the agent and opponent share identical environment settings, encompassing
map details, unit data, and task specifications. This shared information constitutes the environment
prompt, which is fed into the strategy planner for both entities. Subsequently, the decision tree coder
generates Python scripts for each side. Unlike LLM-SMAC, where the scripts are tailored for the
python_sc2 package, we convert the python_sc2 scripts into pysc2 scripts using the deepseek coder
model. Once the win rates for both sides stabilize, we select the script as the opponent script for our
SMAC-HARD setup. The critic module evaluates the rollout outcomes and offers implementation
suggestions and strategic refinements for both agents.
5.2

Implementation

According to Figure 5, in terms of source code, the pysc2 package serves as an abstraction of
the sc2_protocol, which is included in the StarCraft II binary files. Through pysc2, players can
initiate new games, select maps, control units, and establish bases. The SMAC framework further
7

Figure 5: The overall architecture of our proposed SMAC-HARD, opponent decision script, self-play
interface, and the SMAC, PySC2, StarCraft II modules.

encapsulates pysc2 by transforming the raw observations into structured, vectorized observations
and states. Actions executed in SMAC are translated into commands that adhere to the sc2_protocol
API via pysc2. Consequently, the StarCraft II environment inherently supports both standardized
actions from SMAC and actions generated by pysc2 scripts, provided they are converted into the
sc2_protocol API format.
Leveraging this inherent support, we modify the SC2Map to enable multi-player mode, ensuring
that units controlled by the agent spawn at the Team 1 location for player 1, while opponent units
spawn at Team 2 for player 2. To prevent action interference, we disable the default attack policy.
Additionally, we reimplement the starcraft.py module in SMAC to accommodate two players, retrieve
raw observations for both, and process actions from both players simultaneously. To mitigate the
impact of action execution order, we parallelize the action-stepping process for the two players.
Beyond modeling the opponent‚Äôs decision tree, we introduce a random strategy selection function
governed by predefined probability settings. These probabilities are represented as a list of float
values, with equal probabilities set as the default configuration. Furthermore, we replicate the
encapsulation of observations, states, and available actions for the agent and expose a similar interface
for the opponent to facilitate self-play models. The self-play mode or decision-tree mode is controlled
via a parameter, ‚Äômode‚Äô, which defaults to the decision-tree mode.
Under these conditions, users can seamlessly transition their experimental environment from SMAC
to SMAC-HARD by simply updating the import command to ‚Äôimport smac-hard‚Äô.

6

SMAC-HARD Experiments

In this section, we present experimental results for our proposed SMAC-HARD framework. Initially,
we train widely used and state-of-the-art SMAC baselines on SMAC-HARD to evaluate the impact
of the mixed opponent strategies. We select both value-based algorithms, such as QMIX, QPLEX,
and LDSA, as well as policy-based algorithms like MAPPO and HAPPO, as our baseline methods.
Additionally, we conduct black-box testing of these algorithms and provide detailed analysis. The
final performance metrics and learning curves are included in this section, with supplementary details
available in the Appendix.
6.1

Baseline Comparisons

Baseline We select widely adopted and state-of-the-art algorithms for our experiments, including
value-based methods QMIX and QPLEX, policy-based method MAPPO, and the latest actor-critic
8

algorithm, HAPPO, all configured with their officially provided default parameters. The implementations of QMIX and QPLEX are sourced from the pymarl2 codebase (Hu et al., 2021), while HAPPO
and MAPPO are provided by (Yu et al., 2022a). The LDSA is obtained from its respective codebases
(Yang et al., 2022)
SMAC and SMAC-HARD We evaluate the baseline algorithms on both the original SMAC
environment and our proposed SMAC-HARD framework across 20 selected tasks. The difficulty
level is set to 7 by default. The battle win rates are computed as the average of 32 evaluation runs.
Each experiment is repeated five times with different random seeds, and the results are smoothed with
a factor of 0.6 for improved visualization over 2 million and 10 million time steps, respectively. The
shaded regions represent the variance across the five seeds, indicating the stability of the generated
policies. The outcomes are presented in Figure 6 and Table 1.
   P

  P B Y V B  P

  P B Y V B  P

   P B Y V B   P

   P B Y V B   P

 0 0 0

 0 0 0 

  V  ]

  V  ]

  V  ] B Y V B  V  ]

  V B Y V B  ]

  V B Y V B  ]

  V B Y V B  ]

  F  V  ]

  P B Y V B  ]

 F R U U L G R U

  K B Y V B  ]

  V B Y V B  V F

 V R B P D Q \ B E D Q H O L Q J

  F B Y V B   ] J

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   

   

   

   

        
 4 0 , ;

   

   
 4 3 / ( ;

   

        
 / ' 6 $

   

   
 0 $ 3 3 2

   

        

   

   

   

    

 + $ 3 3 2

Figure 6: Comparison of mean test winning rate of different algorithms on new SMAC-HARD
environments. The x-axis is the time steps (1e6) being evaluated and the y-axis is the average winning
rate of 5 different seeds from 32 evaluation processes.
According to the results presented in Figure 6, the mixed opponent strategies significantly increase
the difficulty for current MARL algorithms. In previously straightforward tasks, such as 2s3z, 3s5z,
and 2s_vs_1sc, the convergence speed drops by up to 100%. In more challenging tasks, the mixed
opponent strategies shift the convergence points among MARL algorithms, highlighting differences
in their optimality. In super-hard scenarios, including MMM2, 3s5z_vs_3s6z, 27m_vs_30m, and
corridor, not all baseline MARL algorithms achieve satisfactory final performance. These observations
suggest that MARL algorithms with broader policy coverage capabilities are required to address these
challenges effectively.
9

Table 1: Performance in SMAC-HARD tasks within 10M time steps
SMAC-HARD
QMIX
QPLEX
LDSA
MAPPO
3m
0.9938
0.0438
1
1
8m
0.9628
0.0875
0.9687
0.6438
5m_vs_6m
0.4375
0.6188
0.4
0.3138
8m_vs_9m
0.4938
0
0.7187
0.3528
10m_vs_11m
0.6813
0
0.625
0.5681
25m
0.7813
0
0.4375
0.4562
27m_vs_30m
0.0313
0
0.125
0.0938
2s3z
0.4125
0.4313
0.7737
0.8735
3s5z
0.3625
0.1063
0.1876
0.7225
3s5z_vs_3s6z
0
0
0.1319
0.1569
1c3s5z
0.9625
0.7563
0.9375
0.5445
3s_vs_3z
0.9913
0.9875
1
0.9875
3s_vs_4z
0.7938
0.7938
0.7815
0.9875
3s_vs_5z
0.3438
0.5
0.875
0.9407
bane_vs_bane
0.975
0.2313
0.8229
0.9125
so_many_baneling
0.9625
0.5938
0.875
0.8813
2s_vs_1sc
0.7563
0.8875
0.4375
0.8375
2m_vs_1z
0
0
0
0
2c_vs_64zg
0.7891
0.5
0.9062
0.9741
MMM
0.9875
0.95
0.875
0.3267
MMM2
0.275
0.0006
0.5312
0.1925
6h_vs_8z
0.0188
0
0.0625
0.0607
corridor
0.3063
0.1688
0
0.1804

HAPPO
0.4375
0.8625
0.004
0.5006
0.5672
0.2092
0.0063
0.7547
0.6691
0.2573
0.975
0.4875
0.15
0
0
0.9188
0
0
0.7686
0.8813
0.0425
0
0

In comparison to the original SMAC tasks, where nearly all algorithms achieve close to 100%
win rates within 10 million time steps, SMAC-HARD introduces significantly higher difficulty, as
evidenced by the lower win rates. As shown in Figure 6, the 2m_vs_1z task, which is relatively easy
in the original SMAC environment, becomes a super-hard task in SMAC-HARD. We analyzed this
phenomenon by reviewing replay videos. In SMACv1, the opponent zealot‚Äôs attack target changes
based on the hate values of the two marine agents, causing the zealot to focus on the marine currently
attacking. This leads to a strategy where the two marines alternate attacks to divert the zealot‚Äôs
attention. However, in SMAC-HARD, the zealot consistently targets one marine, requiring one
marine to kite the enemy while the other focuses on attacking. This dominant strategy demands
precise and continuous action from the targeted marine, posing a significant challenge to MARL
algorithms.
In addition to the conservative Q-values introduced by the mixed opponent strategies, the rollout
returns exhibit higher variance. This increased variance poses challenges for MARL algorithms, such
as the collapse of attention matrices after convergence during training, which may affect attentionbased MARL algorithms. To illustrate this phenomenon, we provide the win rates of baseline
algorithms at 2 million time steps in the Appendix, which could serve as a potential benchmark for
evaluating the sample efficiency of new MARL algorithms.
6.2

Black-box Evaluation

To assess the transferability of models trained using MARL algorithms, we conducted a black-box
evaluation. During training, agents were exposed only to the default opponent strategy on the original
maps, but during evaluation, they faced the new mixed opponent strategies in the SMAC-HARD
environment. We trained the baseline algorithms on the SMACv1 environment for 10 million time
steps and saved the trained models. These models were then tested in the SMAC-HARD environment
to measure their win rates. The black-box evaluation results are summarized in Table 2.
The black-box evaluation reveals that MARL algorithms tend to overfit to the specific single opponent
strategy they encounter during training, as evidenced by the low evaluation win rates. This suggests
that the skills learned by these models are not robust or generalizable strategies but rather aggressive
or opportunistic tactics tailored to the specific training scenario.
10

Table 2: Performance in SMAC-HARD tasks in black-box mode.
SMAC-HARD
QMIX
QPLEX
MAPPO
3m
0.0117
0.0898
0.1028
8m
0.0375
0
0.0513
5m_vs_6m
0
0
0
8m_vs_9m
0
0
0
10m_vs_11m
0
0
0
25m
0
0
0
27m_vs_30m
0
0
0
2s3z
0.0508
0
0.1217
3s5z
0.0313
0
0.0375
3s5z_vs_3s6z
0
0
0
1c3s5z
0.0703
0
0.1027
3s_vs_3z
0.3359
0.3555
0.4125
3s_vs_4z
0.7461
0.5052
0.4467
3s_vs_5z
0.5305
0.8573
0.5359
bane_vs_bane
0.2578
0
0.3484
so_many_baneling
0
0
0
2s_vs_1sc
0
0
0
2m_vs_1z
0
0
0
2c_vs_64zg
0.6238
0
0.3419
MMM
0
0
0
MMM2
0
0
0
6h_vs_8z
0.0898
0
0
corridor
0
0
0

HAPPO
0.0573
0.1217
0
0
0
0
0
0.0959
0.0705
0
0.0912
0.3724
0
0
0
0
0
0
0.2565
0
0
0
0

An illustrative example can be found in the comparison of win rates across the 3s_vs_3z, 3s_vs_4z,
and 3s_vs_5z tasks. Unlike other scenarios where MARL models achieve near-zero win rates, the
black-box evaluation win rates increase with the difficulty of the tasks. To effectively solve the stalker
versus zealot task, the optimal strategy involves kiting‚Äîstalker units must alternate between walking
and attacking to exploit their speed advantage. In the easier 3s_vs_3z task in SMACv1, agents may
learn aggressive attacking skills that are sufficient to win. However, for more challenging tasks, agents
must adopt a strict kiting mechanism, which is the optimal response strategy. Consequently, agents
that have learned the optimal response strategy are more likely to succeed against the black-box
edited opponent scripts. This phenomenon underscores the critical importance of training with a
diverse range of strategies to ensure robust and generalizable models.

7

Conclusion

In this study, we present a series of experimental evaluations to demonstrate that the single, default opponent policy used in SMAC and SMACv2 lacks diversity in policy spaces. To address this limitation,
we introduce SMAC-HARD, which supports opponent script editing, probabilistic mixed opponent
policies, and self-play interface alignment, significantly mitigating the issue. Our results show that
even popular and state-of-the-art MARL algorithms, which achieve near-perfect performance in
traditional SMAC environments, struggle to maintain high win rates in SMAC-HARD. Additionally,
we conduct a black-box evaluation of models trained using MARL algorithms to highlight the limited
transferability of strategies when facing a single, vulnerable opponent policy. Finally, we align the
training interface for opponents with that of the agents, providing a platform for potential self-play
research in MARL. We believe that SMAC-HARD can serve as a challenging and editable domain,
contributing to the MARL research community by capturing practical challenges and fostering further
advancements.

11

References
Cao, Y., Yu, W., Ren, W., and Chen, G. An overview of recent progress in the study of distributed
multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):427‚Äì438, 2012.
Deng, Y., Ma, W., Fan, Y., Zhang, Y., Zhang, H., and Zhao, J. A new approach to solving smac
task: Generating decision tree code from large language models. arXiv preprint arXiv:2410.16024,
2024.
Ellis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J., and Whiteson, S.
Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. Advances
in Neural Information Processing Systems, 36, 2024a.
Ellis, B., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J., and Whiteson, S. Smacv2:
A new benchmark for cooperative multi-agent reinforcement learning. Advances in Neural
Information Processing Systems, 2024b.
Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and
Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge.
Advances in Neural Information Processing Systems, Jun 2022.
Feng, X., Luo, Y., Wang, Z., Tang, H., Yang, M., Shao, K., Mguni, D., Du, Y., and Wang, J. Chessgpt:
Bridging policy learning and language modeling. Advances in Neural Information Processing
Systems, Jun 2024.
Gorsane, R., Mahjoub, O., de Kock, R. J., Dubb, R., Singh, S., and Pretorius, A. Towards a
standardised performance evaluation protocol for cooperative marl. Advances in Neural Information
Processing Systems, 35:5510‚Äì5521, 2022.
Hu, J., Jiang, S., Harding, S. A., Wu, H., and Liao, S.-w. Rethinking the implementation tricks
and monotonicity constraint in cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2102.03479, 2021.
Jin, X., Wang, Z., Du, Y., Fang, M., Zhang, H., and Wang, J. Learning to discuss strategically: A
case study on one night ultimate werewolf. arXiv preprint arXiv:2405.19946, 2024.
Kuba, J., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y. Trust region policy optimisation
in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, Sep 2021.
Kurach, K., Raichuk, A., StanÃÅczyk, P., Zajac,
Àõ M., Bachem, O., Espeholt, L., Riquelme, C., Vincent,
D., Michalski, M., Bousquet, O., and Gelly, S. Google research football: A novel reinforcement
learning environment. Proceedings of the AAAI conference on artificial intelligence, Jul 2020.
Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as
policies: Language model programs for embodied control. In 2023 IEEE International Conference
on Robotics and Automation (ICRA), pp. 9493‚Äì9500. IEEE, 2023.
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O., and Mordatch, I. Multi-agent actor-critic for
mixed cooperative-competitive environments. Advances in Neural Information Processing Systems,
Jun 2017.
Ma, Y., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and
Anandkumar, A. Eureka: Human-level reward design via coding large language models. arXiv
preprint arXiv:2310.12931, Oct 2023.
Micah, C., Shah, R., Ho, M., Griffiths, T., Seshia, S., Abbeel, P., and Dragan, A. On the utility of
learning about humans for human-ai coordination. Advances in Neural Information Processing
Systems, Oct 2019.
Mitra, A. Towards robust multi-agent reinforcement learning. In Proceedings of the AAAI Symposium
Series, volume 3, pp. 308‚Äì308, 2024.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, Dec 2013.
12

Oliehoek, F. A. and Amato, C. A Concise Introduction to Decentralized POMDPs. Springer
Publishing Company, Incorporated, 1st edition, 2016. ISBN 3319289276.
Peng, B., Rashid, T., Witt, C., Kamienny, P.-A., Torr, P., B√∂hmer, W., and Whiteson, S. Facmac:
Factored multi-agent centralised policy gradients. Advances in Neural Information Processing
Systems, Mar 2021.
Rashid, T., Samvelyan, M., Witt, C., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: Monotonic
value function factorisation for deep multi-agent reinforcement learning. Journal of Machine
Learning Research, Mar 2020.
Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M.,
Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint
arXiv:1902.04043, 2019a.
Samvelyan, M., Rashid, T., Witt, C., Farquhar, G., Nardelli, N., Rudner, T., Hung, C.-M., Torr, P.,
Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043,
Feb 2019b.
Schulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P. Trust region policy optimization.
arXiv preprint arXiv:1502.05477, Feb 2015.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, Jul 2017.
Singh, S., Mahjoub, O., de Kock, R., Khlifi, W., Vall, A., Tessera, K.-a., and Pretorius, A. How much
can change in a year? revisiting evaluation in multi-agent reinforcement learning. arXiv preprint
arXiv:2312.08463, 2023.
Son, K., Kim, D., Kang, W., Hostallero, D., and Ye, Y. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. International conference on machine
learning, May 2019.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W., Zambaldi, V., Jaderberg, M., Lanctot, M.,
Sonnerat, N., Leibo, J., Tuyls, K., and Graepel, T. Value-decomposition networks for cooperative
multi-agent learning. arXiv preprint arXiv:1706.05296, Jun 2017.
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente,
R. Multiagent cooperation and competition with deep reinforcement learning. PLOS ONE, pp.
e0172395, Apr 2017. doi: 10.1371/journal.pone.0172395. URL http://dx.doi.org/10.1371/
journal.pone.0172395.
Tan, W., Zhang, W., Xu, X., Xia, H., Ding, Z., Li, B., Zhou, B., Yue, J., Jiang, J., Li, Y., An, R., Qin,
M., Zong, C., Zheng, L., Wu, Y., Chai, X., Bi, Y., Xie, T., Gu, P., Li, X., Zhang, C., Tian, L., Wang,
C., Wang, X., Karlsson, B., An, B., Yan, S., and Lu, Z. Cradle: Empowering foundation agents
towards general computer control. Advances in Neural Information Processing Systems, Jul 2024.
Terry, J., Black, B., Grammel, N., Jayakumar, M., Hari, A., Sullivan, R., Santos, L., Perez, R., Horsch,
C., Dieffendahl, C., Williams, N., Lokesh, Y., and Ravi, P. Pettingzoo: Gym for multi-agent
reinforcement learning. Advances in Neural Information Processing Systems, 2021.
Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anandkumar, A., Nvidia, N.,
Caltech, C., Austin, U., and Asu, A. Voyager: An open-ended embodied agent with large language
models. arXiv preprint arXiv:2305.16291, 2023.
Wang, J., Ren, Z., Liu, Z., Yu, Y., and Zhang, C. Qplex: Duplex dueling multi-agent q-learning.
arXiv preprint arXiv:2008.01062v, Aug 2020.
Wei, H., Chen, J., Ji, X., Qin, H., Deng, M., Li, S., Wang, L., Zhang, W., Yu, Y., Liu, L., Huang,
L., Ye, D., Fu, Q., and Yang, W. Honor of kings arena: an environment for generalization in
competitive reinforcement learning. Advances in Neural Information Processing Systems, Sep
2022.
13

Witt, C., Gupta, T., Denys, M., Makoviychuk, V., Torr, P., Sun, M., and Whiteson, S. Is independent
learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533,
Nov 2020.
Xu, Z., Yu, C., Fang, F., Wang, Y., and Wu, Y. Language agents with reinforcement learning for
strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, Oct 2023.
Yang, M., Zhao, J., Hu, X., Zhou, W., and Li, H. Ldsa: Learning dynamic subtask assignment
in cooperative multi-agent reinforcement learning. Advances in Neural Information Processing
Systems, May 2022.
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness
of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:
24611‚Äì24624, 2022a.
Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of
mappo in cooperative, multi-agent games. Advances in Neural Information Processing Systems,
Mar 2022b.
Zhang, C. and Lesser, V. Coordinated multi-agent reinforcement learning in networked distributed
pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L., Wang, X., Qiao,
Y., Zhang, Z., Dai, J., University, T., and Research, S. Ghost in the minecraft: Generally capable
agents for open-world enviroments via large language models with text-based knowledge and
memory. arXiv preprint arXiv:2305.17144, 2023.

14

Appendix
A

Final Performance at 2M time step

In the Baseline Comparisons section above, we have listed the final performance of baseline algorithms at 10M time steps. Additionally, we also list the performance at 2M time step. The results
may also considered as the benchmark to judge the sample efficiency of an algorithm.
Table 3: Final performance at 2M time step with default opponent policy
SMAC

QMIX

QPLEX

LDSA

MAPPO

HAPPO

3m
8m
5m_vs_6m
8m_vs_9m
10m_vs_11m
25m
27m_vs_30m
2s3z
3s5z
3s5z_vs_3s6z
1c3s5z
3s_vs_3z
3s_vs_4z
3s_vs_5z
bane_vs_bane
so_many_baneling
2s_vs_1sc
2m_vs_1z
2c_vs_64zg
MMM
MMM2
6h_vs_8z
corridor

0.9798
0.9797
0.544
0.9137
0.9515
0.975
0.4202
0.9732
0.9485
0.0256
0.9893
0.994
0.9857
0.7771
0.783
0.9672
0.9917
0.9926
0.9226
0.9824
0.7875
0.1438
0

0.9856
0.972
0.4385
0.6479
0.6542
0.5292
0.1245
0.9796
0.9351
0.0783
0.9607
0.9945
0.364
0.3729
0.9967
0.9504
0.9906
0.988
8318
0.9783
0.2747
0.0074
0

1
0.9684
0.7605
0.9308
0.929
0.9655
0.5161
0.9843
0.9301
0.5468
0.9834
0.9991
0.9997
0.9137
1
0.9685
0.9943
1
0.9425
0.9743
0.7582
0.1362
0.8234

0.9897
0.9538
0.5103
0.7821
0.6859
0.9692
0.6051
0.941
0.4512
0.1128
0.982
0.982
0.9744
0.9744
0.9974
0.9821
1
1
0.9359
0.9256
0.4487
0.0154
0.3077

0.9945
0.9947
0.437
0.3976
0.2822
0.94
0.8289
0.9831
0.9695
0.0738
0.9771
0.9846
0.6434
0.2051
0.9862
0.9792
0.9965
0.9936
0.9494
0.989
0.9068
0
0.3693

Table 4: Final performance at 2M time step with mixed edited opponent policy
SMAC

QMIX

QPLEX

LDSA

MAPPO

HAPPO

3m
8m
5m_vs_6m
8m_vs_9m
10m_vs_11m
25m
27m_vs_30m
2s3z
3s5z
3s5z_vs_3s6z
1c3s5z
3s_vs_3z
3s_vs_4z
3s_vs_5z
bane_vs_bane
so_many_baneling
2s_vs_1sc
2m_vs_1z
2c_vs_64zg
MMM
MMM2
6h_vs_8z
corridor

0.9917
0.8083
0.3166
0.4063
0.3646
0.5313
0
0.3438
0.2312
0
0.4708
0.8771
0.4437
0.2125
0.9646
0.9375
0.0167
0
0.0958
0.8167
0
0
0

0.9958
0.8792
0.2688
0.5729
0.5292
0.1563
0.0042
0.2688
0.1292
0.0021
0.2729
0.5458
0.1938
0.3021
0.9667
0.9458
0.0104
0
0.0021
0.0167
0
0
0

1
0.4375
0.2563
0.3646
0.1632
0.1563
0.0083
0.6542
0.1146
0
0.7042
0.9497
0.8247
0.566
0.8125
0.7313
0.2896
0
0.3333
0.7153
0
0.023
0

0.6166
0.5979
0.3104
0.2958
0.3813
0.2583
0.0125
0.7042
0.0667
0
0.5292
0.9979
0.95
0.7479
0.8813
0.7896
0.5604
0
0.6542
0.25
0
0.0125
0

0.35
0.7687
0
0.2938
0.35
0.0063
0
0.4229
0.3417
0
0.852
0.0021
0.1063
0
0.9938
0.8729
0
0
0.4417
0.2854
0
0
0

15

B

Return Performance

In line with the reward calculation methods used in SMAC, there is a general trend where higher
returns correlate with higher win rates. However, the reward is determined by multiple factors,
including health and shield values, while the win condition is based on which side has no remaining
units. This discrepancy can lead to a slight misalignment between the expected return and the actual
win rate. For instance, a strategy that results in a single full-health agent surviving would yield
significantly higher rewards compared to one where three agents survive but with low health. In this
context, we present the expected returns in this section as shown in Figure 7.
   P

  P B Y V B  P

  P B Y V B  P

   P B Y V B   P

   P B Y V B   P

 0 0 0

 0 0 0 

  V  ]

  V  ]

  V  ] B Y V B  V  ]

  V B Y V B  ]

  V B Y V B  ]

  V B Y V B  ]

  F  V  ]

  P B Y V B  ]

 F R U U L G R U

  K B Y V B  ]

  V B Y V B  V F

 V R B P D Q \ B E D Q H O L Q J

  F B Y V B   ] J

  
  
 
  
  
 
  
  
 
  
  
 
  
  
 
   

   

   

   

        
 4 0 , ;

   

   
 4 3 / ( ;

   

        
 / ' 6 $

   

   
 0 $ 3 3 2

   

        

   

   

   

    

 + $ 3 3 2

Figure 7: Comparison of mean test expected return of different algorithms on new SMAC-HARD
environments. The x-axis is the time steps (1e6) being evaluated and the y-axis is the average winning
rate of 5 different seeds from 32 evaluation processes.

16

