LLM-based Multi-Agent Reinforcement Learning:
Current and Future Directions

arXiv:2405.11106v1 [cs.MA] 17 May 2024

Chuanneng Sun, Student Member, IEEE, Songjun Huang, Student Member, IEEE,
and Dario Pompili, Fellow, IEEE

Abstract—In recent years, Large Language Models (LLMs)
have shown great abilities in various tasks, including question
answering, arithmetic problem solving, and poem writing, among
others. Although research on LLM-as-an-agent has shown that
LLM can be applied to Reinforcement Learning (RL) and achieve
decent results, the extension of LLM-based RL to Multi-Agent
System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in
the RL frameworks of a single agent. To inspire more research on
LLM-based MARL, in this letter, we survey the existing LLMbased single-agent and multi-agent RL frameworks and provide
potential research directions for future research. In particular, we
focus on the cooperative tasks of multiple agents with a common
goal and communication among them. We also consider humanin/on-the-loop scenarios enabled by the language component in
the framework.
Index Terms—Multi-Agent Reinforcement Learning, Language
Models, Multi-Agent Systems.

I. I NTRODUCTION

M

ULTI-Agent Reinforcement Learning (MARL) has
emerged as a popular approach to address the coordination problem in Multi-Agent Systems (MAS). As opposed to
Individual Reinforcement Learning (IRL)-based or traditional
optimization-based solutions, MARL has shown a significant
improvement in scalability and robustness to uncertainty and
dynamicity [1]–[4]. This improvement is largely attributed to
the communication and coordination among agents inherent in
MARL, where multiple agents learn and adapt their policies
simultaneously while interacting within a shared environment
and communicating with others. However, how and what
to communicate among the agents in the MAS remains to
be explored. Representative examples include MARL frameworks that learn to generate numerical messages using neural networks, formulate neural communication protocols, and
learn targeted ad hoc communications. Despite the decent
performance of the MARL frameworks achieved in various
applications, they still underperform human experts. As a
result, it is reasonable to think why not leveraging human
knowledge and human languages in MARL?
As recent advances in Natural Language Processing (NLP)
demonstrate great abilities in multi-modal tasks, languageconditioned MARL becomes a promising research problem.
NLP has been an active research topic for decades and many
famous models have been proposed for language modeling
The authors are with the Department of Electrical and Computer Engineering, Rutgers University–New Brunswick, NJ, USA. Emails: {chuanneng.sun,
songjun.huang, pompili}@rutgers.edu
This work was supported by the NSF RTML Award No. CCF-1937403.

Parameter Size
GPT 4-1.76T
PaLM-540B

PaLM-E-562B

PaLM 2-340B
GPT 3.5-175B
LaMDA 137B

Claude 3-137B

Claude 2-130B
LLaMa 2-69B

LLaMa-65B

Gemini 1.5 Pro
175B

LLaMa 3-71B

LLaMa-32B
LLaMa-13B

LLaMa 2-13B

LLaMa-7B

LLaMa 2-7B

LLaMa 3-8B
Year

2022

2023

2024

Fig. 1: Well-known Large Language Models (LLMs) over the
past three years. Among them, only PaLM-E from Google
is trained specifically for embodied applications, e.g., robot
control.

such as Recurrent Neural Network (RNN) [5], [6], Long-Short
Term Memory networks (LSTM) [7], and transformers [8].
These foundational models have greatly improved the ability
of machines to understand and generate human language,
setting the stage for more complex applications.
In recent years, the integration of NLP with single-agent
RL has led to the development of language-conditioned RL
frameworks [9]–[11], especially as Large Language Models (LLMs) [12]–[15] emerged as the rising star in the artificial
intelligence community (see Fig. 1) and has been successfully applied in various fields [16]–[18]. Pre-trained LLMs
contain general human knowledge about the world and can
easily adapt to RL problems without the need for retraining.
This integration not only leverages the semantic richness of
language but also allows for the dynamic adjustment of agent
behaviors based on linguistic input. In particular, LLM is able
to generate new information that it has not seen before on
the basis of a few examples. For example, in Reflexion [19],
the authors showed that the LLM agent could generate decent
reflections on its decisions without any reward/feedback from
the environment. Such capabilities are particularly valuable in
multi-agent systems, where agents must coordinate and cooperate based on shared goals communicated through language.
Due to the need for communication and coordination, the
problem of MARL becomes more complex than simply mul-

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

tiplying the RL of a single agent by the number of agents.
As opposed to conventional MARL, LLMs-based MARL can
leverage linguistic cues to facilitate inter-agent communication and collaboration, further boosting system performance.
For example, agents can use shared language to negotiate
roles, coordinate actions, or exchange information about the
environment or their internal states, thereby aligning their
objectives more effectively. This language-enhanced coordination becomes critical in complex scenarios where agents
must handle ambiguous or evolving tasks that require continual
communication and mutual understanding. The exploration of
these capabilities opens up new possibilities for designing
more intelligent and flexible multi-agent systems capable of
operating in unpredictable, real-world environments.
Guo et al. [20] reviewed LLM-based multi-agent frameworks, but the emphasis of that paper was not on MARL.
Unlike their paper, this letter focuses more on the MAS that
tries to accomplish a task cooperatively. In addition to that,
there are several surveys on the topic of MARL [21]–[23]
and single agent LLM-based RL [24], [25], but none of them
is dedicated to LLM-based MARL. Therefore, we claim that
we are among the first to provide a systematic overview of
the LLM-based MARL problem and provide potential future
research directions.
The remainder of this letter is organized as follows. We
first introduce the problem of MARL and provide a brief
overview of conventional, i.e., non-LLM-based, MARL, and
single-agent LLM-based RL, in Sect. II. Then, we will survey
the existing LLM-based MARL frameworks in Sect. III.
After that, we will discuss the challenges and future research
directions for this field in Sect. IV. Finally, we will conclude
the letter in Sect. V.
II. P RELIMINARIES
In this section, we will first introduce the problem of MARL
(Sect. II-A). Then, we will briefly discuss conventional nonLLM-based MARL in Sect. II-B. To prepare the ground for
LLM-based MARL, we will introduce LLM-based singleagent RL in Sect. II-C.
A. MARL Problem Definition
MARL can be modeled with the Decentralized Partially
Observable Markov Decision Process (Dec-POMDP) [26], an
extension to a multi-agent manner of the Markov Decision
Process (MDP). An MDP for N agents consists of a set
of states s ∈ S, which describes all the configurations for
the participating agents, a set of actions A1 , ..., AN and a
set of observations O1 , ..., ON . Each agent i has a policy
π i : Oi × Ai 7→ [0, 1] parameterized by θi . We denote
deterministic policies by µ i : Oi 7→ Ai . The environment will
generate the next state based on the state transition function
T : S × A1 × ... × AN 7→ S. Each agent will receive
a reward from the environment as a function of state and
action ri : S × Ai 7→ R as well as an individual observation
that is correlated with the state, oi : S 7→ Oi . Each
PT agent
tries to maximize its total expected return Ri = t=0 γ t rit ,
where γ is a discount factor, and T is the total time length.

2

A key difference between Dec-POMDP and normal MDP is
the partial observability, i.e., for one agent, the actions of
other agents and the subsequent outcomes are not directly
observable, thereby increasing the difficulty of solving the
problem. Due to this partial observability, individual uncoordinated learning frameworks will not work well. Typical
deep MARL frameworks adopt the actor-critic structure, where
actors are trained to output the action given the observation,
and the critics output a score to judge whether these actions
are good in the long-term horizon.
B. Traditional MARL
To solve the problem of Dec-POMDP, many frameworks
have been proposed. These frameworks can be roughly categorized into two classes: learning-to-cooperate and learningto-communicate.
Learning to coordinate: The first kind of approach, such
as QMIX [27], QTRAN [28], MADDPG [29], MAPPO [30],
and many others [31]–[36], assumes that through centralized
training with ideal communication, agents can learn to work
with each other during the centralized training; therefore,
communication is not needed during execution. In other words,
these approaches expect the agents to learn to adapt to other
agents’ behavior patterns. These approaches can also be classified as policy-based and value-based approaches. Policy-based
approaches typically adopt the actor-critic architecture where
actors are trained to make decisions, and critics approximate
the long-term return and provide feedback to the actors. Valuebased approaches learn optimized joint Q values given the
team’s observations and actions. A problem that often happens
in this situation is the credit assignment problem, where the
critic needs to determine the contribution of each agent to the
performance.
Learning to communicate: In communication-based approaches, agents are equipped with the capability to share
information through various means, such as adjusting the
content of the shared messages [37] or optimizing the structure
of the communication network [38]. This explicit inter-agent
communication facilitates coordinated strategies and is crucial
in dynamic environments where conditions and objectives may
frequently change [39], [40]. Effective communication enables
agents to form coalitions to achieve common goals, adapt
to peers’ actions, and optimize collective outcomes, improving system performance in tasks ranging from cooperative
manipulation to competitive strategic games [37]. Protocols
for communication, often learned during training, leverage
advanced techniques such as differentiable interagent learning
algorithms, which refine communication patterns based on
environmental feedback [41]–[43]. In addition, frameworks for
learning emergent communication protocols/languages have
also been proposed [44], [45]. These frameworks encourage
the agents to learn a certain “language” that is understandable
by other agents and encodes certain information.
C. LLM-based Single-Agent RL
As LLMs demonstrated their abilities in various tasks,
several LLM-based decision-making frameworks have been

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

proposed. These frameworks are not necessarily RL frameworks because many of them are open-loop, meaning that the
feedback/reward from the environment is not used during the
decision-making process. Instead, many frameworks simply
leverage the generalizability of LLMs and the general knowledge they contain to solve problems. Typically, in these works,
a few examples of how the LLMs are expected to solve the
problem are provided, and the LLMs can generalize from these
examples to new problems.
Open-loop LLM-based RL: Among these frameworks, we
will summarize some significant contributions. Yao et al. [46]
proposed ReAct, in which the LLM is prompted to generate
“thoughts” to solve the problem given the observation, allowing the model to dynamically adjust and refine its strategies in
response to changing environmental cues and task demands.
Based on ReAct, Shinn et al. [19] proposed Reflexion, which
uses a few-shot verbal feedback to enhance decision-making
capabilities. Reflexion processes feedback from interactions
within task environments into textual summaries, which are
then used to augment the model’s episodic memory. Prasad et
al. [47] proposed ADaPT, where LLMs learn to decompose
the task into subtasks through short examples. Although these
approaches can achieve decent performances in reasoning or
word-based games, they are constrained by the knowledge the
LLMs have and could be biased for certain problems. More
importantly, the reward, one of the most important signals from
the environment, is not considered.
Closed-loop LLM-based RL: There are also LLM-based
RL frameworks that incorporate feedback for closed-loop
control. Paul et al. [48] proposed Refiner, in which a finetuned LLM is used to provide feedback on policy decisions.
Zhang et al. [49] introduced a framework that uses feedback
from LLMs to enhance credit assignment in RL tasks. Their
work targeted sparse reward environments and leveraged the
rich domain knowledge available in LLMs to dynamically
generate and refine reward functions. To improve sample
efficiency, the authors proposed sequential, tree-based, and
moving target feedback, facilitating more targeted exploration
and reducing redundancy in state exploration. Yao et al. [50]
proposed Retroformer, where a frozen LLM is used as the
policy, while another smaller LM is trained to provide verbal feedback on the decisions based on the reward. Murthy
et al. [51] proposed REX, adopting the Monte-Carlo Tree
Search (MCTS) algorithm as the basis to solve problems. The
Upper Confidence Bound (UCB) technique is adopted to guide
the agent’s exploration.
Besides the aforementioned work that uses LLMs as RL
policies, multi-modal LLMs that are trained on RL tasks such
as robot control (e.g., PaLM-E [52]) and models for grounding
languages to actions [53], [54] have also been proposed. These
models can achieve decent zero-shot performances in several
robotic tasks because of their parameter scale.
III. E XISTING LLM- BASED MARL
Although LLM-based MARL frameworks have not been
widely studied, there is still some work focused on this topic.
MARL for problem solving: Huang et al. [71] introduced γ-Bench, which encompasses a variety of multi-agent

3

games to assess these models. Their work included a detailed
analysis of different versions of the GPT models, which
demonstrated a systematic improvement in their game ability.
This framework demonstrated the enhanced performance of
newer LLM versions, such as GPT-4, and the potential to
augment these models with reasoning techniques such as CoT.
Liu et al. [55] proposed Dynamic LLM-Agent Network (DyLAN), a framework that studied the capabilities of LLM-agent
collaborations for complex reasoning and code generation
tasks. Unlike previous methods that used static architectures,
DyLAN dynamically adjusted agent interactions based on realtime performance and task demands, incorporating features
such as inference-time agent selection and an early stopping
mechanism. This allowed DyLAN to enhance computational
efficiency and optimize the contribution of individual agents
through an unsupervised scoring metric, the agent importance score. Slumbers et al. [59] introduced the FunctionallyAligned Multi-Agents (FAMA) framework by integrating a
centralized critic architecture and allowing natural language
communication between agents. The framework aligns LLMs
to the functional needs of the environment through an online fine-tuning process, which adjusts the LLM’s pre-trained
knowledge to better fit the specific task requirements. Additionally, FAMA allows for intuitive inter-agent communication
in natural language, making the coordination more efficient
and human-interpretable. Chen et al. [60] present a study on
the dynamics of consensus seeking in multi-agent systems
driven by LLMs. The authors focused on the inter-agent
negotiation processes, where each agent starts with a unique
numerical state and negotiates to reach a unified consensus.
They also provided insights on how different factors, such as
agent personality (stubborn vs. suggestible), agent number, and
network topology, influence the negotiation and consensus process. Li et al. [61] explored Theory of Mind (ToM) modeling
with LLMs generating communication messages and beliefs
about the environment and other agents. Hong et al. [69]
proposed MetaGPT, where agents share messages with all
other agents in a message pool and agents can subscribe to
messages related to their task.
MARL for embodied applications: Other than the aforementioned MARL frameworks for problem solving, there are
also LLM-based MARL frameworks for embodied application.
Zhang et al. [62] proposed a Cooperative Embodied Language
Agent (CoELA), a modular framework that integrates LLM
to improve communication and collaborative decision-making
among multiple agents. The modular structure includes a
perception module for interpreting sensory data, a memory
module for retaining and recalling environmental and taskrelated information, a communication module to facilitate
inter-agent dialogue, a planning module for strategic decision
making, and an execution module for carrying out planned
actions. By incorporating LLMs into the memory, communication, and planning modules, the framework enables agents
to utilize natural language to improve both understanding
and execution of cooperative tasks. Kannan et al. [64] introduced SMART-LLM, a framework that integrated LLM
with multi-agent robot task planning to translate high-level
instructions into executable strategies for robot teams. By

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

4

TABLE I: Existing LLM for MARL frameworks with an emphasis on multi-agent coordination.
Framework
DyLAN [55]
FAMA [59]
Chen et al. [60]
Li et al. [61]
CoELA [62]
SMART-LLM [64]
RoCo [65]
Co-NavGPT [66]
Guo et al. [68]
MetaGPT [69]

Application
Reasoning, Coding
Text Game, Driving
Consensus Seeking
Path Planning
Multi-Agent Planning
Multi-Agent Planning
Motion Planning
Semantic Navigation
Multi-Agent Cooperation
Coding

Dataset/Simulator
MATH, MMLU [56], [57]; HumanEval [58]
BabyAI-Text, Traffic Junction [39]
Generated Data
Close-source simulator
TDW-MAT, C-WAH [63]
Proposed Benchmark Dataset
RoCoBench
Habitat-Matterport 3D [67]
VirtualHome-Social
HumanEval [58], MBPP [70]

Training
✗
✓
✗
✗
✓
✗
✗
✗
✗
✗

LLM Role
Decision, Communication
Decision, Communication
Decision
Decision, Communication, Theory of Mind
Decision, Communication, Memory
Decision, Planning
Decision, Planning
Planning
Decision, Communication
Code Generation, Communication

structuring task planning into sequential phases of decomposition, coalition formation, and allocation, SMART-LLM
generates robot actions to achieve complex objectives. Their
approach leveraged the cognitive processing power of LLMs to
enhance the comprehension and execution capabilities of robot
systems. Mandi et al. [65] introduced RoCo, a multi-robot
arm collaboration framework with each arm equipped with an
LLM agent. The LLM agents are responsible for coordination
among agents by communicating with other LLM agents
and path planning. Yu et al. [66] introduced Co-NavGPT,
an LLM-based multi-agent navigation framework. However,
unlike other frameworks where multiple LLMs are employed,
in Co-NavGPT, only one LLM is used to assign frontiers to
agents globally. Guo et al. [68] studied the collaboration of
multiple LLM-based agents on various tasks with a focus on
communication and coordination among multiple agents. They
proposed the Criticize-Reflect method with an LLM critic and
an LLM coordinator. Table I provides more details on these
works.
In addition to LLM-based MARL, several works explored
multi-agent interaction [72]–[74], e.g., multi-agent conversation and gaming. However, these works fall out of the MARL
scope; we will not use too much space on them.
Overall, these studies illustrated that while the exploration
into language-conditioned MARL is still nascent, it holds
considerable promise for advancing the capabilities of MAS.
Using natural language, these systems can achieve higher
levels of coordination and understanding, which is essential
for complex environments.

frameworks. In these frameworks, agents are distinguished
by their assigned personalities. For example, an agent with
a “curious” personality will tend to explore the environment,
while an agent with a “conservative” personality will tend to
stay in the safe areas. A team of agents with a combination
of different personalities can often achieve better performance
than those with the same personality. In traditional MARL
frameworks, these personalities are encoded in the agents’
model parameters, i.e., the weights of their models. However,
with LLMs as agents, personalities can be assigned to agents
by prompts, in which narratives about the agent’s personality
will be provided.

IV. O PEN R ESEARCH P ROBLEMS

Future research could focus on developing frameworks that
can effectively integrate personality-driven language models
into MARL systems. This integration involves creating robust
prompts with memories that encode the information from
past experiences in a wide range of interactive scenarios,
allowing agents to learn from both their successes and failures.
Furthermore, evaluating these systems will require new metrics
that can assess not just the efficacy of task performance but
also the appropriateness and effectiveness of communication
between agents.

Despite the research efforts mentioned above, languageconditioned MARL is still an unexplored field with many
unexplored aspects. To inspire more research in this field, we
provide several research directions in this section. Specifically,
we discuss four potential research directions: i) personalityenabled cooperation (Sect. IV-A), ii) language-enabled
human-in/on-the-loop frameworks (Sect. IV-B), iii) traditional
MARL and LLM co-design (Sect. IV-C), and iv) safety and
security in MAS (Sect. IV-D). Fig. 2 also provides a more
vivid demonstration of these research ideas.
A. Personality-enabled Cooperation
Previous work [60], [75] has shown that different personalities in MARL frameworks can produce promising results. This
idea can be naturally extended to language-conditioned MARL

Another potential advantage of language-conditioned
MARL with personalized agents is the ability to handle
conflicts and negotiate solutions more effectively. Agents
can be trained to understand and generate language-based
responses that consider the perspectives and goals of other
agents, facilitating a negotiation process that mirrors human
interaction. This capability is particularly useful in scenarios
where agents must share resources or decide on joint actions
that impact the collective outcome.
However, implementing these personalized language behaviors in agents presents several challenges. The primary
concern is ensuring that language models do not perpetuate
or amplify undesirable biases that could lead to unfair or
inefficient outcomes. Additionally, the complexity of training
such models increases as they must not only understand and
generate appropriate responses, but also adapt their linguistic
style based on the evolving context of the interaction.

Another direction of research is to explore competitive
agents instead of cooperative agents. However, the competition
here should be benign, which means that the agents compete
to achieve the same goal. By addressing these challenges,
language-conditioned MARL with diverse agent personalities
has the potential to advance the field of artificial intelligence.

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

5

Input: Move the cargoto
the truck
Input from R1: Take the
cargo from R1 and move it
to the truck
Input from R2: R3 is a
better choice to move the
cargo
R3: Yes, I will move the
cargo

Input: You are a
robot with a
curious
personality.
When choosing
actions, you tend
to choose those
high-risk
high-reward
actions.

R3

Input: You are a
robot with a
conservative
personality.
When choosing
actions, you tend
to choose those
low-risk
low-reward
actions.

Input: Move the cargoto the truck
Input from R1: Take the cargo from R1 and
move it to the truck
R2: No, that is not a good decision. R3 is
closer to you and the truck
R2
Input: Move the cargoto
the truck
R1: I will move the cargo
to R2, so that it can
move it to the truck
Expert Feedback: Actually, R2 is
better, because it can lift more weights

R1

(b)
Danger
Zone

Communication
Module
Context
Distillation

Perception
Module
Decision-making
Module

(a)

(c)

Fig. 2: Potential research directions for language-conditioned Multi-Agent Reinforcement Learning (MARL). (a) Personalityenabled cooperation, where different robots have different personalities defined by the commands. (b) Language-enabled humanon-the-loop frameworks, where humans supervise robots and provide feedback. (c) Traditional co-design of MARL and LLM,
where knowledge about different aspects of LLM is distilled into smaller models that can be executed on board.
B. Language-enabled Human-in/on-the-Loop Frameworks
One of the direct advantages of language-conditioned
MARL frameworks is the possibility of involving humans
in or on the loop. To illustrate, human-in-the-loop frameworks [76]–[78] involve humans as agents that can generate
actions to affect the environment, while human-on-the-loop
frameworks [79] regard humans as supervisors without directly
being involved in the decision-making process.
In human-in-the-loop setups, humans actively participate in
the learning process, often providing corrective feedback or
rewards to shape agent behaviors in real time. This direct
interaction helps in refining the agent’s actions and strategies,
making them more aligned with human-like reasoning and
ethical standards. For example, a human could guide an
agent away from potential pitfalls in its learning process
that might not be immediately apparent through algorithmic
reinforcement signals alone. On the other hand, human-on-theloop frameworks play a crucial oversight role. Here, humans
monitor the system’s performance and intervene only when
necessary. This approach is particularly valuable in applications where autonomous operations are preferable, but human
oversight is necessary to ensure safety and compliance with
regulatory standards. For example, in autonomous driving,
while the system can handle most driving tasks, a human supervisor may only need to intervene in complex or hazardous
road conditions, ensuring that the system operates within safe
limits without requiring constant human control.
Both of these human roles within language-conditioned
MARL can benefit significantly from the integration of natural
language. Language serves as a versatile interface that enables
clearer and more intuitive communication between humans
and agents. Agents can report their status, explain their decisions, or even ask for clarification in human-understandable

language, improving the effectiveness of human interventions.
Furthermore, the use of language can facilitate the transfer
of knowledge between agents by allowing them to share
insights or strategies in a comprehensible format. In scenarios
involving multiple agents with varying roles, language can
help maintain coherence and unity of purpose across the team,
guiding less experienced agents through complex tasks or
strategies articulated by more experienced ones or even by
human supervisors.
Future research could explore optimizing these interactions between human supervisors and agents, possibly by
developing advanced language models that can understand
and generate more context-aware, situation-specific dialogue.
Furthermore, ensuring that language-based communications
are not only informative, but also prompt and actionable will
be crucial for the practical deployment of such systems in
real-world applications. This balance between automation and
human oversight, facilitated by natural language, promises to
enhance the robustness and reliability of multi-agent systems,
pushing the boundaries of what automated systems can achieve
while ensuring they operate under safe and ethical guidelines.
C. Traditional MARL and LLM Co-Design
Since LLMs tend to have large sizes, especially those pretrained models, performing inference on-board on robot hardware is not practical. A popular way towards resource-efficient
computing is through Parameter-Efficient Fine-Tuning (PEFT)
techniques [80]–[83] combined with quantization. However,
this kind of approach still requires inference through the large
LLM network, which is impractical for small robots. To make
this happen, we envision a co-design framework of traditional
MARL policies and the LM models. A typical design for
such systems could be to use the LLM model as a centralized

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

critic to guide the training of the actors. This design follows
the CTDE scheme introduced in Sect. II-B, where the critic
will be removed during execution. To leverage communication
during execution, we can distill the knowledge from the LLMs
about communication into smaller models that can be executed
onboard.
One potential development is the refinement of the distillation process, which aims to transfer knowledge from LLMs
to more compact models suitable for deployment on less
powerful hardware, such as robots or Internet of Things (IoT)
devices. A promising direction in this direction would be incontext distillation [84], [85], where the teacher model is an
LLM with a pre-defined context. For example, for controlling
warehouse robots, the context can be refined to tell the LLM
to avoid people and collisions. By focusing on the essential features necessary for the communication and decisionmaking learned by the LLM, smaller models can execute
complex tasks effectively with a fraction of the computational
overhead. In addition, to facilitate effective communication
between agents during execution, specialized communication
protocols could be designed. These protocols would utilize
the distilled models to ensure that critical information, as
understood and processed by the LLM during the training
phase, is efficiently conveyed between agents. This approach
not only conserves bandwidth, but also optimizes the real-time
decision-making process, allowing for dynamic adjustments
based on the operational environment and agent states.
Additionally, the co-design framework can be enhanced by
integrating adaptive mechanisms that allow the MARL system
to recalibrate its strategies based on feedback from the operational environment. Such adaptive systems could dynamically
adjust the compression level of the distilled models or modify
the communication protocols based on the complexity of the
tasks and the computational capabilities available at that time.
This flexibility would be particularly useful in environments
where conditions change rapidly or unpredictably, requiring
swift responses from the agent collective. Furthermore, the
implementation of this co-design framework would benefit
significantly from the development of specialized hardware
tailored to the execution of compressed models. This hardware
could optimize the execution of neural network operations,
potentially in a power-efficient manner, which is critical for
mobile or embedded systems.
D. Safety and Security in MAS
Ensuring the safety and security of MAS is critical, especially as these systems are increasingly deployed in diverse
and potentially high-stakes environments. The integration of
language models into MARL introduces unique challenges and
vulnerabilities, from the manipulation of agent communication
to the exploitation of model biases.
Many robotic operations have continuous action spaces,
where the output of each agent’s policy is a set of continuous
values. Unlike discrete action spaces, which can be reformulated as multi-choice problems and solved by prompting the
multi-choice question to the LLM, continuous action space
is more tricky, especially in high-stake environments, for

6

example, operation robots. Existing methods replace the last
few layers of the LLMs with new layers that map the observation in languages to continuous action spaces. However,
this kind of approach requires training the new layers in the
desired environment, which might be inaccessible. Therefore,
exploring alternative methods for integrating LLMs into the
control loop of robots operating in continuous action spaces
without the need for substantial retraining or modification of
the LLMs is promising.
In addition to safety in actions, safety and security against
potential attacks are also crucial in MAS. One way towards
safety is through proactive measures. This includes the development of secure communication protocols between agents to
prevent eavesdropping or the injection of malicious data that
could lead to compromised decision-making. Communications
encryption can be a fundamental aspect of this, ensuring that
even if data transmissions are intercepted, the information
remains protected. In addition, securing the language model
training process against adversarial attacks is crucial. Adversarial training, which involves exposing the system to a wide
range of attack vectors during the training phase, can help
models learn to resist or mitigate these attacks in deployment.
In addition, input validation techniques can be employed to
filter out potentially harmful or misleading inputs that could
cause the system to behave unpredictably. This is particularly
important in scenarios where agents interact with humans or
systems outside the controlled environment and are exposed
to a broader range of language inputs and behaviors.
Despite the best proactive defenses, systems may still encounter unforeseen vulnerabilities post-deployment. Thus, reactive strategies are necessary to quickly address any breaches
or failures. This can involve real-time monitoring of agent
behaviors and communications to detect anomalies that may
indicate a security breach or a failure in safety protocols.
Once an anomaly is detected, the systems should be able
to isolate affected agents and roll back their states to secure
configurations.
V. C ONCLUSION
In this letter, we provide a brief overview of Multi-Agent
Reinforcement Learning (MARL) based on conventional nonLarge Language Model (LLM)-based Multi-Agent Reinforcement Learning (MARL), LLM-based single-agent RL, and
existing LLM-based MARL frameworks. These works paved
the way for new ideas that we discuss in later sections.
Specifically, we discussed potential research directions ranging
from multi-agent personality to safety and security in the
LLM-based Multi-Agent System (MAS). Although works are
studying LLM-based MARL, the field is still to be explored
and has significant potential because of the great ability of
LLMs and their in-context and interpretable nature. With
LLMs, designing MARL frameworks becomes more analogous to modeling the group learning process of animals or
even humans, where knowledge is transferred or exchanged
via natural languages. We hope, with this letter, that more
research works can be enlightened and the boundary of multiagent intelligence could be pushed further.

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

R EFERENCES
[1] C. Sun, S. Huang, and D. Pompili, “Hmaac: Hierarchical multi-agent
actor-critic for aerial search with explicit coordination modeling,” in
2023 IEEE International Conference on Robotics and Automation
(ICRA), pp. 7728–7734, IEEE, 2023.
[2] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multiagent, reinforcement learning for autonomous driving,” arXiv preprint
arXiv:1610.03295, 2016.
[3] V. Sadhu, C. Sun, A. Karimian, R. Tron, and D. Pompili, “Aerialdeepsearch: Distributed multi-agent deep reinforcement learning for
search missions,” in 2020 IEEE 17th International Conference on Mobile
Ad Hoc and Sensor Systems (MASS), pp. 165–173, IEEE, 2020.
[4] J. A. Calvo and I. Dusparic, “Heterogeneous multi-agent deep reinforcement learning for traffic lights control.,” in AICS, pp. 2–13, 2018.
[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by error propagation, parallel distributed processing,
explorations in the microstructure of cognition, ed. de rumelhart and
j. mcclelland. vol. 1. 1986,” Biometrika, vol. 71, pp. 599–607, 1986.
[6] M. I. Jordan, “Serial order: A parallel distributed processing approach,”
in Advances in psychology, vol. 121, pp. 471–495, Elsevier, 1997.
[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems, vol. 30, 2017.
[9] S. Peng, X. Hu, R. Zhang, J. Guo, Q. Yi, R. Chen, Z. Du, L. Li,
Q. Guo, and Y. Chen, “Conceptual reinforcement learning for languageconditioned tasks,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 37, pp. 9426–9434, 2023.
[10] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn, “Language as an
abstraction for hierarchical deep reinforcement learning,” Advances in
Neural Information Processing Systems, vol. 32, 2019.
[11] L. Zhou and K. Small, “Inverse reinforcement learning with natural
language goals,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 35, pp. 11116–11124, 2021.
[12] OpenAI, “ChatGPT: Optimizing Language Models for Dialogue.” https:
//www.openai.com/chatgpt, 2023. Accessed: 2024-04-22.
[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., “Llama
2: Open foundation and fine-tuned chat models,” arXiv preprint
arXiv:2307.09288, 2023.
[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling with pathways,” Journal of Machine Learning
Research, vol. 24, no. 240, pp. 1–113, 2023.
[15] R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable
multimodal models,” arXiv preprint arXiv:2312.11805, 2023.
[16] J. Wu, Z. Lai, S. Chen, R. Tao, P. Zhao, and N. Hovakimyan, “The new
agronomists: Language models are experts in crop management,” arXiv
preprint arXiv:2403.19839, 2024.
[17] Z. Lai, J. Wu, S. Chen, Y. Zhou, A. Hovakimyan, and N. Hovakimyan,
“Language models are free boosters for biomedical imaging tasks,” arXiv
preprint arXiv:2403.17343, 2024.
[18] G. Han, W. Liu, X. Huang, and B. Borsari, “Chain-of-interaction:
Enhancing large language models for psychiatric behavior understanding
by dyadic contexts,” arXiv preprint arXiv:2403.13786, 2024.
[19] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” Advances
in Neural Information Processing Systems, vol. 36, 2024.
[20] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest,
and X. Zhang, “Large language model based multi-agents: A survey of
progress and challenges,” arXiv preprint arXiv:2402.01680, 2024.
[21] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learning for multiagent systems: A review of challenges, solutions, and
applications,” IEEE transactions on cybernetics, vol. 50, no. 9, pp. 3826–
3839, 2020.
[22] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique of
multiagent deep reinforcement learning,” Autonomous Agents and MultiAgent Systems, vol. 33, no. 6, pp. 750–797, 2019.
[23] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement learning:
a survey,” Artificial Intelligence Review, pp. 1–49, 2022.
[24] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas,
E. Grefenstette, S. Whiteson, and T. Rocktäschel, “A survey of reinforcement learning informed by natural language,” arXiv preprint
arXiv:1906.03926, 2019.

7

[25] Y. Cao, H. Zhao, Y. Cheng, T. Shu, G. Liu, G. Liang, J. Zhao, and Y. Li,
“Survey on large language model-enhanced reinforcement learning:
Concept, taxonomy, and methods,” arXiv preprint arXiv:2404.00282,
2024.
[26] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
POMDPs, vol. 1. Springer, 2016.
[27] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster,
and S. Whiteson, “Monotonic value function factorisation for deep
multi-agent reinforcement learning,” The Journal of Machine Learning
Research, vol. 21, no. 1, pp. 7234–7284, 2020.
[28] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran:
Learning to factorize with transformation for cooperative multi-agent
reinforcement learning,” in International conference on machine learning, pp. 5887–5896, PMLR, 2019.
[29] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environments,” Advances in neural information processing systems, vol. 30,
2017.
[30] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu,
“The surprising effectiveness of ppo in cooperative multi-agent games,”
Advances in Neural Information Processing Systems, vol. 35, pp. 24611–
24624, 2022.
[31] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.,
“Value-decomposition networks for cooperative multi-agent learning
based on team reward,” in Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–2087,
2018.
[32] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix:
Expanding monotonic value function factorisation for deep multi-agent
reinforcement learning,” Advances in neural information processing
systems, vol. 33, pp. 10199–10210, 2020.
[33] J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang, “Qplex: Duplex dueling multi-agent q-learning,” in International Conference on Learning
Representations, 2021.
[34] J. Ackermann, V. Gabler, T. Osa, and M. Sugiyama, “Reducing overestimation bias in multi-agent domains using double centralized critics,”
arXiv preprint arXiv:1910.01465, 2019.
[35] Y. Wang, B. Han, T. Wang, H. Dong, and C. Zhang, “Dop: Off-policy
multi-agent decomposed policy gradients,” in International conference
on learning representations, 2020.
[36] T. Zhang, Y. Li, C. Wang, G. Xie, and Z. Lu, “Fop: Factorizing optimal
joint policy of maximum-entropy multi-agent reinforcement learning,”
in International Conference on Machine Learning, pp. 12491–12500,
PMLR, 2021.
[37] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to
communicate with deep multi-agent reinforcement learning,” Advances
in neural information processing systems, vol. 29, 2016.
[38] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and
J. Pineau, “Tarmac: Targeted multi-agent communication,” in International Conference on machine learning, pp. 1538–1546, PMLR, 2019.
[39] S. Sukhbaatar, R. Fergus, et al., “Learning multiagent communication
with backpropagation,” Advances in neural information processing systems, vol. 29, 2016.
[40] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” Advances in neural information processing systems, vol. 30, 2017.
[41] J. Jiang and Z. Lu, “Learning attentional communication for multiagent cooperation,” Advances in neural information processing systems,
vol. 31, 2018.
[42] I. Mordatch and P. Abbeel, “Emergence of grounded compositional
language in multi-agent populations,” in Proceedings of the AAAI
conference on artificial intelligence, vol. 32, 2018.
[43] S. Shen, Y. Fu, H. Su, H. Pan, P. Qiao, Y. Dou, and C. Wang,
“Graphcomm: A graph neural network based method for multi-agent
reinforcement learning,” in ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 3510–3514, IEEE, 2021.
[44] S. Gupta, R. Hazra, and A. Dukkipati, “Networked multi-agent reinforcement learning with emergent communication,” arXiv preprint
arXiv:2004.02780, 2020.
[45] A. Lazaridou and M. Baroni, “Emergent multi-agent communication in
the deep learning era,” arXiv preprint arXiv:2006.02419, 2020.
[46] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao,
“React: Synergizing reasoning and acting in language models,” in The
Eleventh International Conference on Learning Representations, 2023.

SUBMITTED TO IEEE ROBOTICS & AUTOMATION LETTERS, MAY 2024

[47] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal,
and T. Khot, “Adapt: As-needed decomposition and planning with
language models,” arXiv preprint arXiv:2311.05772, 2023.
[48] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West,
and B. Faltings, “Refiner: Reasoning feedback on intermediate representations,” arXiv preprint arXiv:2304.01904, 2023.
[49] A. Zhang, A. Parashar, and D. Saha, “A simple framework for intrinsic
reward-shaping for rl using llm feedback,”
[50] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. R.
N, Z. Chen, J. Zhang, D. Arpit, R. Xu, P. L. Mui, H. Wang, C. Xiong,
and S. Savarese, “Retroformer: Retrospective large language agents with
policy gradient optimization,” in The Twelfth International Conference
on Learning Representations, 2024.
[51] R. Murthy, S. Heinecke, J. C. Niebles, Z. Liu, L. Xue, W. Yao, Y. Feng,
Z. Chen, A. Gokul, D. Arpit, et al., “Rex: Rapid exploration and
exploitation for ai agents,” arXiv preprint arXiv:2307.08962, 2023.
[52] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., “Palm-e: An embodied
multimodal language model,” in International Conference on Machine
Learning, pp. 8469–8488, PMLR, 2023.
[53] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models
as zero-shot planners: Extracting actionable knowledge for embodied
agents,” in International Conference on Machine Learning, pp. 9118–
9147, PMLR, 2022.
[54] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,
J. Ibarz, A. Irpan, E. Jang, R. Julian, et al., “Do as i can, not as i
say: Grounding language in robotic affordances,” in Conference on robot
learning, pp. 287–318, PMLR, 2023.
[55] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, “Dynamic llm-agent
network: An llm-agent collaboration framework with agent team optimization,” arXiv preprint arXiv:2310.02170, 2023.
[56] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,
D. Song, and J. Steinhardt, “Measuring mathematical problem solving
with the math dataset,” NeurIPS, 2021.
[57] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and
J. Steinhardt, “Aligning ai with shared human values,” Proceedings of
the International Conference on Learning Representations (ICLR), 2021.
[58] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford,
M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating
large language models trained on code,” 2021.
[59] O. Slumbers, D. H. Mguni, K. Shao, and J. Wang, “Leveraging large
language models for optimised coordination in textual multi-agent
reinforcement learning,” 2023.
[60] H. Chen, W. Ji, L. Xu, and S. Zhao, “Multi-agent consensus seeking via
large language models,” arXiv preprint arXiv:2310.20151, 2023.
[61] H. Li, Y. Chong, S. Stepputtis, J. P. Campbell, D. Hughes, C. Lewis,
and K. Sycara, “Theory of mind for multi-agent collaboration via large
language models,” in Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, pp. 180–192, 2023.
[62] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and
C. Gan, “Building cooperative embodied agents modularly with large
language models,” in The Twelfth International Conference on Learning
Representations, 2024.
[63] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,
and A. Torralba, “Watch-and-help: A challenge for social perception and
human-ai collaboration,” arXiv preprint arXiv:2010.09890, 2020.
[64] S. S. Kannan, V. L. Venkatesh, and B.-C. Min, “Smart-llm: Smart multiagent robot task planning using large language models,” arXiv preprint
arXiv:2309.10062, 2023.
[65] Z. Mandi, S. Jain, and S. Song, “Roco: Dialectic multi-robot collaboration with large language models,” arXiv preprint arXiv:2307.04738,
2023.

8

[66] B. Yu, H. Kasaei, and M. Cao, “Co-navgpt: Multi-robot cooperative
visual semantic navigation using large language models,” arXiv preprint
arXiv:2310.07937, 2023.
[67] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets,
A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X.
Chang, M. Savva, Y. Zhao, and D. Batra, “Habitat-matterport 3d dataset
(HM3d): 1000 large-scale 3d environments for embodied AI,” in Thirtyfifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2021.
[68] X. Guo, K. Huang, J. Liu, W. Fan, N. Vélez, Q. Wu, H. Wang, T. L.
Griffiths, and M. Wang, “Embodied llm agents learn to cooperate in
organized teams,” arXiv preprint arXiv:2403.12482, 2024.
[69] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang,
Z. Wang, S. K. S. Yau, Z. Lin, et al., “Metagpt: Meta programming
for multi-agent collaborative framework,” in The Twelfth International
Conference on Learning Representations, 2023.
[70] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le, et al., “Program synthesis with large
language models,” arXiv preprint arXiv:2108.07732, 2021.
[71] J.-t. Huang, E. J. Li, M. H. Lam, T. Liang, W. Wang, Y. Yuan, W. Jiao,
X. Wang, Z. Tu, and M. R. Lyu, “How far are we on the decision-making
of llms? evaluating llms’ gaming ability in multi-agent environments,”
arXiv preprint arXiv:2403.11807, 2024.
[72] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li,
L. Jiang, X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm
applications via multi-agent conversation framework,” arXiv preprint
arXiv:2308.08155, 2023.
[73] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
Bernstein, “Generative agents: Interactive simulacra of human behavior,”
in Proceedings of the 36th Annual ACM Symposium on User Interface
Software and Technology, pp. 1–22, 2023.
[74] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel:
Communicative agents for “mind” exploration of large language model
society,” Advances in Neural Information Processing Systems, vol. 36,
2024.
[75] A. Szot, U. Jain, D. Batra, Z. Kira, R. Desai, and A. Rai, “Adaptive
coordination in social embodied rearrangement,” in International Conference on Machine Learning, pp. 33365–33380, PMLR, 2023.
[76] D. Abel, J. Salvatier, A. Stuhlmüller, and O. Evans, “Agentagnostic human-in-the-loop reinforcement learning,” arXiv preprint
arXiv:1701.04079, 2017.
[77] H. Liang, L. Yang, H. Cheng, W. Tu, and M. Xu, “Human-in-the-loop
reinforcement learning,” in 2017 Chinese Automation Congress (CAC),
pp. 4511–4518, IEEE, 2017.
[78] B. Luo, Z. Wu, F. Zhou, and B.-C. Wang, “Human-in-the-loop reinforcement learning in continuous-action space,” IEEE Transactions on
Neural Networks and Learning Systems, 2023.
[79] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,
“Deep reinforcement learning from human preferences,” Advances in
neural information processing systems, vol. 30, 2017.
[80] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language models,”
arXiv preprint arXiv:2106.09685, 2021.
[81] Y. Xin, J. Du, Q. Wang, K. Yan, and S. Ding, “Mmap: Multi-modal
alignment prompt for cross-domain multi-task learning,” in Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 16076–
16084, 2024.
[82] Y. Xin, J. Du, Q. Wang, Z. Lin, and K. Yan, “Vmt-adapter: Parameterefficient transfer learning for multi-task dense scene understanding,” in
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38,
pp. 16085–16093, 2024.
[83] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li, and Y. Du,
“Parameter-efficient fine-tuning for pre-trained vision models: A survey,”
arXiv preprint arXiv:2402.02242, 2024.
[84] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, “In-context learning distillation: Transferring few-shot learning ability of pre-trained language
models,” arXiv preprint arXiv:2212.10670, 2022.
[85] C. Snell, D. Klein, and R. Zhong, “Learning by distilling context,” arXiv
preprint arXiv:2209.15189, 2022.

